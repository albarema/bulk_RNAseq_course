{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Introduction to bulk RNAseq analysis \u00b6 Updated: 17/01/2023 This workshop material includes a tutorial on how to approach RNAseq data, starting from your sequencing reads (fastq files). Thus, the workshop only briefly touches upon laboratory protocols, library preparation, and experimental design of RNA sequencing experiments, mainly for the purpose of outlining considerations in the downstream bioinformatic analysis. This workshop is based on the materials developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC) , a collection of modified tutorials from the DESeq2 , R language vignettes and the nf-core rnaseq pipeline . Authors Jos\u00e9 Alejandro Romero Herrera Adrija Kalvisa Diana Andrejeva Henrike Zschach Jennifer Bartell Samuele Soraggi Overview \ud83d\udcac Syllabus: Course introduction Experimental planning Data explanation Read reprocessing and preprocessing pipelines Analysing RNAseq data RNAseq counts Exploratory analysis Differential Expression Analysis Functional analysis Summarized workflow \ud83d\udd70 Total Time Estimation: 8 hours \ud83d\udcc1 Supporting Materials: Workshop slides with theory on bulk RNAseq can be found in this zenodo repository . \ud83d\udccb License: Tutorial Content is licensed under Creative Commons Attribution 4.0 International License Course Requirements Knowledge of R, Rstudio and Rmarkdown. It is recommended that you have at least followed our workshop R basics Basic knowledge of RNAseq technology Basic knowledge of data science and statistics such as PCA, clustering and statistical testing This workshop material includes a tutorial on how to approach RNAseq data, starting from your sequencing reads (fastq files). Thus, the workshop only briefly touches upon laboratory protocols, library preparation, and experimental design of RNA sequencing experiments, mainly for the purpose of outlining considerations in the downstream bioinformatic analysis. This workshop is based on the materials developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC) , a collection of modified tutorials from the DESeq2 , R language vignettes and the nf-core rnaseq pipeline . The aim of this repository is to run a comprehensive but introductory workshop on bulk-RNAseq bioinformatic analyses. Each of the modules of this workshop is accompanied by a powerpoint slideshow explaining the steps and the theory behind a typical bioinformatics analysis (ideally with a teacher). Many of the slides are annotated with extra information and/or point to original sources for extra reading material. Goals By the end of this workshop, you should be able to analyse your own bulk RNAseq data: Preprocess your reads into a count matrix. Normalize your data. Explore your samples with PCAs and heatmaps. Perform Differential Expression Analysis. Annotate your results. Acknowledgements \u00b6 Center for Health Data Science , University of Copenhagen. Hugo Tavares , Bioinformatics Training Facility, University of Cambridge. Silvia Raineri , Center for Stem Cell Medicine (reNew), University of Copenhagen. Harvard Chan Bioinformatics Core (HBC) , check out their github repo nf-core community","title":"Course Introduction"},{"location":"index.html#introduction-to-bulk-rnaseq-analysis","text":"Updated: 17/01/2023 This workshop material includes a tutorial on how to approach RNAseq data, starting from your sequencing reads (fastq files). Thus, the workshop only briefly touches upon laboratory protocols, library preparation, and experimental design of RNA sequencing experiments, mainly for the purpose of outlining considerations in the downstream bioinformatic analysis. This workshop is based on the materials developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC) , a collection of modified tutorials from the DESeq2 , R language vignettes and the nf-core rnaseq pipeline .","title":"Introduction to bulk RNAseq analysis"},{"location":"index.html#acknowledgements","text":"Center for Health Data Science , University of Copenhagen. Hugo Tavares , Bioinformatics Training Facility, University of Cambridge. Silvia Raineri , Center for Stem Cell Medicine (reNew), University of Copenhagen. Harvard Chan Bioinformatics Core (HBC) , check out their github repo nf-core community","title":"Acknowledgements"},{"location":"02_experimental_planning.html","text":"Experimental design considerations \u00b6 Section Overview \ud83d\udd70 Time Estimation: 30 minutes \ud83d\udcac Learning Objectives: Describe the importance of replicates for RNA-seq differential expression experiments. Explain the relationship between the number of biological replicates, sequencing depth and the differentially expressed genes identified. Demonstrate how to design an RNA-seq experiment that avoids confounding and batch effects. Understanding the steps in the experimental process of RNA extraction and preparation of RNA-Seq libraries is helpful for designing an RNA-Seq experiment, but there are special considerations that should be highlighted that can greatly affect the quality of a differential expression analysis. These important considerations include: Proper experiment controls Number and type of replicates Issues related to confounding Addressing batch effects We will go over each of these considerations in detail, discussing best practice and optimal design. Controls \u00b6 Experimental controls must be used in order to minimize the effect of variables which are not the interest of the study. Thus, it allows the experiment to minimize the changes in all other variables except the one being tested, and help us ensure that there have been no deviations in the environment of the experiment that could end up influencing the outcome of the experiment, besides the variable they are investigating. There are different types of controls, but we will mainly see positive and negative controls: Negative : The negative control is a variable or group of samples where no response is expected. Positive : A positive control is a variable or group of samples that receives a treatment with a known positive result. It is very important that you give serious thought about proper controls of your experiment so you can control as many sources of variation as possible. This will greatly strengthen the results of your experiment. Replicates \u00b6 Experimental replicates can be performed as technical replicates or biological replicates . Image credit: Klaus B., EMBO J (2015) 34 : 2727-2730 Technical replicates: use the same biological sample to repeat the technical or experimental steps in order to accurately measure technical variation and remove it during analysis. Biological replicates use different biological samples of the same condition to measure the biological variation between samples. In the days of microarrays, technical replicates were considered a necessity; however, with the current RNA-Seq technologies, technical variation is much lower than biological variation and technical replicates are unnecessary . In contrast, biological replicates are absolutely essential for differential expression analysis. For mice or rats, it might be easy to determine what constitutes a different biological sample, but it's a bit more difficult to determine for cell lines . This article gives some great recommendations for cell line replicates. For differential expression analysis, the more biological replicates, the better the estimates of biological variation and the more precise our estimates of the mean expression levels. This leads to more accurate modeling of our data and identification of more differentially expressed genes. Image credit: Liu, Y., et al., Bioinformatics (2014) 30 (3): 301--304 As the figure above illustrates, biological replicates are of greater importance than sequencing depth , which is the total number of reads sequenced per sample. The figure shows the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified [ 1 ]. Note that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth . Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. Sample pooling: Try to avoid pooling of individuals/experiments, if possible; however, if absolutely necessary, then each pooled set of samples would count as a single replicate . To ensure similar amounts of variation between replicates, you would want to pool the same number of individuals for each pooled set of samples. For example, if you need at least 3 individuals to get enough material for your control replicate and at least 5 individuals to get enough material for your treatment replicate, you would pool 5 individuals for the control and 5 individuals for the treatment conditions. You would also make sure that the individuals that are pooled in both conditions are similar in sex, age, etc. Replicates are almost always preferred to greater sequencing depth for bulk RNA-Seq. However, guidelines depend on the experiment performed and the desired analysis . Below we list some general guidelines for replicates and sequencing depth to help with experimental planning: General gene-level differential expression: ENCODE guidelines suggest 30 million SE reads per sample (stranded). 15 million reads per sample is often sufficient, if there are a good number of replicates (>3). Spend money on more biological replicates, if possible. Generally recommended to have read length >= 50 bp Gene-level differential expression with detection of lowly-expressed genes: Similarly benefits from replicates more than sequencing depth. Sequence deeper with at least 30-60 million reads depending on level of expression (start with 30 million with a good number of replicates). Generally recommended to have read length >= 50 bp Isoform-level differential expression: Of known isoforms, suggested to have a depth of at least 30 million reads per sample and paired-end reads. Of novel isoforms should have more depth (> 60 million reads per sample). Choose biological replicates over paired/deeper sequencing. Generally recommended to have read length >= 50 bp, but longer is better as the reads will be more likely to cross exon junctions Perform careful QC of RNA quality. Be careful to use high quality preparation methods and restrict analysis to high quality RIN # samples. Other types of RNA analyses (intron retention, small RNA-Seq, etc.): Different recommendations depending on the analysis. Almost always more biological replicates are better! NOTE: The factor used to estimate the depth of sequencing for genomes is \"coverage\" - how many times do the number of nucleotides sequenced \"cover\" the genome. This metric is not exact for genomes (whole genome sequencing), but it is good enough and is used extensively. However, the metric does not work for transcriptomes because even though you may know what % of the genome has transcriptional activity, the expression of the genes is highly variable. Confounding variables \u00b6 A confounded RNA-Seq experiment is one where you cannot distinguish the separate effects of two different sources of variation in the data. For example, we know that sex has large effects on gene expression, and if all of our control mice were female and all of the treatment mice were male, then our treatment effect would be confounded by sex. We could not differentiate the effect of treatment from the effect of sex. To AVOID confounding: Ensure animals in each condition are all the same sex, age, litter, and batch , if possible. If not possible, then ensure to split the animals equally between conditions Batch effects \u00b6 A batch effect appears when variance is introduced into your data as a consequence of technical issues such as sample collection, storage, experimental protocol, etc. Batch effects are problematic for RNA-Seq analyses, since you may see significant differences in expression due solely to the batch effect. Image credit: Hicks SC, et al., bioRxiv (2015) To explore the issues generated by poor batch study design, they are highlighted nicely in this paper . How to know whether you have batches? \u00b6 Were all RNA isolations performed on the same day? Were all library preparations performed on the same day? Did the same person perform the RNA isolation/library preparation for all samples? Did you use the same reagents for all samples? Did you perform the RNA isolation/library preparation in the same location? If any of the answers is 'No' , then you have batches. Best practices regarding batches: \u00b6 Design the experiment in a way to avoid batches , if possible. If unable to avoid batches: Do NOT confound your experiment by batch: Image credit: Hicks SC, et al., bioRxiv (2015) DO split replicates of the different sample groups across batches. The more replicates the better (definitely more than 2). Image credit: Hicks SC, et al., bioRxiv (2015) DO make a balanced batch design. For example if you can only prepare a subset of samples in the lab on a given day, do not do 90% of samples on day 1 and the remaining 10% on day 2, aim for balance, 50% each day. DO include batch information in your experimental metadata . During the analysis, we can regress out the variation due to batch if not confounded so it doesn't affect our results if we have that information. NOTE: The sample preparation of cell line \"biological\" replicates \"should be performed as independently as possible\" (as batches), \"meaning that cell culture media should be prepared freshly for each experiment, different frozen cell stocks and growth factor batches, etc. should be used [ 2 ].\" However, preparation across all conditions should be performed at the same time. This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Experimental planning"},{"location":"02_experimental_planning.html#experimental-design-considerations","text":"Section Overview \ud83d\udd70 Time Estimation: 30 minutes \ud83d\udcac Learning Objectives: Describe the importance of replicates for RNA-seq differential expression experiments. Explain the relationship between the number of biological replicates, sequencing depth and the differentially expressed genes identified. Demonstrate how to design an RNA-seq experiment that avoids confounding and batch effects. Understanding the steps in the experimental process of RNA extraction and preparation of RNA-Seq libraries is helpful for designing an RNA-Seq experiment, but there are special considerations that should be highlighted that can greatly affect the quality of a differential expression analysis. These important considerations include: Proper experiment controls Number and type of replicates Issues related to confounding Addressing batch effects We will go over each of these considerations in detail, discussing best practice and optimal design.","title":"Experimental design considerations"},{"location":"02_experimental_planning.html#controls","text":"Experimental controls must be used in order to minimize the effect of variables which are not the interest of the study. Thus, it allows the experiment to minimize the changes in all other variables except the one being tested, and help us ensure that there have been no deviations in the environment of the experiment that could end up influencing the outcome of the experiment, besides the variable they are investigating. There are different types of controls, but we will mainly see positive and negative controls: Negative : The negative control is a variable or group of samples where no response is expected. Positive : A positive control is a variable or group of samples that receives a treatment with a known positive result. It is very important that you give serious thought about proper controls of your experiment so you can control as many sources of variation as possible. This will greatly strengthen the results of your experiment.","title":"Controls"},{"location":"02_experimental_planning.html#replicates","text":"Experimental replicates can be performed as technical replicates or biological replicates . Image credit: Klaus B., EMBO J (2015) 34 : 2727-2730 Technical replicates: use the same biological sample to repeat the technical or experimental steps in order to accurately measure technical variation and remove it during analysis. Biological replicates use different biological samples of the same condition to measure the biological variation between samples. In the days of microarrays, technical replicates were considered a necessity; however, with the current RNA-Seq technologies, technical variation is much lower than biological variation and technical replicates are unnecessary . In contrast, biological replicates are absolutely essential for differential expression analysis. For mice or rats, it might be easy to determine what constitutes a different biological sample, but it's a bit more difficult to determine for cell lines . This article gives some great recommendations for cell line replicates. For differential expression analysis, the more biological replicates, the better the estimates of biological variation and the more precise our estimates of the mean expression levels. This leads to more accurate modeling of our data and identification of more differentially expressed genes. Image credit: Liu, Y., et al., Bioinformatics (2014) 30 (3): 301--304 As the figure above illustrates, biological replicates are of greater importance than sequencing depth , which is the total number of reads sequenced per sample. The figure shows the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified [ 1 ]. Note that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth . Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. Sample pooling: Try to avoid pooling of individuals/experiments, if possible; however, if absolutely necessary, then each pooled set of samples would count as a single replicate . To ensure similar amounts of variation between replicates, you would want to pool the same number of individuals for each pooled set of samples. For example, if you need at least 3 individuals to get enough material for your control replicate and at least 5 individuals to get enough material for your treatment replicate, you would pool 5 individuals for the control and 5 individuals for the treatment conditions. You would also make sure that the individuals that are pooled in both conditions are similar in sex, age, etc. Replicates are almost always preferred to greater sequencing depth for bulk RNA-Seq. However, guidelines depend on the experiment performed and the desired analysis . Below we list some general guidelines for replicates and sequencing depth to help with experimental planning: General gene-level differential expression: ENCODE guidelines suggest 30 million SE reads per sample (stranded). 15 million reads per sample is often sufficient, if there are a good number of replicates (>3). Spend money on more biological replicates, if possible. Generally recommended to have read length >= 50 bp Gene-level differential expression with detection of lowly-expressed genes: Similarly benefits from replicates more than sequencing depth. Sequence deeper with at least 30-60 million reads depending on level of expression (start with 30 million with a good number of replicates). Generally recommended to have read length >= 50 bp Isoform-level differential expression: Of known isoforms, suggested to have a depth of at least 30 million reads per sample and paired-end reads. Of novel isoforms should have more depth (> 60 million reads per sample). Choose biological replicates over paired/deeper sequencing. Generally recommended to have read length >= 50 bp, but longer is better as the reads will be more likely to cross exon junctions Perform careful QC of RNA quality. Be careful to use high quality preparation methods and restrict analysis to high quality RIN # samples. Other types of RNA analyses (intron retention, small RNA-Seq, etc.): Different recommendations depending on the analysis. Almost always more biological replicates are better! NOTE: The factor used to estimate the depth of sequencing for genomes is \"coverage\" - how many times do the number of nucleotides sequenced \"cover\" the genome. This metric is not exact for genomes (whole genome sequencing), but it is good enough and is used extensively. However, the metric does not work for transcriptomes because even though you may know what % of the genome has transcriptional activity, the expression of the genes is highly variable.","title":"Replicates"},{"location":"02_experimental_planning.html#confounding-variables","text":"A confounded RNA-Seq experiment is one where you cannot distinguish the separate effects of two different sources of variation in the data. For example, we know that sex has large effects on gene expression, and if all of our control mice were female and all of the treatment mice were male, then our treatment effect would be confounded by sex. We could not differentiate the effect of treatment from the effect of sex. To AVOID confounding: Ensure animals in each condition are all the same sex, age, litter, and batch , if possible. If not possible, then ensure to split the animals equally between conditions","title":"Confounding variables"},{"location":"02_experimental_planning.html#batch-effects","text":"A batch effect appears when variance is introduced into your data as a consequence of technical issues such as sample collection, storage, experimental protocol, etc. Batch effects are problematic for RNA-Seq analyses, since you may see significant differences in expression due solely to the batch effect. Image credit: Hicks SC, et al., bioRxiv (2015) To explore the issues generated by poor batch study design, they are highlighted nicely in this paper .","title":"Batch effects"},{"location":"02_experimental_planning.html#how-to-know-whether-you-have-batches","text":"Were all RNA isolations performed on the same day? Were all library preparations performed on the same day? Did the same person perform the RNA isolation/library preparation for all samples? Did you use the same reagents for all samples? Did you perform the RNA isolation/library preparation in the same location? If any of the answers is 'No' , then you have batches.","title":"How to know whether you have batches?"},{"location":"02_experimental_planning.html#best-practices-regarding-batches","text":"Design the experiment in a way to avoid batches , if possible. If unable to avoid batches: Do NOT confound your experiment by batch: Image credit: Hicks SC, et al., bioRxiv (2015) DO split replicates of the different sample groups across batches. The more replicates the better (definitely more than 2). Image credit: Hicks SC, et al., bioRxiv (2015) DO make a balanced batch design. For example if you can only prepare a subset of samples in the lab on a given day, do not do 90% of samples on day 1 and the remaining 10% on day 2, aim for balance, 50% each day. DO include batch information in your experimental metadata . During the analysis, we can regress out the variation due to batch if not confounded so it doesn't affect our results if we have that information. NOTE: The sample preparation of cell line \"biological\" replicates \"should be performed as independently as possible\" (as batches), \"meaning that cell culture media should be prepared freshly for each experiment, different frozen cell stocks and growth factor batches, etc. should be used [ 2 ].\" However, preparation across all conditions should be performed at the same time. This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Best practices regarding batches:"},{"location":"03_data_explanation.html","text":"Dataset explanation \u00b6 Section Overview \ud83d\udd70 Time Estimation: 5 minutes \ud83d\udcac Learning Objectives: Explain the experiment and its objectives. We will be using the sequencing reads from the RNA-Seq dataset that is part of a larger study described in Kenny PJ et al, Cell Rep 2014 . RNA sequencing was performed on HEK293F cells which were either transfected with a MOV10 transgene, or siRNA to knock down Mov10 expression, or non-specific (irrelevant) siRNA. This resulted in 3 conditions Mov10 oe (over expression), Mov10 kd (knock down) and Irrelevant kd , respectively. The number of replicates is shown below. Using these data, we will evaluate transcriptional patterns associated with perturbation of MOV10 expression. Please note that the irrelevant siRNA will be treated as our control condition. What is the purpose of these datasets? What does Mov10 do? \u00b6 The authors are investigating interactions between various genes involved in Fragile X syndrome, a disease in which there is aberrant production of the FMRP protein. FMRP is \u201cmost commonly found in the brain, is essential for normal cognitive development and female reproductive function. Mutations of this gene can lead to fragile X syndrome, mental retardation, premature ovarian failure, autism, Parkinson's disease, developmental delays and other cognitive deficits.\u201d - from Wikipedia MOV10 , is a putative RNA helicase that is also associated with FMRP in the context of the microRNA pathway. The hypothesis the paper is testing is that FMRP and MOV10 associate and regulate the translation of a subset of RNAs. Our questions \u00b6 What patterns of expression can we identify with the loss or gain of MOV10? Are there any genes shared between the two conditions? This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Data Explanation"},{"location":"03_data_explanation.html#dataset-explanation","text":"Section Overview \ud83d\udd70 Time Estimation: 5 minutes \ud83d\udcac Learning Objectives: Explain the experiment and its objectives. We will be using the sequencing reads from the RNA-Seq dataset that is part of a larger study described in Kenny PJ et al, Cell Rep 2014 . RNA sequencing was performed on HEK293F cells which were either transfected with a MOV10 transgene, or siRNA to knock down Mov10 expression, or non-specific (irrelevant) siRNA. This resulted in 3 conditions Mov10 oe (over expression), Mov10 kd (knock down) and Irrelevant kd , respectively. The number of replicates is shown below. Using these data, we will evaluate transcriptional patterns associated with perturbation of MOV10 expression. Please note that the irrelevant siRNA will be treated as our control condition.","title":"Dataset explanation"},{"location":"03_data_explanation.html#what-is-the-purpose-of-these-datasets-what-does-mov10-do","text":"The authors are investigating interactions between various genes involved in Fragile X syndrome, a disease in which there is aberrant production of the FMRP protein. FMRP is \u201cmost commonly found in the brain, is essential for normal cognitive development and female reproductive function. Mutations of this gene can lead to fragile X syndrome, mental retardation, premature ovarian failure, autism, Parkinson's disease, developmental delays and other cognitive deficits.\u201d - from Wikipedia MOV10 , is a putative RNA helicase that is also associated with FMRP in the context of the microRNA pathway. The hypothesis the paper is testing is that FMRP and MOV10 associate and regulate the translation of a subset of RNAs.","title":"What is the purpose of these datasets? What does Mov10 do?"},{"location":"03_data_explanation.html#our-questions","text":"What patterns of expression can we identify with the loss or gain of MOV10? Are there any genes shared between the two conditions? This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Our questions"},{"location":"04a_preprocessing.html","text":"From raw sequence reads to count matrix: the RNA-seq workflow \u00b6 Section Overview \ud83d\udd70 Time Estimation: 40 minutes \ud83d\udcac Learning Objectives: Understand the different steps of the RNA-seq workflow, from RNA extraction to assessing the expression levels of genes. To perform differential gene expression analysis (DEA), we need to start with a matrix of counts representing the levels of gene expression. It is important to understand how the count matrix is generated, before diving into the statistical analysis. In this lesson we will briefly discuss the RNA-processing pipeline for bulk RNA-seq, and the different steps we take to go from raw sequencing reads to a gene expression count matrix . 1. RNA Extraction and library preparation \u00b6 Before RNA can be sequenced, it must first be extracted and separated from its cellular environment and prepared into a cDNA library. There are a number of steps involved which are outlined in the figure below, and in parallel there are various quality checks implemented to make sure we have good quality RNA to move forward with. We briefly describe some of these steps below. a. Enriching for RNA. Once the sample has been treated with DNAse to remove any contaminating DNA sequence, the sample undergoes either selection of the mRNA (polyA selection) or depletion of the ribosomal RNA (rRNA). Generally, rRNA represents the majority of the RNA present in a cell, while messenger RNAs represent a small percentage of total RNA, ~2% in humans. Therefore, if we want to study the protein-coding genes, we need to enrich mRNA or deplete the rRNA. For differential gene expression analysis, it is best to enrich for Poly(A)+, unless you are aiming to obtain information about long non-coding RNAs, in which case rRNA depletion is recommended. RNA Quality check : It is essential to check the integrity of the extracted RNA prior to starting the cDNA library prepation. Traditionally, RNA integrity was assessed via gel electrophoresis by visual inspection of the ribosomal RNA bands; but that method is time consuming and imprecise. The Bioanalyzer system from Agilent will rapidly assess RNA integrity and calculate an RNA Integrity Number (RIN), which facilitates the interpretation and reproducibility of RNA quality. RIN, essentially, provides a means by which RNA quality from different samples can be compared to each other in a standardized manner. b. Fragmentation and size selection. The remaining RNA molecules are then fragmented. This is done either via chemical, enzymatic (e.g., RNAses) or physical processes (e.g., chemical/mechanical shearing). These fragments then undergo size selection to retain only those fragments within a size range that Illumina sequencing machines can handle best, i.e., between 150 to 300 bp. Fragment size quality check : After size selection/exclusion the fragment size distribution should be assessed to ensure that it is unimodal and well-defined. c. Reverse transcribe RNA into double-stranded cDNA. Information about which strand a fragment originated from can be preserved by creating stranded libraries. The most commonly used method incorporates deoxy-UTP during the synthesis of the second cDNA strand (for details see Levin et al. (2010) ). Once double-stranded cDNA fragments are generated, sequence adapters are ligated to the ends. (Size selection can be performed here instead of at the RNA-level.) d. PCR amplification. If the amount of starting material is low and/or to increase the number of cDNA molecules to an amount sufficient for sequencing, libraries are usually PCR amplified. Run as few amplification cycles as possible to avoid PCR artifacts. Image source: Zeng and Mortavi, 2012 2. Sequencing (Illumina) \u00b6 Sequencing of the cDNA libraries will generate reads . Reads correspond to the nucleotide sequences of the ends of each of the cDNA fragments in the library. You will have the choice of sequencing either a single end of the cDNA fragments (single-end reads) or both ends of the fragments (paired-end reads). SE - Single end dataset => Only Read1 PE - Paired-end dataset => Read1 + Read2 PE can be 2 separate FastQ files or just one with interleaved pairs Generally, single-end sequencing is sufficient unless it is expected that the reads will match multiple locations on the genome (e.g. organisms with many paralogous genes), assemblies are being performed, or for splice isoform differentiation. On the other hand, paired-end sequencing helps resolve structural genome rearrangements e.g. insertions, deletions, or inversions. Furthermore, paired reads improve the alignment/assembly of reads from repetitive regions. The downside of this type of sequencing is that it may be twice as expensive. The scientific community is moving towards paired-end sequencing in general. However, for many purposes, single-end reads are perfectly adequate. Sequencing-by-synthesis \u00b6 Illumina sequencing technology uses a sequencing-by-synthesis approach. To explore sequencing by synthesis in more depth, please watch this linked video on Illumina's YouTube channel . We have provided a brief explanation of the steps below: Cluster growth : The DNA fragments in the cDNA library are denatured and hybridized to the glass flowcell (adapter complementarity). Each fragment is then clonally amplified, forming a cluster of double-stranded DNA. This step is necessary to ensure that the sequencing signal will be strong enough to be detected/captured unambiguously for each base of each fragment. Number of clusters ~= Number of reads Sequencing: The sequencing of the fragment ends is based on fluorophore labelled dNTPs with reversible terminator elements. In each sequencing cycle, a base is incorporated into every cluster and excited by a laser. Image acquisition: Each dNTP has a distinct excitatory signal emission which is captured by cameras. Base calling: The Base calling program will then generate the sequence of bases, i.e. reads , for each fragment/cluster by assessing the images captured during the many sequencing cycles. In addition to calling the base in every position, the base caller will also report the certainty with which it was able to make the call (quality information). Number of sequencing cycles = Length of reads 3. Quality control of raw sequencing data (FastQC) \u00b6 The raw reads obtained from the sequencer are stored as FASTQ files . The FASTQ file format is the de facto file format for sequence reads generated from next-generation sequencing technologies. Each FASTQ file is a text file which represents sequence readouts for a sample. Each read is represented by 4 lines as shown below: @HWI-ST330:304:H045HADXX:1:1101:1111:61397 CACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG + @?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI################################################################################################################## Line Description 1 Always begins with '\\@' and then information about the read 2 The actual DNA sequence, where N means that no base was called (poor quality) 3 Always begins with a '+' and sometimes the same info as in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 FastQC is a commonly used software that provides a simple way to do some quality control checks on raw sequence data . The main functions include: Providing a quick overview to tell you in which areas there may be problems Summary graphs and tables to quickly assess your data Export of results to an HTML based permanent report 4. Quantify expression \u00b6 Once we have explored the quality of our raw reads, we can move on to quantifying expression at the transcript level. The goal of this step is to identify from which transcript each of the reads originated from and the total number of reads associated with each transcript . Tools that have been found to be most accurate for this step in the analysis are referred to as lightweight alignment tools , which include: * Kallisto , * Sailfish and * Salmon Each of the tools in the list above work slightly differently from one another. However, common to all of them is that they avoid base-to-base genomic alignment of the reads . Genomic alignment is a step performed by older splice-aware alignment tools such as STAR and HISAT2 . In comparison to these tools, the lightweight alignment tools not only provide quantification estimates much faster (typically more than 20 times faster), but also improvements in precision . Nonetheless, a recent Nature article suggests that pseudoaligners have low accuracy compared to classic aligners. We will use the expression estimates, often referred to as 'pseudocounts', obtained from Salmon as the starting point for the differential gene expression analysis. 5. Quality control of aligned sequence reads (STAR/Qualimap) \u00b6 As mentioned above, the differential gene expression analysis will use transcript/gene pseudocounts generated by Salmon. However, to perform some basic quality checks on the sequencing data, it is important to align the reads to the whole genome. Either STAR or HiSAT2 are able to perform this step and generate a BAM file that can be used for QC. A tool called Qualimap explores the features of aligned reads in the context of the genomic region they map to , hence providing an overall view of the data quality (as an HTML file). Various quality metrics assessed by Qualimap include: DNA or rRNA contamination 5'-3' biases Coverage biases 6. Quality control: aggregating results with MultiQC \u00b6 Throughout the workflow we have performed various steps of quality checks on our data. You will need to do this for every sample in your dataset , making sure these metrics are consistent across the samples for a given experiment. Outlier samples should be flagged for further investigation and potential removal. Manually tracking these metrics and browsing through multiple HTML reports (FastQC, Qualimap) and log files (Salmon, STAR) for each sample is tedious and prone to errors. MultiQC is a tool which aggregates results from several tools and generates a single HTML report with plots to visualize and compare various QC metrics between the samples. Assessment of the QC metrics may result in the removal of samples before proceeding to the next step, if necessary. Once the QC has been performed on all the samples, we are ready to get started with Differential Gene Expression analysis with DESeq2 ! This lesson was originally developed by members of the teaching team at the Harvard Chan Bioinformatics Core (Meeta Mistry, Radhika Khetani and Mary Piper) (HBC) .","title":"Preprocessing steps"},{"location":"04a_preprocessing.html#from-raw-sequence-reads-to-count-matrixthe-rna-seq-workflow","text":"Section Overview \ud83d\udd70 Time Estimation: 40 minutes \ud83d\udcac Learning Objectives: Understand the different steps of the RNA-seq workflow, from RNA extraction to assessing the expression levels of genes. To perform differential gene expression analysis (DEA), we need to start with a matrix of counts representing the levels of gene expression. It is important to understand how the count matrix is generated, before diving into the statistical analysis. In this lesson we will briefly discuss the RNA-processing pipeline for bulk RNA-seq, and the different steps we take to go from raw sequencing reads to a gene expression count matrix .","title":"From raw sequence reads to count matrix:the RNA-seq workflow"},{"location":"04a_preprocessing.html#1-rna-extraction-and-library-preparation","text":"Before RNA can be sequenced, it must first be extracted and separated from its cellular environment and prepared into a cDNA library. There are a number of steps involved which are outlined in the figure below, and in parallel there are various quality checks implemented to make sure we have good quality RNA to move forward with. We briefly describe some of these steps below. a. Enriching for RNA. Once the sample has been treated with DNAse to remove any contaminating DNA sequence, the sample undergoes either selection of the mRNA (polyA selection) or depletion of the ribosomal RNA (rRNA). Generally, rRNA represents the majority of the RNA present in a cell, while messenger RNAs represent a small percentage of total RNA, ~2% in humans. Therefore, if we want to study the protein-coding genes, we need to enrich mRNA or deplete the rRNA. For differential gene expression analysis, it is best to enrich for Poly(A)+, unless you are aiming to obtain information about long non-coding RNAs, in which case rRNA depletion is recommended. RNA Quality check : It is essential to check the integrity of the extracted RNA prior to starting the cDNA library prepation. Traditionally, RNA integrity was assessed via gel electrophoresis by visual inspection of the ribosomal RNA bands; but that method is time consuming and imprecise. The Bioanalyzer system from Agilent will rapidly assess RNA integrity and calculate an RNA Integrity Number (RIN), which facilitates the interpretation and reproducibility of RNA quality. RIN, essentially, provides a means by which RNA quality from different samples can be compared to each other in a standardized manner. b. Fragmentation and size selection. The remaining RNA molecules are then fragmented. This is done either via chemical, enzymatic (e.g., RNAses) or physical processes (e.g., chemical/mechanical shearing). These fragments then undergo size selection to retain only those fragments within a size range that Illumina sequencing machines can handle best, i.e., between 150 to 300 bp. Fragment size quality check : After size selection/exclusion the fragment size distribution should be assessed to ensure that it is unimodal and well-defined. c. Reverse transcribe RNA into double-stranded cDNA. Information about which strand a fragment originated from can be preserved by creating stranded libraries. The most commonly used method incorporates deoxy-UTP during the synthesis of the second cDNA strand (for details see Levin et al. (2010) ). Once double-stranded cDNA fragments are generated, sequence adapters are ligated to the ends. (Size selection can be performed here instead of at the RNA-level.) d. PCR amplification. If the amount of starting material is low and/or to increase the number of cDNA molecules to an amount sufficient for sequencing, libraries are usually PCR amplified. Run as few amplification cycles as possible to avoid PCR artifacts. Image source: Zeng and Mortavi, 2012","title":"1. RNA Extraction and library preparation"},{"location":"04a_preprocessing.html#2-sequencing-illumina","text":"Sequencing of the cDNA libraries will generate reads . Reads correspond to the nucleotide sequences of the ends of each of the cDNA fragments in the library. You will have the choice of sequencing either a single end of the cDNA fragments (single-end reads) or both ends of the fragments (paired-end reads). SE - Single end dataset => Only Read1 PE - Paired-end dataset => Read1 + Read2 PE can be 2 separate FastQ files or just one with interleaved pairs Generally, single-end sequencing is sufficient unless it is expected that the reads will match multiple locations on the genome (e.g. organisms with many paralogous genes), assemblies are being performed, or for splice isoform differentiation. On the other hand, paired-end sequencing helps resolve structural genome rearrangements e.g. insertions, deletions, or inversions. Furthermore, paired reads improve the alignment/assembly of reads from repetitive regions. The downside of this type of sequencing is that it may be twice as expensive. The scientific community is moving towards paired-end sequencing in general. However, for many purposes, single-end reads are perfectly adequate.","title":"2. Sequencing (Illumina)"},{"location":"04a_preprocessing.html#sequencing-by-synthesis","text":"Illumina sequencing technology uses a sequencing-by-synthesis approach. To explore sequencing by synthesis in more depth, please watch this linked video on Illumina's YouTube channel . We have provided a brief explanation of the steps below: Cluster growth : The DNA fragments in the cDNA library are denatured and hybridized to the glass flowcell (adapter complementarity). Each fragment is then clonally amplified, forming a cluster of double-stranded DNA. This step is necessary to ensure that the sequencing signal will be strong enough to be detected/captured unambiguously for each base of each fragment. Number of clusters ~= Number of reads Sequencing: The sequencing of the fragment ends is based on fluorophore labelled dNTPs with reversible terminator elements. In each sequencing cycle, a base is incorporated into every cluster and excited by a laser. Image acquisition: Each dNTP has a distinct excitatory signal emission which is captured by cameras. Base calling: The Base calling program will then generate the sequence of bases, i.e. reads , for each fragment/cluster by assessing the images captured during the many sequencing cycles. In addition to calling the base in every position, the base caller will also report the certainty with which it was able to make the call (quality information). Number of sequencing cycles = Length of reads","title":"Sequencing-by-synthesis"},{"location":"04a_preprocessing.html#3-quality-control-of-raw-sequencing-data-fastqc","text":"The raw reads obtained from the sequencer are stored as FASTQ files . The FASTQ file format is the de facto file format for sequence reads generated from next-generation sequencing technologies. Each FASTQ file is a text file which represents sequence readouts for a sample. Each read is represented by 4 lines as shown below: @HWI-ST330:304:H045HADXX:1:1101:1111:61397 CACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG + @?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI################################################################################################################## Line Description 1 Always begins with '\\@' and then information about the read 2 The actual DNA sequence, where N means that no base was called (poor quality) 3 Always begins with a '+' and sometimes the same info as in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 FastQC is a commonly used software that provides a simple way to do some quality control checks on raw sequence data . The main functions include: Providing a quick overview to tell you in which areas there may be problems Summary graphs and tables to quickly assess your data Export of results to an HTML based permanent report","title":"3. Quality control of raw sequencing data (FastQC)"},{"location":"04a_preprocessing.html#4-quantify-expression","text":"Once we have explored the quality of our raw reads, we can move on to quantifying expression at the transcript level. The goal of this step is to identify from which transcript each of the reads originated from and the total number of reads associated with each transcript . Tools that have been found to be most accurate for this step in the analysis are referred to as lightweight alignment tools , which include: * Kallisto , * Sailfish and * Salmon Each of the tools in the list above work slightly differently from one another. However, common to all of them is that they avoid base-to-base genomic alignment of the reads . Genomic alignment is a step performed by older splice-aware alignment tools such as STAR and HISAT2 . In comparison to these tools, the lightweight alignment tools not only provide quantification estimates much faster (typically more than 20 times faster), but also improvements in precision . Nonetheless, a recent Nature article suggests that pseudoaligners have low accuracy compared to classic aligners. We will use the expression estimates, often referred to as 'pseudocounts', obtained from Salmon as the starting point for the differential gene expression analysis.","title":"4. Quantify expression"},{"location":"04a_preprocessing.html#5-quality-control-of-aligned-sequence-reads-starqualimap","text":"As mentioned above, the differential gene expression analysis will use transcript/gene pseudocounts generated by Salmon. However, to perform some basic quality checks on the sequencing data, it is important to align the reads to the whole genome. Either STAR or HiSAT2 are able to perform this step and generate a BAM file that can be used for QC. A tool called Qualimap explores the features of aligned reads in the context of the genomic region they map to , hence providing an overall view of the data quality (as an HTML file). Various quality metrics assessed by Qualimap include: DNA or rRNA contamination 5'-3' biases Coverage biases","title":"5. Quality control of aligned sequence reads (STAR/Qualimap)"},{"location":"04a_preprocessing.html#6-quality-control-aggregating-results-with-multiqc","text":"Throughout the workflow we have performed various steps of quality checks on our data. You will need to do this for every sample in your dataset , making sure these metrics are consistent across the samples for a given experiment. Outlier samples should be flagged for further investigation and potential removal. Manually tracking these metrics and browsing through multiple HTML reports (FastQC, Qualimap) and log files (Salmon, STAR) for each sample is tedious and prone to errors. MultiQC is a tool which aggregates results from several tools and generates a single HTML report with plots to visualize and compare various QC metrics between the samples. Assessment of the QC metrics may result in the removal of samples before proceeding to the next step, if necessary. Once the QC has been performed on all the samples, we are ready to get started with Differential Gene Expression analysis with DESeq2 ! This lesson was originally developed by members of the teaching team at the Harvard Chan Bioinformatics Core (Meeta Mistry, Radhika Khetani and Mary Piper) (HBC) .","title":"6. Quality control: aggregating results with MultiQC"},{"location":"04b_pipelines.html","text":"Automating your workflow: nf-core pipelines \u00b6 Section Overview \ud83d\udd70 Time Estimation: X minutes \ud83d\udcac Learning Objectives: Understand what is a pipeline. Learn about existing automated workflows from the bioinformatics community. Learn how to use the nf-core pipeline for bulk RNAseq analysis. The nf-core project is a community effort to collect a curated set of analysis pipelines built using [Nextflow] ( https://www.nextflow.io/ ), an incredibly powerful and flexible workflow language. This means that all the tools and steps used in your RNAseq workflow can be automated and easily reproduced by other researchers if necessary. In addition, if you use any of the nf-core pipelines, you will be sure that all the necessary tools are available to you in any computer platform (Cloud computing, HPC or your personal computer). The RNAseq pipeline enables using many different tools, such as STAR, RSEM, HISAT2 or Salmon, and allows quantification of gene/isoform counts and provides extensive quality control checks at each step of the workflow. We encourage your to take a look at the pipeline and its documentation if you need to preprocess your RNAseq reads from stratch.","title":"Nextflow and nf-core pipelines"},{"location":"04b_pipelines.html#automating-your-workflow-nf-core-pipelines","text":"Section Overview \ud83d\udd70 Time Estimation: X minutes \ud83d\udcac Learning Objectives: Understand what is a pipeline. Learn about existing automated workflows from the bioinformatics community. Learn how to use the nf-core pipeline for bulk RNAseq analysis. The nf-core project is a community effort to collect a curated set of analysis pipelines built using [Nextflow] ( https://www.nextflow.io/ ), an incredibly powerful and flexible workflow language. This means that all the tools and steps used in your RNAseq workflow can be automated and easily reproduced by other researchers if necessary. In addition, if you use any of the nf-core pipelines, you will be sure that all the necessary tools are available to you in any computer platform (Cloud computing, HPC or your personal computer). The RNAseq pipeline enables using many different tools, such as STAR, RSEM, HISAT2 or Salmon, and allows quantification of gene/isoform counts and provides extensive quality control checks at each step of the workflow. We encourage your to take a look at the pipeline and its documentation if you need to preprocess your RNAseq reads from stratch.","title":"Automating your workflow: nf-core pipelines"},{"location":"04c_preprocessing_setup.html","text":"Running the bulk RNAseq pipeline in uCloud \u00b6 Section Overview \ud83d\udd70 Time Estimation: X minutes \ud83d\udcac Learning Objectives: Learn about the UCloud computing system. Learn how to submit a job and explore your results folders. Submit a nf-core RNAseq run on our data Submit the job in Ucloud \u00b6 Access Ucloud with your account and choose the project Sandbox RNASeq Workshop where you have been invited. Or ask to be invited to jose.romero@sund.ku.dk. Click on Apps on the left-side menu, and search for the application Transcriptomics Sandbox and click on it. You will be met with a series of possible parameters to choose. However, we have prepared the parameters already for you! Just click on Import parameters : Then, Import file from UCloud : And select the jobParameters.json in: sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json Make sure that the hard-drive icon says sandbox_bulkRNASeq!! You are ready to run the app by clicking on the button on the right column of the screen ( submit ). Now, wait some time until the screen looks like the figure below. It usually takes a few minutes for everything to be ready and installed. You can always come back to this screen from the left menu Runs on uCloud, so that you can add extra time or stop the app if you will not use it. Now, click on open interface on the top right-hand side of the screen. You will start Rstudio through your browser! On the lower right side of Rstudio, where you see the file explorer, there shoud be a folder bulk_RNAseq_course . Here you will find the materials of the course, but it is a \"read only\" file. In order to have a copy for your own purposes, we will use a script. Go to the Terminal tab on the top left of the Rstudio session and copy-paste this command: ./bulk_RNAseq_course/Scripts/ucloud_analysis_setup/ucloud_setup.sh Now there should be another folder in the file explorer called introduction_bulkRNAseq_analysis . This is the folder you should use from now on. Now you can go back to the Console tab. You are ready to start analysing your data! Stopping the app \u00b6 When you are done, go on Runs in uCloud, and choose your app if it is still running. Then you will be able to stop it from using resources. Saved work \u00b6 After running a first work session, everything that you have created, including the scripts and results of your analysis, will be saved in your own personal \"Jobs\" folder. Inside this folder there will be a subfolder called Transcriptomics Sandbox , which will contain all the jobs you have run with the Transcriptomics Sandbox app. Inside this folder, you will find your folder named after the job name you gave in the previous step. Your material will be saved in a volume with your username, that you should be able to see under the menu Files . Go to Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis Restarting the Rstudio session \u00b6 If you want to keep working on your previous results, you can restart an Rstudio session following these steps: Click on Apps on the left-side menu, and look for the application Transcriptomics Sandbox and click on it. You will be met again with a series of possible parameters to choose. You have to assign again the Import parameters file as before. sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json In \"Select folders to use\" , add the folder with the results of your previous job: Go to Member Files: your_username \u2192 Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis , click \"Use.\" You are ready to run the app by clicking on the button on the right column of the screen ( submit ). After opening the Rstudio interface, you should be able to access the folder introduction_bulkRNAseq_analysis , where you will find your course notebooks and results from your previous work!","title":"UCloud setup"},{"location":"04c_preprocessing_setup.html#running-the-bulk-rnaseq-pipeline-in-ucloud","text":"Section Overview \ud83d\udd70 Time Estimation: X minutes \ud83d\udcac Learning Objectives: Learn about the UCloud computing system. Learn how to submit a job and explore your results folders. Submit a nf-core RNAseq run on our data","title":"Running the bulk RNAseq pipeline in uCloud"},{"location":"04c_preprocessing_setup.html#submit-the-job-in-ucloud","text":"Access Ucloud with your account and choose the project Sandbox RNASeq Workshop where you have been invited. Or ask to be invited to jose.romero@sund.ku.dk. Click on Apps on the left-side menu, and search for the application Transcriptomics Sandbox and click on it. You will be met with a series of possible parameters to choose. However, we have prepared the parameters already for you! Just click on Import parameters : Then, Import file from UCloud : And select the jobParameters.json in: sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json Make sure that the hard-drive icon says sandbox_bulkRNASeq!! You are ready to run the app by clicking on the button on the right column of the screen ( submit ). Now, wait some time until the screen looks like the figure below. It usually takes a few minutes for everything to be ready and installed. You can always come back to this screen from the left menu Runs on uCloud, so that you can add extra time or stop the app if you will not use it. Now, click on open interface on the top right-hand side of the screen. You will start Rstudio through your browser! On the lower right side of Rstudio, where you see the file explorer, there shoud be a folder bulk_RNAseq_course . Here you will find the materials of the course, but it is a \"read only\" file. In order to have a copy for your own purposes, we will use a script. Go to the Terminal tab on the top left of the Rstudio session and copy-paste this command: ./bulk_RNAseq_course/Scripts/ucloud_analysis_setup/ucloud_setup.sh Now there should be another folder in the file explorer called introduction_bulkRNAseq_analysis . This is the folder you should use from now on. Now you can go back to the Console tab. You are ready to start analysing your data!","title":"Submit the job in Ucloud"},{"location":"04c_preprocessing_setup.html#stopping-the-app","text":"When you are done, go on Runs in uCloud, and choose your app if it is still running. Then you will be able to stop it from using resources.","title":"Stopping the app"},{"location":"04c_preprocessing_setup.html#saved-work","text":"After running a first work session, everything that you have created, including the scripts and results of your analysis, will be saved in your own personal \"Jobs\" folder. Inside this folder there will be a subfolder called Transcriptomics Sandbox , which will contain all the jobs you have run with the Transcriptomics Sandbox app. Inside this folder, you will find your folder named after the job name you gave in the previous step. Your material will be saved in a volume with your username, that you should be able to see under the menu Files . Go to Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis","title":"Saved work"},{"location":"04c_preprocessing_setup.html#restarting-the-rstudio-session","text":"If you want to keep working on your previous results, you can restart an Rstudio session following these steps: Click on Apps on the left-side menu, and look for the application Transcriptomics Sandbox and click on it. You will be met again with a series of possible parameters to choose. You have to assign again the Import parameters file as before. sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json In \"Select folders to use\" , add the folder with the results of your previous job: Go to Member Files: your_username \u2192 Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis , click \"Use.\" You are ready to run the app by clicking on the button on the right column of the screen ( submit ). After opening the Rstudio interface, you should be able to access the folder introduction_bulkRNAseq_analysis , where you will find your course notebooks and results from your previous work!","title":"Restarting the Rstudio session"},{"location":"05a_data_analysis_setup.html","text":"Setup for teaching in uCloud \u00b6 Access Ucloud with your account and choose the project Sandbox RNASeq Workshop where you have been invited. Click on Apps on the left-side menu, and search for the application Transcriptomics Sandbox and click on it. You will be met with a series of possible parameters to choose. However, we have prepared the parameters already for you! Just click on Import parameters : Then, Import file from UCloud : And select the jobParameters.json in: sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json Make sure that the hard-drive icon says sandbox_bulkRNASeq!! You are ready to run the app by clicking on the button on the right column of the screen ( submit ). Now, wait some time until the screen looks like the figure below. It usually takes a few minutes for everything to be ready and installed. You can always come back to this screen from the left menu Runs on uCloud, so that you can add extra time or stop the app if you will not use it. Now, click on open interface on the top right-hand side of the screen. You will start Rstudio through your browser! On the lower right side of Rstudio, where you see the file explorer, there shoud be a folder bulk_RNAseq_course . Here you will find the materials of the course, but it is a \"read only\" file. In order to have a copy for your own purposes, we will use a script. Go to the Terminal tab on the top left of the Rstudio session and copy-paste this command: ./bulk_RNAseq_course/Scripts/ucloud_analysis_setup/ucloud_setup.sh Now there should be another folder in the file explorer called introduction_bulkRNAseq_analysis . This is the folder you should use from now on. Now you can go back to the Console tab. You are ready to start analysing your data! Stopping the app \u00b6 When you are done, go on Runs in uCloud, and choose your app if it is still running. Then you will be able to stop it from using resources. Saved work \u00b6 After running a first work session, everything that you have created, including the scripts and results of your analysis, will be saved in your own personal \"Jobs\" folder. Inside this folder there will be a subfolder called Transcriptomics Sandbox , which will contain all the jobs you have run with the Transcriptomics Sandbox app. Inside this folder, you will find your folder named after the job name you gave in the previous step. Your material will be saved in a volume with your username, that you should be able to see under the menu Files . Go to Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis Restarting the Rstudio session \u00b6 If you want to keep working on your previous results, you can restart an Rstudio session following these steps: Click on Apps on the left-side menu, and look for the application Transcriptomics Sandbox and click on it. You will be met again with a series of possible parameters to choose. You have to assign again the Import parameters file as before. sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json In \"Select folders to use\" , add the folder with the results of your previous job: Go to Member Files: your_username \u2192 Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis , click \"Use.\" You are ready to run the app by clicking on the button on the right column of the screen ( submit ). After opening the Rstudio interface, you should be able to access the folder introduction_bulkRNAseq_analysis , where you will find your course notebooks and results from your previous work!","title":"Ucloud setup"},{"location":"05a_data_analysis_setup.html#setup-for-teaching-in-ucloud","text":"Access Ucloud with your account and choose the project Sandbox RNASeq Workshop where you have been invited. Click on Apps on the left-side menu, and search for the application Transcriptomics Sandbox and click on it. You will be met with a series of possible parameters to choose. However, we have prepared the parameters already for you! Just click on Import parameters : Then, Import file from UCloud : And select the jobParameters.json in: sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json Make sure that the hard-drive icon says sandbox_bulkRNASeq!! You are ready to run the app by clicking on the button on the right column of the screen ( submit ). Now, wait some time until the screen looks like the figure below. It usually takes a few minutes for everything to be ready and installed. You can always come back to this screen from the left menu Runs on uCloud, so that you can add extra time or stop the app if you will not use it. Now, click on open interface on the top right-hand side of the screen. You will start Rstudio through your browser! On the lower right side of Rstudio, where you see the file explorer, there shoud be a folder bulk_RNAseq_course . Here you will find the materials of the course, but it is a \"read only\" file. In order to have a copy for your own purposes, we will use a script. Go to the Terminal tab on the top left of the Rstudio session and copy-paste this command: ./bulk_RNAseq_course/Scripts/ucloud_analysis_setup/ucloud_setup.sh Now there should be another folder in the file explorer called introduction_bulkRNAseq_analysis . This is the folder you should use from now on. Now you can go back to the Console tab. You are ready to start analysing your data!","title":"Setup for teaching in uCloud"},{"location":"05a_data_analysis_setup.html#stopping-the-app","text":"When you are done, go on Runs in uCloud, and choose your app if it is still running. Then you will be able to stop it from using resources.","title":"Stopping the app"},{"location":"05a_data_analysis_setup.html#saved-work","text":"After running a first work session, everything that you have created, including the scripts and results of your analysis, will be saved in your own personal \"Jobs\" folder. Inside this folder there will be a subfolder called Transcriptomics Sandbox , which will contain all the jobs you have run with the Transcriptomics Sandbox app. Inside this folder, you will find your folder named after the job name you gave in the previous step. Your material will be saved in a volume with your username, that you should be able to see under the menu Files . Go to Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis","title":"Saved work"},{"location":"05a_data_analysis_setup.html#restarting-the-rstudio-session","text":"If you want to keep working on your previous results, you can restart an Rstudio session following these steps: Click on Apps on the left-side menu, and look for the application Transcriptomics Sandbox and click on it. You will be met again with a series of possible parameters to choose. You have to assign again the Import parameters file as before. sandbox_bulkRNASeq -> bulk_RNAseq_course -> Scripts -> ucloud_analysis_setup -> jobParameters.json In \"Select folders to use\" , add the folder with the results of your previous job: Go to Member Files: your_username \u2192 Jobs \u2192 Transcriptomics Sandbox \u2192 job_name \u2192 introduction_bulkRNAseq_analysis , click \"Use.\" You are ready to run the app by clicking on the button on the right column of the screen ( submit ). After opening the Rstudio interface, you should be able to access the folder introduction_bulkRNAseq_analysis , where you will find your course notebooks and results from your previous work!","title":"Restarting the Rstudio session"},{"location":"05b_count_matrix.html","text":"Approximate time: 20 minutes Learning Objectives \u00b6 Describe how to set up an RNA-seq project in R Describe the RNA-seq and the differential gene expression analysis workflow Explain why negative binomial distribution is used to model RNA-seq count data Differential gene expression (DGE) analysis overview \u00b6 The goal of RNA-seq is often to perform differential expression testing to determine which genes are expressed at different levels between conditions. These genes can offer biological insight into the processes affected by the condition(s) of interest. To determine the expression levels of genes, our RNA-seq workflow followed the steps detailed in the image below. All steps were performed on the command line (Linux/Unix) through the generation of the read counts per gene. The differential expression analysis and any downstream functional analysis are generally performed in R using R packages specifically designed for the complex statistical analyses required to determine whether genes are differentially expressed. In the next few lessons, we will walk you through an end-to-end gene-level RNA-seq differential expression workflow using various R packages. We will start with the count matrix, do some exploratory data analysis for quality assessment and explore the relationship between samples. Next, we will perform differential expression analysis, and visually explore the results prior to performing downstream functional analysis. Setting up \u00b6 Before we get into the details of the analysis, let\u2019s get started by opening up RStudio and setting up a new project for this analysis. Go to the File menu and select New Project . In the New Project window, choose Existing Directory . Then, choose introduction_bulkRNAseq_analysis as your project working directory. The new project should automatically open in RStudio. To check whether or not you are in the correct working directory, use getwd() . The path /work/introduction_bulkRNAseq_analysis should be returned to you in the console. When finished your working directory should now look similar to this: Inside the folder Notebooks you will find the scripts (in Rmd format) that we will follow during the sessions. In the folder Results you will save the results of your scripts, analysis and tests. To avoid copying the original dataset for each student (very inefficient) the dataset is contained inside the shared folder /work/introduction_bulkRNAseq_analysis/Data/ . Do not attempt to modify this folder, as it might mess up the files for the rest of your colleagues. Now you can open the first practical session: 06a_count_matrix.Rmd Loading libraries \u00b6 For this analysis we will be using several R packages, some which have been installed from CRAN and others from Bioconductor. To use these packages (and the functions contained within them), we need to load the libraries. Add the following to your script and don\u2019t forget to comment liberally! ## Setup ### Bioconductor and CRAN libraries used library ( tidyverse ) library ( RColorBrewer ) library ( DESeq2 ) library ( pheatmap ) library ( DEGreport ) library ( ggrepel ) Loading data \u00b6 To load the data into our current environment, we will be using the read.table function. By default the function expects tab-delimited files, which is what we have. ## Load in data data <- read_table ( \"/work/introduction_bulkRNAseq_analysis/Data/Mov10_full_counts.txt\" ) meta <- read_table ( \"/work/introduction_bulkRNAseq_analysis/Data/Mov10_full_meta.txt\" ) Use class() to inspect our data and make sure we are working with data frames: ### Check classes of the data we just brought in class ( meta ) class ( data ) Viewing data \u00b6 Make sure your datasets contain the expected samples / information before proceeding to perfom any type of analysis. View ( meta ) View ( data ) Using the abundance estimates from Salmon as input to DESeq2 \u00b6 The counts used in these lessons were generated using the standard approach for RNA-seq analysis, where samples were aligned to the genome using a splice-aware aligner followed by counting. If you are using lightweight algorithms such as Salmon, Sailfish or Kallisto to generate abundance estimates, you can also use DESeq2 to perform gene-level differential expression analysis. These transcript abundance estimates, often referred to as \u2018pseudocounts\u2019, can be converted for use with DESeq2 but the setup is slightly more involved. If you are interested in knowing more about using Salmon pseudocounts for DESeq2, there are materials linked here . Differential gene expression analysis overview \u00b6 So, what does this count data actually represent? The count data used for differential expression analysis represents the number of sequence reads that originated from a particular gene. The higher the number of counts, the more reads associated with that gene, and the assumption that there was a higher level of expression of that gene in the sample. With differential expression analysis, we are looking for genes that change in expression between two or more groups (defined in the metadata) - case vs. control - correlation of expression with some variable or clinical outcome Why does it not work to identify differentially expressed gene by ranking the genes by how different they are between the two groups (based on fold change values)? Genes that vary in expression level between groups of samples may do so solely as a consequence of the biological variable(s) of interest. However, this difference is often also related to extraneous effects, in fact, sometimes these effects exclusively account for the observed variation. The goal of differential expression analysis to determine the relative role of these effects, hence separating the \u201cinteresting\u201d variance from the \u201cuninteresting\u201d variance. Although the mean expression levels between sample groups may appear to be quite different, it is possible that the difference is not actually significant. This is illustrated for \u2018GeneA\u2019 expression between \u2018untreated\u2019 and \u2018treated\u2019 groups in the figure below. The mean expression level of geneA for the \u2018treated\u2019 group is twice as large as for the \u2018untreated\u2019 group, but the variation between replicates indicates that this may not be a significant difference. We need to take into account the variation in the data (and where it might be coming from) when determining whether genes are differentially expressed. Differential expression analysis is used to determine, for each gene, whether the differences in expression (counts) between groups is significant given the amount of variation observed within groups (replicates). To test for significance, we need an appropriate statistical model that accurately performs normalization (to account for differences in sequencing depth, etc.) and variance modeling (to account for few numbers of replicates and large dynamic expression range). RNA-seq count distribution \u00b6 To determine the appropriate statistical model, we need information about the distribution of counts. To get an idea about how RNA-seq counts are distributed, let\u2019s plot the counts of all the samples: pdata <- data %>% gather ( key = Sample , value = Count , - GeneSymbol ) pdata ggplot ( pdata ) + geom_histogram ( aes ( x = Count ), stat = \"bin\" , bins = 200 ) + xlab ( \"Raw expression counts\" ) + ylab ( \"Number of genes\" ) If we zoom in close to zero, we can see a large number of genes with counts of zero: ggplot ( pdata ) + geom_histogram ( aes ( x = Count ), stat = \"bin\" , bins = 200 ) + xlim ( -5 , 500 ) + xlab ( \"Raw expression counts\" ) + ylab ( \"Number of genes\" ) These images illustrate some common features of RNA-seq count data, including a low number of counts associated with a large proportion of genes , and a long right tail due to the lack of any upper limit for expression . Unlike microarray data, which has a dynamic range maximum limited due to when the probes max out, there is no limit of maximum expression for RNA-seq data. Due to the differences in these technologies, the statistical models used to fit the data are different between the two methods. NOTE: The log intensities of the microarray data approximate a normal distribution. However, due to the different properties of the of RNA-seq count data, such as integer counts instead of continuous measurements and non-normally distributed data, the normal distribution does not accurately model RNA-seq counts [ 1 ]. Modeling count data \u00b6 RNAseq count data can be modeled using a Poisson distribution . this particular distribution is fitting for data where the number of cases is very large but the probability of an event occurring is very small . To give you an example, think of the lottery: many people buy lottery tickets (high number of cases), but only very few win (the probability of the event is small). Check this video from Rafael Irizarry in the EdX class for more details . With RNA-Seq data, a very large number of RNAs are represented and the probability of pulling out a particular transcript is very small . Thus, it would be an appropriate situation to use the Poisson distribution. However, a unique property of this distribution is that the mean == variance. Realistically, with RNA-Seq data there is always some biological variation present across the replicates (within a sample class). Genes with larger average expression levels will tend to have larger observed variances across replicates. The model that fits best, given this type of variability observed for replicates, is the Negative Binomial (NB) model . Essentially, the NB model is a good approximation for data where the mean \\< variance , as is the case with RNA-Seq count data. NOTE: Biological replicates represent multiple samples (i.e. RNA from different mice) representing the same sample class Technical replicates represent the same sample (i.e. RNA from the same mouse) but with technical steps replicated Usually biological variance is much greater than technical variance, so we do not need to account for technical variance to identify biological differences in expression Don\u2019t spend money on technical replicates - biological replicates are much more useful NOTE: If you are using cell lines and are unsure whether or not you have prepared biological or technical replicates, take a look at this link . This is a useful resource in helping you determine how best to set up your in-vitro experiment. **How do I know if my data should be modeled using the Poisson distribution or Negative Binomial distribution??** If it\u2019s count data, it should fit the negative binomial, as discussed previously. However, it can be helpful to plot the *mean versus the variance* of your data. *Remember for the Poisson model, mean = variance, but for NB, mean \\ < variance.* Run the following code to plot the *mean versus variance* for our data: df <- data %>% rowwise () %>% summarise ( mean_counts = mean ( Mov10_kd_2 : Irrel_kd_3 ), variance_counts = var ( Mov10_kd_2 : Irrel_kd_3 )) ggplot ( df ) + geom_point ( aes ( x = mean_counts , y = variance_counts )) + geom_abline ( intercept = 0 , slope = 1 , color = \"red\" ) + scale_y_log10 () + scale_x_log10 () Note that in the above figure, the variance across replicates tends to be greater than the mean (red line), especially for genes with large mean expression levels. *This is a good indication that our data do not fit the Poisson distribution and we need to account for this increase in variance using the Negative Binomial model (i.e. Poisson will underestimate variability leading to an increase in false positive DE genes).* Improving mean estimates (i.e. reducing variance) with biological replicates \u00b6 The variance or scatter tends to reduce as we increase the number of biological replicates ( the distribution will approach the Poisson distribution with increasing numbers of replicates ), since standard deviations of averages are smaller than standard deviations of individual observations. The value of additional replicates is that as you add more data (replicates), you get increasingly precise estimates of group means, and ultimately greater confidence in the ability to distinguish differences between sample classes (i.e. more DE genes). The figure below illustrates the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified (from Liu et al. (2013) : Note that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth . Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. Generally, the minimum sequencing depth recommended is 20-30 million reads per sample, but we have seen good RNA-seq experiments with 10 million reads if there are a good number of replicates. Differential expression analysis workflow \u00b6 To model counts appropriately when performing a differential expression analysis, there are a number of software packages that have been developed for differential expression analysis of RNA-seq data. Even as new methods are continuously being developed a few tools are generally recommended as best practice, like DESeq2 , EdgeR and Limma-Voom . Many studies describing comparisons between these methods show that while there is some agreement, there is also much variability between tools. Additionally, there is no one method that performs optimally under all conditions ( Soneson and Dleorenzi, 2013 , Corchete et al, 2020 ) . We will be using DESeq2 for the DE analysis, and the analysis steps with DESeq2 are shown in the flowchart below in green . DESeq2 first normalizes the count data to account for differences in library sizes and RNA composition between samples. Then, we will use the normalized counts to make some plots for QC at the gene and sample level. The final step is to use the appropriate functions from the DESeq2 package to perform the differential expression analysis. We will go in-depth into each of these steps in the following lessons, but additional details and helpful suggestions regarding DESeq2 can be found in the DESeq2 vignette . As you go through this workflow and questions arise, you can reference the vignette from within RStudio: vignette(\"DESeq2\") This is very convenient, as it provides a wealth of information at your fingertips! Be sure to use this as you need during the workshop. This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Count matrix"},{"location":"05b_count_matrix.html#learning-objectives","text":"Describe how to set up an RNA-seq project in R Describe the RNA-seq and the differential gene expression analysis workflow Explain why negative binomial distribution is used to model RNA-seq count data","title":"Learning Objectives"},{"location":"05b_count_matrix.html#differential-gene-expression-dge-analysis-overview","text":"The goal of RNA-seq is often to perform differential expression testing to determine which genes are expressed at different levels between conditions. These genes can offer biological insight into the processes affected by the condition(s) of interest. To determine the expression levels of genes, our RNA-seq workflow followed the steps detailed in the image below. All steps were performed on the command line (Linux/Unix) through the generation of the read counts per gene. The differential expression analysis and any downstream functional analysis are generally performed in R using R packages specifically designed for the complex statistical analyses required to determine whether genes are differentially expressed. In the next few lessons, we will walk you through an end-to-end gene-level RNA-seq differential expression workflow using various R packages. We will start with the count matrix, do some exploratory data analysis for quality assessment and explore the relationship between samples. Next, we will perform differential expression analysis, and visually explore the results prior to performing downstream functional analysis.","title":"Differential gene expression (DGE) analysis overview"},{"location":"05b_count_matrix.html#setting-up","text":"Before we get into the details of the analysis, let\u2019s get started by opening up RStudio and setting up a new project for this analysis. Go to the File menu and select New Project . In the New Project window, choose Existing Directory . Then, choose introduction_bulkRNAseq_analysis as your project working directory. The new project should automatically open in RStudio. To check whether or not you are in the correct working directory, use getwd() . The path /work/introduction_bulkRNAseq_analysis should be returned to you in the console. When finished your working directory should now look similar to this: Inside the folder Notebooks you will find the scripts (in Rmd format) that we will follow during the sessions. In the folder Results you will save the results of your scripts, analysis and tests. To avoid copying the original dataset for each student (very inefficient) the dataset is contained inside the shared folder /work/introduction_bulkRNAseq_analysis/Data/ . Do not attempt to modify this folder, as it might mess up the files for the rest of your colleagues. Now you can open the first practical session: 06a_count_matrix.Rmd","title":"Setting up"},{"location":"05b_count_matrix.html#loading-libraries","text":"For this analysis we will be using several R packages, some which have been installed from CRAN and others from Bioconductor. To use these packages (and the functions contained within them), we need to load the libraries. Add the following to your script and don\u2019t forget to comment liberally! ## Setup ### Bioconductor and CRAN libraries used library ( tidyverse ) library ( RColorBrewer ) library ( DESeq2 ) library ( pheatmap ) library ( DEGreport ) library ( ggrepel )","title":"Loading libraries"},{"location":"05b_count_matrix.html#loading-data","text":"To load the data into our current environment, we will be using the read.table function. By default the function expects tab-delimited files, which is what we have. ## Load in data data <- read_table ( \"/work/introduction_bulkRNAseq_analysis/Data/Mov10_full_counts.txt\" ) meta <- read_table ( \"/work/introduction_bulkRNAseq_analysis/Data/Mov10_full_meta.txt\" ) Use class() to inspect our data and make sure we are working with data frames: ### Check classes of the data we just brought in class ( meta ) class ( data )","title":"Loading data"},{"location":"05b_count_matrix.html#viewing-data","text":"Make sure your datasets contain the expected samples / information before proceeding to perfom any type of analysis. View ( meta ) View ( data )","title":"Viewing data"},{"location":"05b_count_matrix.html#using-the-abundance-estimates-from-salmon-as-input-to-deseq2","text":"The counts used in these lessons were generated using the standard approach for RNA-seq analysis, where samples were aligned to the genome using a splice-aware aligner followed by counting. If you are using lightweight algorithms such as Salmon, Sailfish or Kallisto to generate abundance estimates, you can also use DESeq2 to perform gene-level differential expression analysis. These transcript abundance estimates, often referred to as \u2018pseudocounts\u2019, can be converted for use with DESeq2 but the setup is slightly more involved. If you are interested in knowing more about using Salmon pseudocounts for DESeq2, there are materials linked here .","title":"Using the abundance estimates from Salmon as input to DESeq2"},{"location":"05b_count_matrix.html#differential-gene-expression-analysis-overview","text":"So, what does this count data actually represent? The count data used for differential expression analysis represents the number of sequence reads that originated from a particular gene. The higher the number of counts, the more reads associated with that gene, and the assumption that there was a higher level of expression of that gene in the sample. With differential expression analysis, we are looking for genes that change in expression between two or more groups (defined in the metadata) - case vs. control - correlation of expression with some variable or clinical outcome Why does it not work to identify differentially expressed gene by ranking the genes by how different they are between the two groups (based on fold change values)? Genes that vary in expression level between groups of samples may do so solely as a consequence of the biological variable(s) of interest. However, this difference is often also related to extraneous effects, in fact, sometimes these effects exclusively account for the observed variation. The goal of differential expression analysis to determine the relative role of these effects, hence separating the \u201cinteresting\u201d variance from the \u201cuninteresting\u201d variance. Although the mean expression levels between sample groups may appear to be quite different, it is possible that the difference is not actually significant. This is illustrated for \u2018GeneA\u2019 expression between \u2018untreated\u2019 and \u2018treated\u2019 groups in the figure below. The mean expression level of geneA for the \u2018treated\u2019 group is twice as large as for the \u2018untreated\u2019 group, but the variation between replicates indicates that this may not be a significant difference. We need to take into account the variation in the data (and where it might be coming from) when determining whether genes are differentially expressed. Differential expression analysis is used to determine, for each gene, whether the differences in expression (counts) between groups is significant given the amount of variation observed within groups (replicates). To test for significance, we need an appropriate statistical model that accurately performs normalization (to account for differences in sequencing depth, etc.) and variance modeling (to account for few numbers of replicates and large dynamic expression range).","title":"Differential gene expression analysis overview"},{"location":"05b_count_matrix.html#rna-seq-count-distribution","text":"To determine the appropriate statistical model, we need information about the distribution of counts. To get an idea about how RNA-seq counts are distributed, let\u2019s plot the counts of all the samples: pdata <- data %>% gather ( key = Sample , value = Count , - GeneSymbol ) pdata ggplot ( pdata ) + geom_histogram ( aes ( x = Count ), stat = \"bin\" , bins = 200 ) + xlab ( \"Raw expression counts\" ) + ylab ( \"Number of genes\" ) If we zoom in close to zero, we can see a large number of genes with counts of zero: ggplot ( pdata ) + geom_histogram ( aes ( x = Count ), stat = \"bin\" , bins = 200 ) + xlim ( -5 , 500 ) + xlab ( \"Raw expression counts\" ) + ylab ( \"Number of genes\" ) These images illustrate some common features of RNA-seq count data, including a low number of counts associated with a large proportion of genes , and a long right tail due to the lack of any upper limit for expression . Unlike microarray data, which has a dynamic range maximum limited due to when the probes max out, there is no limit of maximum expression for RNA-seq data. Due to the differences in these technologies, the statistical models used to fit the data are different between the two methods. NOTE: The log intensities of the microarray data approximate a normal distribution. However, due to the different properties of the of RNA-seq count data, such as integer counts instead of continuous measurements and non-normally distributed data, the normal distribution does not accurately model RNA-seq counts [ 1 ].","title":"RNA-seq count distribution"},{"location":"05b_count_matrix.html#modeling-count-data","text":"RNAseq count data can be modeled using a Poisson distribution . this particular distribution is fitting for data where the number of cases is very large but the probability of an event occurring is very small . To give you an example, think of the lottery: many people buy lottery tickets (high number of cases), but only very few win (the probability of the event is small). Check this video from Rafael Irizarry in the EdX class for more details . With RNA-Seq data, a very large number of RNAs are represented and the probability of pulling out a particular transcript is very small . Thus, it would be an appropriate situation to use the Poisson distribution. However, a unique property of this distribution is that the mean == variance. Realistically, with RNA-Seq data there is always some biological variation present across the replicates (within a sample class). Genes with larger average expression levels will tend to have larger observed variances across replicates. The model that fits best, given this type of variability observed for replicates, is the Negative Binomial (NB) model . Essentially, the NB model is a good approximation for data where the mean \\< variance , as is the case with RNA-Seq count data. NOTE: Biological replicates represent multiple samples (i.e. RNA from different mice) representing the same sample class Technical replicates represent the same sample (i.e. RNA from the same mouse) but with technical steps replicated Usually biological variance is much greater than technical variance, so we do not need to account for technical variance to identify biological differences in expression Don\u2019t spend money on technical replicates - biological replicates are much more useful NOTE: If you are using cell lines and are unsure whether or not you have prepared biological or technical replicates, take a look at this link . This is a useful resource in helping you determine how best to set up your in-vitro experiment. **How do I know if my data should be modeled using the Poisson distribution or Negative Binomial distribution??** If it\u2019s count data, it should fit the negative binomial, as discussed previously. However, it can be helpful to plot the *mean versus the variance* of your data. *Remember for the Poisson model, mean = variance, but for NB, mean \\ < variance.* Run the following code to plot the *mean versus variance* for our data: df <- data %>% rowwise () %>% summarise ( mean_counts = mean ( Mov10_kd_2 : Irrel_kd_3 ), variance_counts = var ( Mov10_kd_2 : Irrel_kd_3 )) ggplot ( df ) + geom_point ( aes ( x = mean_counts , y = variance_counts )) + geom_abline ( intercept = 0 , slope = 1 , color = \"red\" ) + scale_y_log10 () + scale_x_log10 () Note that in the above figure, the variance across replicates tends to be greater than the mean (red line), especially for genes with large mean expression levels. *This is a good indication that our data do not fit the Poisson distribution and we need to account for this increase in variance using the Negative Binomial model (i.e. Poisson will underestimate variability leading to an increase in false positive DE genes).*","title":"Modeling count data"},{"location":"05b_count_matrix.html#improving-mean-estimates-ie-reducing-variance-with-biological-replicates","text":"The variance or scatter tends to reduce as we increase the number of biological replicates ( the distribution will approach the Poisson distribution with increasing numbers of replicates ), since standard deviations of averages are smaller than standard deviations of individual observations. The value of additional replicates is that as you add more data (replicates), you get increasingly precise estimates of group means, and ultimately greater confidence in the ability to distinguish differences between sample classes (i.e. more DE genes). The figure below illustrates the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified (from Liu et al. (2013) : Note that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth . Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. Generally, the minimum sequencing depth recommended is 20-30 million reads per sample, but we have seen good RNA-seq experiments with 10 million reads if there are a good number of replicates.","title":"Improving mean estimates (i.e.\u00a0reducing variance) with biological replicates"},{"location":"05b_count_matrix.html#differential-expression-analysis-workflow","text":"To model counts appropriately when performing a differential expression analysis, there are a number of software packages that have been developed for differential expression analysis of RNA-seq data. Even as new methods are continuously being developed a few tools are generally recommended as best practice, like DESeq2 , EdgeR and Limma-Voom . Many studies describing comparisons between these methods show that while there is some agreement, there is also much variability between tools. Additionally, there is no one method that performs optimally under all conditions ( Soneson and Dleorenzi, 2013 , Corchete et al, 2020 ) . We will be using DESeq2 for the DE analysis, and the analysis steps with DESeq2 are shown in the flowchart below in green . DESeq2 first normalizes the count data to account for differences in library sizes and RNA composition between samples. Then, we will use the normalized counts to make some plots for QC at the gene and sample level. The final step is to use the appropriate functions from the DESeq2 package to perform the differential expression analysis. We will go in-depth into each of these steps in the following lessons, but additional details and helpful suggestions regarding DESeq2 can be found in the DESeq2 vignette . As you go through this workflow and questions arise, you can reference the vignette from within RStudio: vignette(\"DESeq2\") This is very convenient, as it provides a wealth of information at your fingertips! Be sure to use this as you need during the workshop. This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Differential expression analysis workflow"},{"location":"05c_count_normalization.html","text":"Approximate time: 40 minutes Learning Objectives \u00b6 Explore different types of normalization methods Become familiar with the DESeqDataSet object Understand how to normalize counts using DESeq2 Normalization \u00b6 The first step in the DE analysis workflow is count normalization, which is necessary to make accurate comparisons of gene expression between samples. The counts of mapped reads for each gene is proportional to the expression of RNA (\u201cinteresting\u201d) in addition to many other factors (\u201cuninteresting\u201d). Normalization is the process of scaling raw count values to account for the \u201cuninteresting\u201d factors. In this way the expression levels are more comparable between and/or within samples. The main factors often considered during normalization are: Sequencing depth: Accounting for sequencing depth is necessary for comparison of gene expression between samples. In the example below, each gene appears to have doubled in expression in Sample A relative to Sample B , however this is a consequence of Sample A having double the sequencing depth. >***NOTE:** In the figure above, each pink and green rectangle represents a read aligned to a gene. Reads connected by dashed lines connect a read spanning an intron.* Gene length: Accounting for gene length is necessary for comparing expression between different genes within the same sample. In the example, Gene X and Gene Y have similar levels of expression, but the number of reads mapped to Gene X would be many more than the number mapped to Gene Y because Gene X is longer. GC-content : Genomic features such as GC-content may result in a read count biases, as GC-rich and GC-poor fragments are under-represented in RNAseq experiments. This under-representation is attributed to the fact that fragments with high and low GC-content are not adequately amplified in a standard high throughput sequencing protocol and, subsequently, that the fragments are difficult to align (correctly) to refence genome, i.e. less unique, repeat regions, etc. ([Benjamini & Speed, 2012] ( https://academic.oup.com/nar/article/40/10/e72/2411059 ) and [Risso et al, 2011] ( https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-480 )). RNA composition: A few highly differentially expressed genes between samples, differences in the number of genes expressed between samples, or presence of contamination can skew some types of normalization methods. Accounting for RNA composition is recommended for accurate comparison of expression between samples, and is particularly important when performing differential expression analyses [Anders & Huber, 2010] ( https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-10-r106 ). In the example, if we were to divide each sample by the total number of counts to normalize, the counts would be greatly skewed by the DE gene, which takes up most of the counts for Sample A , but not Sample B . Most other genes for Sample A would be divided by the larger number of total counts and appear to be less expressed than those same genes in Sample B . While normalization is essential for differential expression analyses, it is also necessary for exploratory data analysis, visualization of data, and whenever you are exploring or comparing counts between or within samples. Common normalization methods \u00b6 Several common normalization methods exist to account for these differences: Normalization method Description Accounted factors Recommendations for use CPM (counts per million) counts scaled by total number of reads sequencing depth gene count comparisons between replicates of the same samplegroup; NOT for within sample comparisons or DE analysis TPM (transcripts per kilobase million) counts per length of transcript (kb) per million reads mapped sequencing depth and gene length gene count comparisons within a sample or between samples of the same sample group; NOT for DE analysis RPKM/FPKM (reads/fragments per kilobase of exon per million reads/fragments mapped) similar to TPM sequencing depth and gene length gene count comparisons between genes within a sample; NOT for between sample comparisons or DE analysis DESeq2\u2019s median of ratios counts divided by sample-specific size factors determined by median ratio of gene counts relative to geometric mean per gene sequencing depth and RNA composition gene count comparisons between samples and for DE analysis ; NOT for within sample comparisons EdgeR\u2019s trimmed mean of M values (TMM) uses a weighted trimmed mean of the log expression ratios between samples sequencing depth, RNA composition gene count comparisons between samples and for DE analysis ; NOT for within sample comparisons RPKM/FPKM (not recommended) \u00b6 While TPM and RPKM/FPKM normalization methods both account for sequencing depth and gene length, RPKM/FPKM are not recommended. The reason is that the normalized count values output by the RPKM/FPKM method are not comparable between samples. Using RPKM/FPKM normalization, the total number of RPKM/FPKM normalized counts for each sample will be different. Therefore, you cannot compare the normalized counts for each gene equally between samples. RPKM-normalized counts table gene sampleA sampleB XCR1 5.5 5.5 WASHC1 73.4 21.8 \u2026 \u2026 \u2026 Total RPKM-normalized counts 1,000,000 1,500,000 For example, in the table above, SampleA has a greater proportion of counts associated with XCR1 (5.5/1,000,000) than does sampleB (5.5/1,500,000) even though the RPKM count values are the same. Therefore, we cannot directly compare the counts for XCR1 (or any other gene) between sampleA and sampleB because the total number of normalized counts are different between samples. DESeq2-normalized counts: Median of ratios method \u00b6 Since tools for differential expression analysis are comparing the counts between sample groups for the same gene, gene length does not need to be accounted for by the tool. However, sequencing depth and RNA composition do need to be taken into account. To normalize for sequencing depth and RNA composition, DESeq2 uses the median of ratios method. On the user-end there is only one step, but on the back-end there are multiple steps involved, as described below. NOTE: The steps below describe in detail some of the steps performed by DESeq2 when you run a single function to get DE genes. Basically, for a typical RNA-seq analysis, you would not run these steps individually . Step 1: creates a pseudo-reference sample (row-wise geometric mean) For each gene, a pseudo-reference sample is created that is equal to the geometric mean across all samples. gene sampleA sampleB pseudo-reference sample EF2A 1489 906 sqrt(1489 * 906) = 1161.5 ABCD1 22 13 sqrt(22 * 13) = 17.7 \u2026 \u2026 \u2026 \u2026 Step 2: calculates ratio of each sample to the reference For every gene in a sample, the ratios (sample/ref) are calculated (as shown below). This is performed for each sample in the dataset. Since the majority of genes are not differentially expressed, the majority of genes in each sample should have similar ratios within the sample. gene sampleA sampleB pseudo-reference sample ratio of sampleA/ref ratio of sampleB/ref EF2A 1489 906 1161.5 1489/1161.5 = 1.28 906/1161.5 = 0.78 ABCD1 22 13 16.9 22/16.9 = 1.30 13/16.9 = 0.77 MEFV 793 410 570.2 793/570.2 = 1.39 410/570.2 = 0.72 BAG1 76 42 56.5 76/56.5 = 1.35 42/56.5 = 0.74 MOV10 521 1196 883.7 521/883.7 = 0.590 1196/883.7 = 1.35 \u2026 \u2026 \u2026 \u2026 Step 3: calculate the normalization factor for each sample (size factor) The median value (column-wise for the above table) of all ratios for a given sample is taken as the normalization factor (size factor) for that sample, as calculated below. Notice that the differentially expressed genes should not affect the median value: normalization_factor_sampleA <- median(c(1.28, 1.3, 1.39, 1.35, 0.59)) normalization_factor_sampleB <- median(c(0.78, 0.77, 0.72, 0.74, 1.35)) The figure below illustrates the median value for the distribution of all gene ratios for a single sample (frequency is on the y-axis). The median of ratios method assumes that not ALL genes are differentially expressed; therefore, the normalization factors should account for sequencing depth and RNA composition of the sample (large outlier genes will not represent the median ratio values). This method is robust to imbalance in up-/down-regulation and large numbers of differentially expressed genes. Usually, these size factors are around 1, if you see large variations between samples it is important to take note since it might indicate the presence of extreme outliers. Step 4: calculate the normalized count values using the normalization factor This is performed by dividing each raw count value in a given sample by that sample\u2019s normalization factor to generate normalized count values. This is performed for all count values (every gene in every sample). For example, if the median ratio for SampleA was 1.3 and the median ratio for SampleB was 0.77, you could calculate normalized counts as follows: SampleA median ratio = 1.3 SampleB median ratio = 0.77 Raw Counts gene sampleA sampleB EF2A 1489 906 ABCD1 22 13 \u2026 \u2026 \u2026 Normalized Counts gene sampleA sampleB EF2A 1489 / 1.3 = 1145.39 906 / 0.77 = 1176.62 ABCD1 22 / 1.3 = 16.92 13 / 0.77 = 16.88 \u2026 \u2026 \u2026 Please note that normalized count values are not whole numbers. Exercise 1 Determine the normalized (median of ratios) counts for your gene of interest, PD1, given the raw counts and size factors below. NOTE: You will need to run the code below to generate the raw counts dataframe (PD1) and the size factor vector (size_factors), then use these objects to determine the normalized counts values: # Raw counts for PD1 PD1 <- t ( c ( 21 , 58 , 17 , 97 , 83 , 10 )) %>% as_tibble () %>% rename_all ( ~ paste0 ( \"Sample\" , 1 : 6 )) # Size factors for each sample size_factors <- c ( 1.32 , 0.70 , 1.04 , 1.27 , 1.11 , 0.85 ) Count normalization of Mov10 dataset using DESeq2 \u00b6 Now that we know the theory of count normalization, we will normalize the counts for the Mov10 dataset using DESeq2. This requires a few steps: Ensure the row names of the metadata dataframe are present and in the same order as the column names of the counts dataframe. Create a DESeqDataSet object Generate the normalized counts 1. Match the metadata and counts data \u00b6 We should always make sure that we have sample names that match between the two files, and that the samples are in the right order. DESeq2 will output an error if this is not the case. We are using tibbles, which unfortunately do not support row names, but DESeq uses rownames, so we need to do some work around it. ### Check that sample names match in both files all ( meta $ samplename %in% colnames ( data )) all ( meta $ samplename == colnames ( data )[ -1 ]) If your data did not match, you could use the match() function to rearrange them to be matching. Exercise 2 Suppose we had sample names matching in the counts matrix and metadata file, but they were out of order. Write the line(s) of code required make the data_random dataframe with columns ordered such that they were identical to the row names of the metadata. Remember that data contains the column GeneSymbol with the names of the genes! # randomize count data columns and metadata rownames meta_random <- meta [ sample ( 1 : nrow ( meta )),] data_random <- data [, c ( 1 , sample ( 2 : ncol ( data )))] # your code here 2. Create DESEq2 object \u00b6 Bioconductor software packages often define and use a custom class within R for storing data (input data, intermediate data and also results). These custom data structures are similar to lists in that they can contain multiple different data types/structures within them. But, unlike lists they have pre-specified data slots , which hold specific types/classes of data. The data stored in these pre-specified slots can be accessed by using specific package-defined functions. Let\u2019s start by creating the DESeqDataSet object and then we can talk a bit more about what is stored inside it. To create the object we will need the count matrix and the metadata table as input. We will also need to specify a design formula . The design formula specifies the column(s) in the metadata table and how they should be used in the analysis. For our dataset we only have one column we are interested in, that is ~sampletype . This column has three factor levels, which tells DESeq2 that for each gene we want to evaluate gene expression change with respect to these different levels. ## Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( countData = data.frame ( data [, -1 ], row.names = data $ GeneSymbol ), colData = data.frame ( meta [, -1 ], row.names = meta $ samplename ), design = ~ sampletype ) You can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts() ( Note: we nested it within the View() function so that rather than getting printed in the console we can see it in the script editor ) : View ( counts ( dds )) As we go through the workflow we will use the relevant functions to check what information gets stored inside our object. 3. Generate the Mov10 normalized counts \u00b6 The next step is to normalize the count data in order to be able to make fair gene comparisons between samples. To perform the median of ratios method of normalization, DESeq2 has a single estimateSizeFactors() function that will generate size factors for us. We will use the function in the example below, but in a typical RNA-seq analysis this step is automatically performed by the DESeq() function , which we will see later. dds <- estimateSizeFactors ( dds ) By assigning the results back to the dds object we are filling in the slots of the DESeqDataSet object with the appropriate information. We can take a look at the normalization factor applied to each sample using: sizeFactors ( dds ) Now, to retrieve the normalized counts matrix from dds , we use the counts() function and add the argument normalized=TRUE . normalized_counts <- counts ( dds , normalized = TRUE ) We can save this normalized data matrix to file for later use: write.table ( normalized_counts , file = \"/work/introduction_bulkRNAseq_analysis/Results/normalized_counts.txt\" , sep = \"\\t\" , quote = F ) NOTE: DESeq2 doesn\u2019t actually use normalized counts, rather it uses the raw counts and models the normalization inside the Generalized Linear Model (GLM). These normalized counts will be useful for downstream visualization of results, but cannot be used as input to DESeq2 or any other tools that perform differential expression analysis which use the negative binomial model. This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Normalization"},{"location":"05c_count_normalization.html#learning-objectives","text":"Explore different types of normalization methods Become familiar with the DESeqDataSet object Understand how to normalize counts using DESeq2","title":"Learning Objectives"},{"location":"05c_count_normalization.html#normalization","text":"The first step in the DE analysis workflow is count normalization, which is necessary to make accurate comparisons of gene expression between samples. The counts of mapped reads for each gene is proportional to the expression of RNA (\u201cinteresting\u201d) in addition to many other factors (\u201cuninteresting\u201d). Normalization is the process of scaling raw count values to account for the \u201cuninteresting\u201d factors. In this way the expression levels are more comparable between and/or within samples. The main factors often considered during normalization are: Sequencing depth: Accounting for sequencing depth is necessary for comparison of gene expression between samples. In the example below, each gene appears to have doubled in expression in Sample A relative to Sample B , however this is a consequence of Sample A having double the sequencing depth. >***NOTE:** In the figure above, each pink and green rectangle represents a read aligned to a gene. Reads connected by dashed lines connect a read spanning an intron.* Gene length: Accounting for gene length is necessary for comparing expression between different genes within the same sample. In the example, Gene X and Gene Y have similar levels of expression, but the number of reads mapped to Gene X would be many more than the number mapped to Gene Y because Gene X is longer. GC-content : Genomic features such as GC-content may result in a read count biases, as GC-rich and GC-poor fragments are under-represented in RNAseq experiments. This under-representation is attributed to the fact that fragments with high and low GC-content are not adequately amplified in a standard high throughput sequencing protocol and, subsequently, that the fragments are difficult to align (correctly) to refence genome, i.e. less unique, repeat regions, etc. ([Benjamini & Speed, 2012] ( https://academic.oup.com/nar/article/40/10/e72/2411059 ) and [Risso et al, 2011] ( https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-480 )). RNA composition: A few highly differentially expressed genes between samples, differences in the number of genes expressed between samples, or presence of contamination can skew some types of normalization methods. Accounting for RNA composition is recommended for accurate comparison of expression between samples, and is particularly important when performing differential expression analyses [Anders & Huber, 2010] ( https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-10-r106 ). In the example, if we were to divide each sample by the total number of counts to normalize, the counts would be greatly skewed by the DE gene, which takes up most of the counts for Sample A , but not Sample B . Most other genes for Sample A would be divided by the larger number of total counts and appear to be less expressed than those same genes in Sample B . While normalization is essential for differential expression analyses, it is also necessary for exploratory data analysis, visualization of data, and whenever you are exploring or comparing counts between or within samples.","title":"Normalization"},{"location":"05c_count_normalization.html#common-normalization-methods","text":"Several common normalization methods exist to account for these differences: Normalization method Description Accounted factors Recommendations for use CPM (counts per million) counts scaled by total number of reads sequencing depth gene count comparisons between replicates of the same samplegroup; NOT for within sample comparisons or DE analysis TPM (transcripts per kilobase million) counts per length of transcript (kb) per million reads mapped sequencing depth and gene length gene count comparisons within a sample or between samples of the same sample group; NOT for DE analysis RPKM/FPKM (reads/fragments per kilobase of exon per million reads/fragments mapped) similar to TPM sequencing depth and gene length gene count comparisons between genes within a sample; NOT for between sample comparisons or DE analysis DESeq2\u2019s median of ratios counts divided by sample-specific size factors determined by median ratio of gene counts relative to geometric mean per gene sequencing depth and RNA composition gene count comparisons between samples and for DE analysis ; NOT for within sample comparisons EdgeR\u2019s trimmed mean of M values (TMM) uses a weighted trimmed mean of the log expression ratios between samples sequencing depth, RNA composition gene count comparisons between samples and for DE analysis ; NOT for within sample comparisons","title":"Common normalization methods"},{"location":"05c_count_normalization.html#rpkmfpkm-not-recommended","text":"While TPM and RPKM/FPKM normalization methods both account for sequencing depth and gene length, RPKM/FPKM are not recommended. The reason is that the normalized count values output by the RPKM/FPKM method are not comparable between samples. Using RPKM/FPKM normalization, the total number of RPKM/FPKM normalized counts for each sample will be different. Therefore, you cannot compare the normalized counts for each gene equally between samples. RPKM-normalized counts table gene sampleA sampleB XCR1 5.5 5.5 WASHC1 73.4 21.8 \u2026 \u2026 \u2026 Total RPKM-normalized counts 1,000,000 1,500,000 For example, in the table above, SampleA has a greater proportion of counts associated with XCR1 (5.5/1,000,000) than does sampleB (5.5/1,500,000) even though the RPKM count values are the same. Therefore, we cannot directly compare the counts for XCR1 (or any other gene) between sampleA and sampleB because the total number of normalized counts are different between samples.","title":"RPKM/FPKM (not recommended)"},{"location":"05c_count_normalization.html#deseq2-normalized-counts-median-of-ratios-method","text":"Since tools for differential expression analysis are comparing the counts between sample groups for the same gene, gene length does not need to be accounted for by the tool. However, sequencing depth and RNA composition do need to be taken into account. To normalize for sequencing depth and RNA composition, DESeq2 uses the median of ratios method. On the user-end there is only one step, but on the back-end there are multiple steps involved, as described below. NOTE: The steps below describe in detail some of the steps performed by DESeq2 when you run a single function to get DE genes. Basically, for a typical RNA-seq analysis, you would not run these steps individually . Step 1: creates a pseudo-reference sample (row-wise geometric mean) For each gene, a pseudo-reference sample is created that is equal to the geometric mean across all samples. gene sampleA sampleB pseudo-reference sample EF2A 1489 906 sqrt(1489 * 906) = 1161.5 ABCD1 22 13 sqrt(22 * 13) = 17.7 \u2026 \u2026 \u2026 \u2026 Step 2: calculates ratio of each sample to the reference For every gene in a sample, the ratios (sample/ref) are calculated (as shown below). This is performed for each sample in the dataset. Since the majority of genes are not differentially expressed, the majority of genes in each sample should have similar ratios within the sample. gene sampleA sampleB pseudo-reference sample ratio of sampleA/ref ratio of sampleB/ref EF2A 1489 906 1161.5 1489/1161.5 = 1.28 906/1161.5 = 0.78 ABCD1 22 13 16.9 22/16.9 = 1.30 13/16.9 = 0.77 MEFV 793 410 570.2 793/570.2 = 1.39 410/570.2 = 0.72 BAG1 76 42 56.5 76/56.5 = 1.35 42/56.5 = 0.74 MOV10 521 1196 883.7 521/883.7 = 0.590 1196/883.7 = 1.35 \u2026 \u2026 \u2026 \u2026 Step 3: calculate the normalization factor for each sample (size factor) The median value (column-wise for the above table) of all ratios for a given sample is taken as the normalization factor (size factor) for that sample, as calculated below. Notice that the differentially expressed genes should not affect the median value: normalization_factor_sampleA <- median(c(1.28, 1.3, 1.39, 1.35, 0.59)) normalization_factor_sampleB <- median(c(0.78, 0.77, 0.72, 0.74, 1.35)) The figure below illustrates the median value for the distribution of all gene ratios for a single sample (frequency is on the y-axis). The median of ratios method assumes that not ALL genes are differentially expressed; therefore, the normalization factors should account for sequencing depth and RNA composition of the sample (large outlier genes will not represent the median ratio values). This method is robust to imbalance in up-/down-regulation and large numbers of differentially expressed genes. Usually, these size factors are around 1, if you see large variations between samples it is important to take note since it might indicate the presence of extreme outliers. Step 4: calculate the normalized count values using the normalization factor This is performed by dividing each raw count value in a given sample by that sample\u2019s normalization factor to generate normalized count values. This is performed for all count values (every gene in every sample). For example, if the median ratio for SampleA was 1.3 and the median ratio for SampleB was 0.77, you could calculate normalized counts as follows: SampleA median ratio = 1.3 SampleB median ratio = 0.77 Raw Counts gene sampleA sampleB EF2A 1489 906 ABCD1 22 13 \u2026 \u2026 \u2026 Normalized Counts gene sampleA sampleB EF2A 1489 / 1.3 = 1145.39 906 / 0.77 = 1176.62 ABCD1 22 / 1.3 = 16.92 13 / 0.77 = 16.88 \u2026 \u2026 \u2026 Please note that normalized count values are not whole numbers. Exercise 1 Determine the normalized (median of ratios) counts for your gene of interest, PD1, given the raw counts and size factors below. NOTE: You will need to run the code below to generate the raw counts dataframe (PD1) and the size factor vector (size_factors), then use these objects to determine the normalized counts values: # Raw counts for PD1 PD1 <- t ( c ( 21 , 58 , 17 , 97 , 83 , 10 )) %>% as_tibble () %>% rename_all ( ~ paste0 ( \"Sample\" , 1 : 6 )) # Size factors for each sample size_factors <- c ( 1.32 , 0.70 , 1.04 , 1.27 , 1.11 , 0.85 )","title":"DESeq2-normalized counts: Median of ratios method"},{"location":"05c_count_normalization.html#count-normalization-of-mov10-dataset-using-deseq2","text":"Now that we know the theory of count normalization, we will normalize the counts for the Mov10 dataset using DESeq2. This requires a few steps: Ensure the row names of the metadata dataframe are present and in the same order as the column names of the counts dataframe. Create a DESeqDataSet object Generate the normalized counts","title":"Count normalization of Mov10 dataset using DESeq2"},{"location":"05c_count_normalization.html#1-match-the-metadata-and-counts-data","text":"We should always make sure that we have sample names that match between the two files, and that the samples are in the right order. DESeq2 will output an error if this is not the case. We are using tibbles, which unfortunately do not support row names, but DESeq uses rownames, so we need to do some work around it. ### Check that sample names match in both files all ( meta $ samplename %in% colnames ( data )) all ( meta $ samplename == colnames ( data )[ -1 ]) If your data did not match, you could use the match() function to rearrange them to be matching. Exercise 2 Suppose we had sample names matching in the counts matrix and metadata file, but they were out of order. Write the line(s) of code required make the data_random dataframe with columns ordered such that they were identical to the row names of the metadata. Remember that data contains the column GeneSymbol with the names of the genes! # randomize count data columns and metadata rownames meta_random <- meta [ sample ( 1 : nrow ( meta )),] data_random <- data [, c ( 1 , sample ( 2 : ncol ( data )))] # your code here","title":"1. Match the metadata and counts data"},{"location":"05c_count_normalization.html#2-create-deseq2-object","text":"Bioconductor software packages often define and use a custom class within R for storing data (input data, intermediate data and also results). These custom data structures are similar to lists in that they can contain multiple different data types/structures within them. But, unlike lists they have pre-specified data slots , which hold specific types/classes of data. The data stored in these pre-specified slots can be accessed by using specific package-defined functions. Let\u2019s start by creating the DESeqDataSet object and then we can talk a bit more about what is stored inside it. To create the object we will need the count matrix and the metadata table as input. We will also need to specify a design formula . The design formula specifies the column(s) in the metadata table and how they should be used in the analysis. For our dataset we only have one column we are interested in, that is ~sampletype . This column has three factor levels, which tells DESeq2 that for each gene we want to evaluate gene expression change with respect to these different levels. ## Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( countData = data.frame ( data [, -1 ], row.names = data $ GeneSymbol ), colData = data.frame ( meta [, -1 ], row.names = meta $ samplename ), design = ~ sampletype ) You can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts() ( Note: we nested it within the View() function so that rather than getting printed in the console we can see it in the script editor ) : View ( counts ( dds )) As we go through the workflow we will use the relevant functions to check what information gets stored inside our object.","title":"2. Create DESEq2 object"},{"location":"05c_count_normalization.html#3-generate-the-mov10-normalized-counts","text":"The next step is to normalize the count data in order to be able to make fair gene comparisons between samples. To perform the median of ratios method of normalization, DESeq2 has a single estimateSizeFactors() function that will generate size factors for us. We will use the function in the example below, but in a typical RNA-seq analysis this step is automatically performed by the DESeq() function , which we will see later. dds <- estimateSizeFactors ( dds ) By assigning the results back to the dds object we are filling in the slots of the DESeqDataSet object with the appropriate information. We can take a look at the normalization factor applied to each sample using: sizeFactors ( dds ) Now, to retrieve the normalized counts matrix from dds , we use the counts() function and add the argument normalized=TRUE . normalized_counts <- counts ( dds , normalized = TRUE ) We can save this normalized data matrix to file for later use: write.table ( normalized_counts , file = \"/work/introduction_bulkRNAseq_analysis/Results/normalized_counts.txt\" , sep = \"\\t\" , quote = F ) NOTE: DESeq2 doesn\u2019t actually use normalized counts, rather it uses the raw counts and models the normalization inside the Generalized Linear Model (GLM). These normalized counts will be useful for downstream visualization of results, but cannot be used as input to DESeq2 or any other tools that perform differential expression analysis which use the negative binomial model. This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"3. Generate the Mov10 normalized counts"},{"location":"06_exploratory_analysis.html","text":"Approximate time: 80 minutes Learning Objectives \u00b6 Recognize the importance of methods for count data transformation Describe the PCA (principal component analysis) technique Interpret different examples of PCA plots Evaluate sample quality using PCA and hierarchical clustering Quality Control \u00b6 The next step in the DESeq2 workflow is QC, which includes sample-level and gene-level steps to perform QC checks on the count data to help us ensure that the samples/replicates look good. Sample-level QC \u00b6 A useful initial step in an RNA-seq analysis is often to assess overall similarity between samples: Which samples are similar to each other, which are different? Does this fit to the expectation from the experiment\u2019s design? What are the major sources of variation in the dataset? To explore the similarity of our samples, we will be performing sample-level QC using Principal Component Analysis (PCA) and hierarchical clustering methods. These methods/tools allow us to check how similar the replicates are to each other (clustering) and to make sure that the experimental condition is the major source of variation in the data. Sample-level QC can also help identify any samples behaving like outliers; we can further explore any potential outliers to determine whether they need to be removed prior to DE analysis. These unsupervised clustering methods are run using log2 transformed normalized counts . The log2 transformation improves the sample distances for clustering visualization , i.e., it reduces the impact of large outlier counts. Instead of using a classical log2 transform, we will be using the regularized log transform (rlog). This type of transformation helps to avoid any bias from the abundance of low-count genes; Note1 below explains this in more detail. Image adapted from \u201c Beginner\u2019s guide to using the DESeq2 package \u201d by Love, Anders and Huber, 2014 NOTE1: \u201cMany common statistical methods for exploratory analysis of multidimensional data, especially methods for clustering and ordination (e. g., principal-component analysis and the like), work best for (at least approximately) homoskedastic data; this means that the variance of an observable quantity (i.e., here, the expression strength of a gene) does not depend on the mean. In RNA-Seq data, however, variance grows with the mean. For example, if one performs PCA directly on a matrix of normalized read counts, the result typically depends only on the few most strongly expressed genes because they show the largest absolute differences between samples. A simple and often used strategy to avoid this is to take the logarithm of the normalized count values plus a small pseudocount; however, now the genes with low counts tend to dominate the results because, due to the strong Poisson noise inherent to small count values, they show the strongest relative differences between samples. As a solution, DESeq2 offers the regularized-logarithm transformation, or rlog for short. For genes with high counts, the rlog transformation differs not much from an ordinary log2 transformation. For genes with lower counts, however, the values are shrunken towards the genes\u2019 averages across all samples. Using an empirical Bayesian prior in the form of a ridge penality, this is done such that the rlog-transformed data are approximately homoskedastic.\u201d - From the \u201cBeginner\u2019s guide to using the DESeq2 package\u201d by Love, Anders and Huber, 2014 (the DESeq2 vignette is the updated version of this doc). NOTE2: The DESeq2 vignette suggests large datasets (100s of samples) to use the variance-stabilizing transformation (vst) instead of rlog for transformation of the counts, since the rlog function might take too long to run and the vst() function is faster with similar properties to rlog. Principal Component Analysis (PCA) \u00b6 Principal Component Analysis (PCA) is a technique used to represent and visualize the variation in a dataset of high dimensionality. The number of dimensions, d , in a dataset may be thought of as the number of variables it has, e.g., for an RNA-seq dataset with 20.000 different transcripts, d is 20.000. Principally, this means we would need a dimensional space of size d to fully represent that dataset. However, as we are only able to view and comprehend things in 1,2 or 3 dimensions, we would like to project this dataset into a lower dimensional space, a process called dimensionality reduction . This makes PCA is a very important technique used in the QC and analysis of both bulk and single-cell RNAseq data, specially because many their dimensions (transcripts) do not contain any information. To better understand how it works, please go through this YouTube video from StatQuest . After you have gone through the video, please proceed with the interpretation section below. Interpreting PCA plots \u00b6 Essentially, if two samples have similar levels of expression for the genes that contribute significantly to the variation represented by a given PC (Principal Component), they will be plotted close together on the axis that represents that PC. Therefore, we would expect that biological replicates to have similar scores (because our expectation is that the same genes are changing) and cluster together. This is easiest to understand by visualizing some example PCA plots. We have an example dataset and a few associated PCA plots below to get a feel for how to interpret them. The metadata for the experiment is displayed below. The main condition of interest is treatment . When visualizing on PC1 and PC2, we don\u2019t see the samples separate by treatment , so we decide to explore other sources of variation present in the data. We hope that we have included all possible known sources of variation in our metadata table, and we can use these factors to color the PCA plot. We start with the factor cage , but the cage factor does not seem to explain the variation on PC1 or PC2. Then, we color by the sex factor, which appears to separate samples on PC2. This is good information to take note of, as we can use it downstream to account for the variation due to sex in the model and regress it out. Next we explore the strain factor and find that it explains the variation on PC1. It\u2019s great that we have been able to identify the sources of variation for both PC1 and PC2. By accounting for it in our model, we should be able to detect more genes differentially expressed due to treatment . Worrisome about this plot is that we see two samples that do not cluster with the correct strain. This would indicate a likely sample swap and should be investigated to determine whether these samples are indeed the labeled strains. If we found there was a switch, we could swap the samples in the metadata. However, if we think they are labeled correctly or are unsure, we could just remove the samples from the dataset. Still we haven\u2019t found if treatment is a major source of variation after strain and sex . So, we explore PC3 and PC4 to see if treatment is driving the variation represented by either of these PCs. We find that the samples separate by treatment on PC3, and are optimistic about our DE analysis since our condition of interest, treatment , is separating on PC3 and we can regress out the variation driving PC1 and PC2. Depending on how much variation is explained by the first few principal components, you may want to explore more (i.e consider more components and plot pairwise combinations) . Even if your samples do not separate clearly by the experimental variable, you may still get biologically relevant results from the DE analysis. If you are expecting very small effect sizes, then it\u2019s possible the signal is drowned out by extraneous sources of variation. In situations where you can identify those sources, it is important to account for these in your model , as it provides more power to the tool for detecting DE genes. Hierarchical Clustering Heatmap \u00b6 Hierarchical clustering is another method for identifying correlation patterns in a dataset and potential sample outliers. A heatmap displays the correlation of gene expression for all pairwise combinations of samples in the dataset. The hierarchical tree along the axes indicates which samples are more similar to each other, i.e. cluster together. The color blocks at the top indicate substructure in the data, and you would expect to see your replicates cluster together as a block for each sample group. Our expectation would be that the samples cluster together similar to the groupings we\u2019ve observed in the PCA plot. In the example above, we see a clustering of wild-type (Wt) and knock-down (KD) cell line samples and we would be quite concerned that the \u2018Wt_3\u2019 and \u2018KD_3\u2019 samples are not clustering with the other replicates. Furthermore, since the majority of genes are not differentially expressed, we observe that the samples generally have high correlations with each other (values higher than 0.80). In this case, samples with correlations below 0.80 may indicate an outlier in your data and/or sample contamination. N.B It is important to stress that these is no universal cut-off for what is a good/bad correlation/distance score, it depends on the particular dataset. Mov10 quality assessment and exploratory analysis using DESeq2 \u00b6 Now that we have a good understanding of the QC steps normally employed for RNA-seq, let\u2019s implement them for the Mov10 dataset we are going to be working with. Transform normalized counts for the MOV10 dataset \u00b6 To improve the distances/clustering for the PCA and hierarchical clustering visualization methods , we need to moderate the variance across the mean by applying the rlog transformation to the normalized counts. The rlog transformation of the normalized counts is only necessary for these visualization methods during this quality assessment. We will not be using these transformed counts for determining differential expression. ### Transform counts for data visualization rld <- rlog ( dds , blind = TRUE ) The blind=TRUE argument is to make sure that the rlog() function does not take our sample groups into account - i.e. does the transformation in an unbiased manner. When performing quality assessment, it is important to include this option. The DESeq2 vignette has more details about this. The rlog() function returns a DESeqTransform object, another type of DESeq-specific object. The reason you don\u2019t just get a matrix of transformed values is because all of the parameters (i.e. size factors) that went into computing the rlog transform are stored in that object. We use this object to plot the PCA and hierarchical clustering figures for quality assessment. NOTE: The rlog() function can be a bit slow when you have e.g. > 20 samples. In these situations the vst() function is much faster and performs a similar transformation appropriate for use with plotPCA() . It\u2019s typically just a few seconds with vst() due to optimizations and the nature of the transformation. Principal component analysis (PCA) for the MOV10 dataset \u00b6 We are now ready for the QC steps, let\u2019s start with PCA! DESeq2 has a built-in function for generating PCA plots using ggplot2 under the hood. This is great because it saves us having to type out lines of code and having to fiddle with the different ggplot2 layers. In addition, it takes the rlog object as an input directly, hence saving us the trouble of extracting the relevant information from it. The function plotPCA() requires two arguments as input: a DESeqTransform object and the \u201cintgroup\u201d (interesting group), i.e. the name of the column in our metadata that has information about the experimental sample groups. ### Plot PCA plotPCA ( rld , intgroup = \"sampletype\" ) Exercise 1 : What does the above plot tell you about the similarity of samples? Does it fit the expectation from the experimental design? What do you think the %variance information (in the axes titles) tell you about the data in the context of the PCA? By default plotPCA() uses the top 500 most variable genes . You can change this by adding the ntop= argument and specifying how many of the genes you want the function to consider. NOTE: The plotPCA() function will only return the values for PC1 and PC2. If you would like to explore the additional PCs in your data or if you would like to identify genes that contribute most to the PCs, you can use the prcomp() function. For example, to plot any of the PCs we could run the following code: # Input is a matrix of log transformed values rld <- rlog ( dds , blind = T ) rld_mat <- assay ( rld ) pca <- prcomp ( t ( rld_mat )) # Create data frame with metadata and PC3 and PC4 values for input to ggplot df <- cbind ( meta , pca $ x ) ggplot ( df ) + geom_point ( aes ( x = PC3 , y = PC4 , color = sampletype )) Resources are available to learn how to do more complex inquiries using the PCs. Hierarchical Clustering for the MOV10 dataset \u00b6 There is no built-in function in DESeq2 for plotting the heatmap for diplaying the pairwise correlation between all the samples and the hierarchical clustering information; we will use the pheatmap() function from the pheatmap package. This function cannot use the DESeqTransform object as input, but requires a matrix or dataframe. So, the first thing to do is retrieve that information from the rld object using a function called assay() (from the SummarizedExperiment package) that converts the data in a DESeqTransform object to a simple 2-dimensional data structure (a matrix in this case). ### Extract the rlog matrix from the object rld_mat <- assay ( rld ) ## \"assay()\" is part of the \"SummarizedExperiment\" package which is a DESeq2 dependency and is loaded with the DESeq2 library Next, we need to compute the pairwise correlation values for all the samples. We can do this using the cor() function: ### Compute pairwise correlation values rld_cor <- as.matrix ( cor ( rld_mat )) ## cor() is a base R function Let\u2019s take a look at the column and row names of the correlation matrix. ## check the output of cor(), make note of the row names and column names head(rld_cor) head(meta) You will notice that they match the names we have given our samples in the metadata data frame we started with. It is important that these match, so we can use the annotation argument below to plot a color block across the top. This block enables easy visualization of the hierarchical clustering. Let\u2019s plot the heatmap! ### Load pheatmap package library ( pheatmap ) ### Plot heatmap using the correlation matrix and the metadata object pheatmap ( rld_cor , annotation_col = meta %>% column_to_rownames ( \"samplename\" )) When you plot using pheatmap() the hierarchical clustering information is used to place similar samples together and this information is represented by the tree structure along the axes. The annotation argument accepts a dataframe as input, in our case it is the meta data frame. Overall, we observe pretty high correlations across the board ( > 0.999) suggesting no outlying sample(s). Also, similar to the PCA plot you see the samples clustering together by sample group. Together, these plots suggest to us that the data are of good quality and we have the green light to proceed to differential expression analysis. NOTE: The pheatmap function has a number of different arguments that we can alter from default values to enhance the aesthetics of the plot. If you are curious and want to explore more, try running the code below. How does your plot change? Take a look through the help pages ( ?pheatmap ) and identify what each of the added arguments is contributing to the plot. heat.colors <- RColorBrewer :: brewer.pal ( 6 , \"Blues\" ) pheatmap ( rld_cor , annotation = meta , color = heat.colors , border_color = NA , fontsize = 10 , fontsize_row = 10 , height = 20 ) Curious about all of the available color palettes offered by the RColorBrewer package ? Try typing in your console display.brewer.all() and see what happens! This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Exploratory Analysis"},{"location":"06_exploratory_analysis.html#learning-objectives","text":"Recognize the importance of methods for count data transformation Describe the PCA (principal component analysis) technique Interpret different examples of PCA plots Evaluate sample quality using PCA and hierarchical clustering","title":"Learning Objectives"},{"location":"06_exploratory_analysis.html#quality-control","text":"The next step in the DESeq2 workflow is QC, which includes sample-level and gene-level steps to perform QC checks on the count data to help us ensure that the samples/replicates look good.","title":"Quality Control"},{"location":"06_exploratory_analysis.html#sample-level-qc","text":"A useful initial step in an RNA-seq analysis is often to assess overall similarity between samples: Which samples are similar to each other, which are different? Does this fit to the expectation from the experiment\u2019s design? What are the major sources of variation in the dataset? To explore the similarity of our samples, we will be performing sample-level QC using Principal Component Analysis (PCA) and hierarchical clustering methods. These methods/tools allow us to check how similar the replicates are to each other (clustering) and to make sure that the experimental condition is the major source of variation in the data. Sample-level QC can also help identify any samples behaving like outliers; we can further explore any potential outliers to determine whether they need to be removed prior to DE analysis. These unsupervised clustering methods are run using log2 transformed normalized counts . The log2 transformation improves the sample distances for clustering visualization , i.e., it reduces the impact of large outlier counts. Instead of using a classical log2 transform, we will be using the regularized log transform (rlog). This type of transformation helps to avoid any bias from the abundance of low-count genes; Note1 below explains this in more detail. Image adapted from \u201c Beginner\u2019s guide to using the DESeq2 package \u201d by Love, Anders and Huber, 2014 NOTE1: \u201cMany common statistical methods for exploratory analysis of multidimensional data, especially methods for clustering and ordination (e. g., principal-component analysis and the like), work best for (at least approximately) homoskedastic data; this means that the variance of an observable quantity (i.e., here, the expression strength of a gene) does not depend on the mean. In RNA-Seq data, however, variance grows with the mean. For example, if one performs PCA directly on a matrix of normalized read counts, the result typically depends only on the few most strongly expressed genes because they show the largest absolute differences between samples. A simple and often used strategy to avoid this is to take the logarithm of the normalized count values plus a small pseudocount; however, now the genes with low counts tend to dominate the results because, due to the strong Poisson noise inherent to small count values, they show the strongest relative differences between samples. As a solution, DESeq2 offers the regularized-logarithm transformation, or rlog for short. For genes with high counts, the rlog transformation differs not much from an ordinary log2 transformation. For genes with lower counts, however, the values are shrunken towards the genes\u2019 averages across all samples. Using an empirical Bayesian prior in the form of a ridge penality, this is done such that the rlog-transformed data are approximately homoskedastic.\u201d - From the \u201cBeginner\u2019s guide to using the DESeq2 package\u201d by Love, Anders and Huber, 2014 (the DESeq2 vignette is the updated version of this doc). NOTE2: The DESeq2 vignette suggests large datasets (100s of samples) to use the variance-stabilizing transformation (vst) instead of rlog for transformation of the counts, since the rlog function might take too long to run and the vst() function is faster with similar properties to rlog.","title":"Sample-level QC"},{"location":"06_exploratory_analysis.html#principal-component-analysis-pca","text":"Principal Component Analysis (PCA) is a technique used to represent and visualize the variation in a dataset of high dimensionality. The number of dimensions, d , in a dataset may be thought of as the number of variables it has, e.g., for an RNA-seq dataset with 20.000 different transcripts, d is 20.000. Principally, this means we would need a dimensional space of size d to fully represent that dataset. However, as we are only able to view and comprehend things in 1,2 or 3 dimensions, we would like to project this dataset into a lower dimensional space, a process called dimensionality reduction . This makes PCA is a very important technique used in the QC and analysis of both bulk and single-cell RNAseq data, specially because many their dimensions (transcripts) do not contain any information. To better understand how it works, please go through this YouTube video from StatQuest . After you have gone through the video, please proceed with the interpretation section below.","title":"Principal Component Analysis (PCA)"},{"location":"06_exploratory_analysis.html#interpreting-pca-plots","text":"Essentially, if two samples have similar levels of expression for the genes that contribute significantly to the variation represented by a given PC (Principal Component), they will be plotted close together on the axis that represents that PC. Therefore, we would expect that biological replicates to have similar scores (because our expectation is that the same genes are changing) and cluster together. This is easiest to understand by visualizing some example PCA plots. We have an example dataset and a few associated PCA plots below to get a feel for how to interpret them. The metadata for the experiment is displayed below. The main condition of interest is treatment . When visualizing on PC1 and PC2, we don\u2019t see the samples separate by treatment , so we decide to explore other sources of variation present in the data. We hope that we have included all possible known sources of variation in our metadata table, and we can use these factors to color the PCA plot. We start with the factor cage , but the cage factor does not seem to explain the variation on PC1 or PC2. Then, we color by the sex factor, which appears to separate samples on PC2. This is good information to take note of, as we can use it downstream to account for the variation due to sex in the model and regress it out. Next we explore the strain factor and find that it explains the variation on PC1. It\u2019s great that we have been able to identify the sources of variation for both PC1 and PC2. By accounting for it in our model, we should be able to detect more genes differentially expressed due to treatment . Worrisome about this plot is that we see two samples that do not cluster with the correct strain. This would indicate a likely sample swap and should be investigated to determine whether these samples are indeed the labeled strains. If we found there was a switch, we could swap the samples in the metadata. However, if we think they are labeled correctly or are unsure, we could just remove the samples from the dataset. Still we haven\u2019t found if treatment is a major source of variation after strain and sex . So, we explore PC3 and PC4 to see if treatment is driving the variation represented by either of these PCs. We find that the samples separate by treatment on PC3, and are optimistic about our DE analysis since our condition of interest, treatment , is separating on PC3 and we can regress out the variation driving PC1 and PC2. Depending on how much variation is explained by the first few principal components, you may want to explore more (i.e consider more components and plot pairwise combinations) . Even if your samples do not separate clearly by the experimental variable, you may still get biologically relevant results from the DE analysis. If you are expecting very small effect sizes, then it\u2019s possible the signal is drowned out by extraneous sources of variation. In situations where you can identify those sources, it is important to account for these in your model , as it provides more power to the tool for detecting DE genes.","title":"Interpreting PCA plots"},{"location":"06_exploratory_analysis.html#hierarchical-clustering-heatmap","text":"Hierarchical clustering is another method for identifying correlation patterns in a dataset and potential sample outliers. A heatmap displays the correlation of gene expression for all pairwise combinations of samples in the dataset. The hierarchical tree along the axes indicates which samples are more similar to each other, i.e. cluster together. The color blocks at the top indicate substructure in the data, and you would expect to see your replicates cluster together as a block for each sample group. Our expectation would be that the samples cluster together similar to the groupings we\u2019ve observed in the PCA plot. In the example above, we see a clustering of wild-type (Wt) and knock-down (KD) cell line samples and we would be quite concerned that the \u2018Wt_3\u2019 and \u2018KD_3\u2019 samples are not clustering with the other replicates. Furthermore, since the majority of genes are not differentially expressed, we observe that the samples generally have high correlations with each other (values higher than 0.80). In this case, samples with correlations below 0.80 may indicate an outlier in your data and/or sample contamination. N.B It is important to stress that these is no universal cut-off for what is a good/bad correlation/distance score, it depends on the particular dataset.","title":"Hierarchical Clustering Heatmap"},{"location":"06_exploratory_analysis.html#mov10-quality-assessment-and-exploratory-analysis-using-deseq2","text":"Now that we have a good understanding of the QC steps normally employed for RNA-seq, let\u2019s implement them for the Mov10 dataset we are going to be working with.","title":"Mov10 quality assessment and exploratory analysis using DESeq2"},{"location":"06_exploratory_analysis.html#transform-normalized-counts-for-the-mov10-dataset","text":"To improve the distances/clustering for the PCA and hierarchical clustering visualization methods , we need to moderate the variance across the mean by applying the rlog transformation to the normalized counts. The rlog transformation of the normalized counts is only necessary for these visualization methods during this quality assessment. We will not be using these transformed counts for determining differential expression. ### Transform counts for data visualization rld <- rlog ( dds , blind = TRUE ) The blind=TRUE argument is to make sure that the rlog() function does not take our sample groups into account - i.e. does the transformation in an unbiased manner. When performing quality assessment, it is important to include this option. The DESeq2 vignette has more details about this. The rlog() function returns a DESeqTransform object, another type of DESeq-specific object. The reason you don\u2019t just get a matrix of transformed values is because all of the parameters (i.e. size factors) that went into computing the rlog transform are stored in that object. We use this object to plot the PCA and hierarchical clustering figures for quality assessment. NOTE: The rlog() function can be a bit slow when you have e.g. > 20 samples. In these situations the vst() function is much faster and performs a similar transformation appropriate for use with plotPCA() . It\u2019s typically just a few seconds with vst() due to optimizations and the nature of the transformation.","title":"Transform normalized counts for the MOV10 dataset"},{"location":"06_exploratory_analysis.html#principal-component-analysis-pca-for-the-mov10-dataset","text":"We are now ready for the QC steps, let\u2019s start with PCA! DESeq2 has a built-in function for generating PCA plots using ggplot2 under the hood. This is great because it saves us having to type out lines of code and having to fiddle with the different ggplot2 layers. In addition, it takes the rlog object as an input directly, hence saving us the trouble of extracting the relevant information from it. The function plotPCA() requires two arguments as input: a DESeqTransform object and the \u201cintgroup\u201d (interesting group), i.e. the name of the column in our metadata that has information about the experimental sample groups. ### Plot PCA plotPCA ( rld , intgroup = \"sampletype\" ) Exercise 1 : What does the above plot tell you about the similarity of samples? Does it fit the expectation from the experimental design? What do you think the %variance information (in the axes titles) tell you about the data in the context of the PCA? By default plotPCA() uses the top 500 most variable genes . You can change this by adding the ntop= argument and specifying how many of the genes you want the function to consider. NOTE: The plotPCA() function will only return the values for PC1 and PC2. If you would like to explore the additional PCs in your data or if you would like to identify genes that contribute most to the PCs, you can use the prcomp() function. For example, to plot any of the PCs we could run the following code: # Input is a matrix of log transformed values rld <- rlog ( dds , blind = T ) rld_mat <- assay ( rld ) pca <- prcomp ( t ( rld_mat )) # Create data frame with metadata and PC3 and PC4 values for input to ggplot df <- cbind ( meta , pca $ x ) ggplot ( df ) + geom_point ( aes ( x = PC3 , y = PC4 , color = sampletype )) Resources are available to learn how to do more complex inquiries using the PCs.","title":"Principal component analysis (PCA) for the MOV10 dataset"},{"location":"06_exploratory_analysis.html#hierarchical-clustering-for-the-mov10-dataset","text":"There is no built-in function in DESeq2 for plotting the heatmap for diplaying the pairwise correlation between all the samples and the hierarchical clustering information; we will use the pheatmap() function from the pheatmap package. This function cannot use the DESeqTransform object as input, but requires a matrix or dataframe. So, the first thing to do is retrieve that information from the rld object using a function called assay() (from the SummarizedExperiment package) that converts the data in a DESeqTransform object to a simple 2-dimensional data structure (a matrix in this case). ### Extract the rlog matrix from the object rld_mat <- assay ( rld ) ## \"assay()\" is part of the \"SummarizedExperiment\" package which is a DESeq2 dependency and is loaded with the DESeq2 library Next, we need to compute the pairwise correlation values for all the samples. We can do this using the cor() function: ### Compute pairwise correlation values rld_cor <- as.matrix ( cor ( rld_mat )) ## cor() is a base R function Let\u2019s take a look at the column and row names of the correlation matrix. ## check the output of cor(), make note of the row names and column names head(rld_cor) head(meta) You will notice that they match the names we have given our samples in the metadata data frame we started with. It is important that these match, so we can use the annotation argument below to plot a color block across the top. This block enables easy visualization of the hierarchical clustering. Let\u2019s plot the heatmap! ### Load pheatmap package library ( pheatmap ) ### Plot heatmap using the correlation matrix and the metadata object pheatmap ( rld_cor , annotation_col = meta %>% column_to_rownames ( \"samplename\" )) When you plot using pheatmap() the hierarchical clustering information is used to place similar samples together and this information is represented by the tree structure along the axes. The annotation argument accepts a dataframe as input, in our case it is the meta data frame. Overall, we observe pretty high correlations across the board ( > 0.999) suggesting no outlying sample(s). Also, similar to the PCA plot you see the samples clustering together by sample group. Together, these plots suggest to us that the data are of good quality and we have the green light to proceed to differential expression analysis. NOTE: The pheatmap function has a number of different arguments that we can alter from default values to enhance the aesthetics of the plot. If you are curious and want to explore more, try running the code below. How does your plot change? Take a look through the help pages ( ?pheatmap ) and identify what each of the added arguments is contributing to the plot. heat.colors <- RColorBrewer :: brewer.pal ( 6 , \"Blues\" ) pheatmap ( rld_cor , annotation = meta , color = heat.colors , border_color = NA , fontsize = 10 , fontsize_row = 10 , height = 20 ) Curious about all of the available color palettes offered by the RColorBrewer package ? Try typing in your console display.brewer.all() and see what happens! This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Hierarchical Clustering for the MOV10 dataset"},{"location":"07_extra_contrast_design.html","text":"Approximate time: 40 minutes Learning Objectives \u00b6 Demonstrate the use of the design formula with simple and complex designs Construct R code to execute the differential expression analysis workflow with DESeq2 Differential expression analysis with DESeq2 \u00b6 The final step in the differential expression analysis workflow is fitting the raw counts to the NB model and performing the statistical test for differentially expressed genes. In this step we essentially want to determine whether the mean expression levels of different sample groups are significantly different. Image credit: Paul Pavlidis, UBC The DESeq2 paper was published in 2014, but the package is continually updated and available for use in R through Bioconductor. It builds on good ideas for dispersion estimation and use of Generalized Linear Models from the DSS and edgeR methods. Differential expression analysis with DESeq2 involves multiple steps as displayed in the flowchart below in blue. Briefly, DESeq2 will model the raw counts, using normalization factors (size factors) to account for differences in library depth. Then, it will estimate the gene-wise dispersions and shrink these estimates to generate more accurate estimates of dispersion to model the counts. Finally, DESeq2 will fit the negative binomial model and perform hypothesis testing using the Wald test or Likelihood Ratio Test. NOTE: DESeq2 is actively maintained by the developers and continuously being updated. As such, it is important that you note the version you are working with. Recently, there have been some rather big changes implemented that impact the output. To find out more detail about the specific modifications made to methods described in the original 2014 paper , take a look at this section in the DESeq2 vignette . Additional details on the statistical concepts underlying DESeq2 are elucidated nicely in Rafael Irizarry\u2019s materials for the EdX course, \u201cData Analysis for the Life Sciences Series\u201d. ## Running DESeq2 Prior to performing the differential expression analysis, it is a good idea to know what sources of variation are present in your data, either by exploration during the QC and/or prior knowledge. Once you know the major sources of variation, you can remove them prior to analysis or control for them in the statistical model by including them in your design formula . Design formula \u00b6 A design formula tells the statistical software the known sources of variation to control for, as well as, the factor of interest to test for during differential expression testing. For example, if you know that sex is a significant source of variation in your data, then sex should be included in your model. The design formula should have all of the factors in your metadata that account for major sources of variation in your data. The last factor entered in the formula should be the condition of interest. For example, suppose you have the following metadata: If you want to examine the expression differences between condition , and you know that major sources of variation include bloodtype and patient , then your design formula would be: design = ~ bloodtype + patient + condition The tilde ( ~ ) should always precede your factors and tells DESeq2 to model the counts using the following formula. Note the factors included in the design formula need to match the column names in the metadata . In this tutorial we show a general and flexible way to define contrasts, and is often useful for more complex contrasts or when the design of the experiment is imbalanced (e.g. different number of replicates in each group). Although we focus on DESeq2 , the approach can also be used with the other popular package edgeR . Each section below covers a particular experimental design, from simpler to more complex ones. The first chunk of code in each section is to simulate data, which has no particular meaning and is only done in order to have a DESeqDataSet object with the right kind of variables for each example. In practice, users can ignore this step as they should have created a DESeqDataSet object from their own data following the instructions in the vignette . One factor, two levels \u00b6 # simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 6 , betaSD = 2 ) dds $ condition <- factor ( rep ( c ( \"control\" , \"treat\" ), each = 3 )) First we can look at our sample information: colData ( dds ) ## DataFrame with 6 rows and 1 column ## condition ## <factor> ## sample1 control ## sample2 control ## sample3 control ## sample4 treat ## sample5 treat ## sample6 treat Our factor of interest is condition and so we define our design and run the DESeq model fitting routine: design ( dds ) <- ~ 1 + condition # or just `~ condition` dds <- DESeq ( dds ) # equivalent to edgeR::glmFit() Then check what coefficients DESeq estimated: resultsNames ( dds ) ## [1] \"Intercept\" \"condition_treat_vs_control\" We can see that we have a coefficient for our intercept and coefficient for the effect of treat (i.e. differences between treat versus control). Using the more standard syntax, we can obtain the results for the effect of treat as such: res1 <- results ( dds , contrast = list ( \"condition_treat_vs_control\" )) res1 ## log2 fold change (MLE): condition_treat_vs_control effect ## Wald test p-value: condition_treat_vs_control effect ## DataFrame with 1000 rows and 6 columns ## baseMean log2FoldChange lfcSE stat pvalue ## <numeric> <numeric> <numeric> <numeric> <numeric> ## gene1 40.90941 1.267525859 0.574144 2.207679752 0.0272666 ## gene2 12.21876 -0.269917301 1.111127 -0.242922069 0.8080658 ## gene3 1.91439 -3.538133611 2.564464 -1.379677442 0.1676860 ## gene4 10.24472 0.954811627 1.166408 0.818591708 0.4130194 ## gene5 13.16824 0.000656519 0.868780 0.000755679 0.9993971 ## ... ... ... ... ... ... ## gene996 40.43827 -1.0291276 0.554587 -1.855664 0.063501471 ## gene997 52.88360 0.0622133 0.561981 0.110704 0.911851377 ## gene998 73.06582 1.3271896 0.576695 2.301373 0.021370581 ## gene999 8.87701 -5.8385374 1.549471 -3.768084 0.000164506 ## gene1000 37.06533 1.2669314 0.602010 2.104501 0.035334764 ## padj ## <numeric> ## gene1 0.0712378 ## gene2 0.8779871 ## gene3 0.2943125 ## gene4 0.5692485 ## gene5 0.9996728 ## ... ... ## gene996 0.138827354 ## gene997 0.948279388 ## gene998 0.059599481 ## gene999 0.000914882 ## gene1000 0.087737235 The above is a simple way to obtain the results of interest. But it is worth understanding how DESeq is getting to these results by looking at the model\u2019s matrix. DESeq defines the model matrix using base R functionality: model.matrix ( design ( dds ), colData ( dds )) ## (Intercept) conditiontreat ## sample1 1 0 ## sample2 1 0 ## sample3 1 0 ## sample4 1 1 ## sample5 1 1 ## sample6 1 1 ## attr(,\"assign\") ## [1] 0 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" We can see that R coded condition as a dummy variable, with an intercept (common to all samples) and a \u201cconditiontreat\u201d variable, which adds the effect of treat to samples 4-6. We can actually set our contrasts in DESeq2::results() using a numeric vector. The way it works is to define a vector of \u201cweights\u201d for the coefficient(s) we want to test for. In this case, we have (Intercept) and conditiontreat as our coefficients (see model matrix above), and we want to test for the effect of treat, so our contrast vector would be c(0, 1) . In other words, we don\u2019t care about the value of (Intercept) (so it has a weight of 0), and we\u2019re only interested in the effect of treat (so we give it a weight of 1). In this case the design is very simple, so we could define our contrast vector \u201cmanually\u201d. But for complex designs this can get more difficult to do, so it\u2019s worth mentioning the general way in which we can define this. For any contrast of interest, we can follow three steps: Get the model matrix Subset the matrix for each group of interest and calculate its column means - this results in a vector of coefficients for each group Subtract the group vectors from each other according to the comparison we\u2019re interested in Let\u2019s see this example in action: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## (Intercept) conditiontreat ## sample1 1 0 ## sample2 1 0 ## sample3 1 0 ## sample4 1 1 ## sample5 1 1 ## sample6 1 1 ## attr(,\"assign\") ## [1] 0 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" # calculate the vector of coefficient weights in the treat treat <- colMeans ( mod_mat [ dds $ condition == \"treat\" , ]) treat ## (Intercept) conditiontreat ## 1 1 # calculate the vector of coefficient weights in the control control <- colMeans ( mod_mat [ dds $ condition == \"control\" , ]) control ## (Intercept) conditiontreat ## 1 0 # The contrast we are interested in is the difference between treat and control treat - control ## (Intercept) conditiontreat ## 0 1 That last step is where we define our contrast vector, and we can give this directly to the results function: # get the results for this contrast res2 <- results ( dds , contrast = treat - control ) This gives us exactly the same results as before, which we can check for example by plotting the log-fold-changes between the first and second approach: plot ( res1 $ log2FoldChange , res2 $ log2FoldChange ) Recoding the design \u00b6 Often, we can use different model matrices that essentially correspond to the same design. For example, we could recode our design above by removing the intercept: design ( dds ) <- ~ 0 + condition dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"conditioncontrol\" \"conditiontreat\" In this case we get a coefficient corresponding to the average expression in control and the average expression in the treat (rather than the difference between treat and control). If we use the same contrast trick as before (using the model matrix), we can see the result is the same: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## conditioncontrol conditiontreat ## sample1 1 0 ## sample2 1 0 ## sample3 1 0 ## sample4 0 1 ## sample5 0 1 ## sample6 0 1 ## attr(,\"assign\") ## [1] 1 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" # calculate weights for coefficients in each condition treat <- colMeans ( mod_mat [ which ( dds $ condition == \"treat\" ), ]) control <- colMeans ( mod_mat [ which ( dds $ condition == \"control\" ), ]) # get the results for our contrast res3 <- results ( dds , contrast = treat - control ) Again, the results are essentially the same: plot ( res1 $ log2FoldChange , res3 $ log2FoldChange ) In theory there\u2019s no difference between these two ways of defining our design. The design with an intercept is more common, but for the purposes of understanding what\u2019s going on, it\u2019s sometimes easier to look at models without intercept. One factor, three levels \u00b6 # simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 9 , betaSD = 2 ) dds $ condition <- NULL dds $ bloodtype <- factor ( rep ( c ( \"bloodA\" , \"bloodB\" , \"bloodO\" ), each = 3 )) dds $ bloodtype <- relevel ( dds $ bloodtype , \"bloodO\" ) First we can look at our sample information: colData ( dds ) ## DataFrame with 9 rows and 1 column ## bloodtype ## <factor> ## sample1 bloodA ## sample2 bloodA ## sample3 bloodA ## sample4 bloodB ## sample5 bloodB ## sample6 bloodB ## sample7 bloodO ## sample8 bloodO ## sample9 bloodO As in the previous example, we only have one factor of interest, bloodtype , and so we define our design and run the DESeq as before: design ( dds ) <- ~ 1 + bloodtype dds <- DESeq ( dds ) # check the coefficients estimated by DEseq resultsNames ( dds ) ## [1] \"Intercept\" \"bloodtype_bloodA_vs_bloodO\" ## [3] \"bloodtype_bloodB_vs_bloodO\" We see that now we have 3 coefficients: \u201cIntercept\u201d corresponds to bloodO bloodtype (our reference level) \u201cbloodtype_bloodA_vs_bloodO\u201d corresponds to the difference between the reference level and bloodA \u201cbloodtype_bloodB_vs_bloodO\u201d corresponds to the difference between the reference level and bloodB We could obtain the difference between bloodO and any of the two bloodtypes easily: res1_bloodA_bloodO <- results ( dds , contrast = list ( \"bloodtype_bloodA_vs_bloodO\" )) res1_bloodB_bloodO <- results ( dds , contrast = list ( \"bloodtype_bloodB_vs_bloodO\" )) For comparing bloodA vs bloodB, however, we need to compare two coefficients with each other to check whether they are themselves different (check the slide to see the illustration). This is how the standard DESeq syntax would be: res1_bloodA_bloodB <- results ( dds , contrast = list ( \"bloodtype_bloodA_vs_bloodO\" , \"bloodtype_bloodB_vs_bloodO\" )) However, following our three steps detailed in the first section, we can define our comparisons from the design matrix: # define the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## (Intercept) bloodtypebloodA bloodtypebloodB ## sample1 1 1 0 ## sample2 1 1 0 ## sample3 1 1 0 ## sample4 1 0 1 ## sample5 1 0 1 ## sample6 1 0 1 ## sample7 1 0 0 ## sample8 1 0 0 ## sample9 1 0 0 ## attr(,\"assign\") ## [1] 0 1 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$bloodtype ## [1] \"contr.treatment\" # calculate coefficient vectors for each group bloodA <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" , ]) bloodB <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodB\" , ]) bloodO <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" , ]) And we can now define any contrasts we want: # obtain results for each pairwise contrast res2_bloodA_bloodO <- results ( dds , contrast = bloodA - bloodO ) res2_bloodB_bloodO <- results ( dds , contrast = bloodB - bloodO ) res2_bloodA_bloodB <- results ( dds , contrast = bloodA - bloodB ) # plot the results from the two approaches to check that they are identical plot ( res1_bloodA_bloodO $ log2FoldChange , res2_bloodA_bloodO $ log2FoldChange ) plot ( res1_bloodB_bloodO $ log2FoldChange , res2_bloodB_bloodO $ log2FoldChange ) plot ( res1_bloodA_bloodB $ log2FoldChange , res2_bloodA_bloodB $ log2FoldChange ) A and B against O \u00b6 With this approach, we could even define a more unusual contrast, for example to find genes that differ between A and B against and O samples: # define vector of coefficients for A_B samples A_B <- colMeans ( mod_mat [ dds $ bloodtype %in% c ( \"bloodA\" , \"bloodB\" ),]) # Our contrast of interest is A_B - bloodO ## (Intercept) bloodtypebloodA bloodtypebloodB ## 0.0 0.5 0.5 Notice the contrast vector in this case assigns a \u201cweight\u201d of 0.5 to each of bloodtypebloodA and bloodtypebloodB . This is equivalent to saying that we want to consider the average of bloodA and bloodB expression. In fact, we could have also defined our contrast vector like this: # average of bloodA and bloodB minus bloodO ( bloodA + bloodB ) / 2 - bloodO ## (Intercept) bloodtypebloodA bloodtypebloodB ## 0.0 0.5 0.5 To obtain our results, we use the results() function as before: # get the results between A_B and bloodA res2_AB <- results ( dds , contrast = A_B - bloodO ) Extra: why not define a new group in our design matrix? \u00b6 For this last example (A_B vs bloodO), we may have considered creating a new variable in our column data: dds $ A_B <- factor ( dds $ bloodtype %in% c ( \"bloodA\" , \"bloodB\" )) colData ( dds ) ## DataFrame with 9 rows and 3 columns ## bloodtype sizeFactor A_B ## <factor> <numeric> <factor> ## sample1 bloodA 0.972928 TRUE ## sample2 bloodA 0.985088 TRUE ## sample3 bloodA 0.960749 TRUE ## sample4 bloodB 0.916582 TRUE ## sample5 bloodB 0.936918 TRUE ## sample6 bloodB 1.137368 TRUE ## sample7 bloodO 1.071972 FALSE ## sample8 bloodO 1.141490 FALSE ## sample9 bloodO 1.140135 FALSE and then re-run DESeq with a new design: design ( dds ) <- ~ 1 + A_B dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"A_B_TRUE_vs_FALSE\" res1_A_B <- results ( dds , contrast = list ( \"A_B_TRUE_vs_FALSE\" )) However, in this model the gene dispersion is estimated together for bloodA and bloodB samples as if they were replicates of each other, which may result in inflated/deflated estimates. Instead, our approach above estimates the error within each of those groups. To check the difference one could compare the two approaches visually: # compare the log-fold-changes between the two approaches plot ( res1_A_B $ log2FoldChange , res2_AB $ log2FoldChange ) abline ( 0 , 1 , col = \"brown\" , lwd = 2 ) # compare the errors between the two approaches plot ( res1_A_B $ lfcSE , res2_AB $ lfcSE ) abline ( 0 , 1 , col = \"brown\" , lwd = 2 ) Two factors with interaction \u00b6 # simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 12 , betaSD = 2 ) dds $ bloodtype <- factor ( rep ( c ( \"bloodO\" , \"bloodA\" ), each = 6 )) dds $ bloodtype <- relevel ( dds $ bloodtype , \"bloodO\" ) dds $ condition <- factor ( rep ( c ( \"treat\" , \"control\" ), 6 )) dds <- dds [, order ( dds $ bloodtype , dds $ condition )] colnames ( dds ) <- paste0 ( \"sample\" , 1 : ncol ( dds )) First let\u2019s look at our sample information: colData ( dds ) ## DataFrame with 12 rows and 2 columns ## condition bloodtype ## <factor> <factor> ## sample1 control bloodO ## sample2 control bloodO ## sample3 control bloodO ## sample4 treat bloodO ## sample5 treat bloodO ## ... ... ... ## sample8 control bloodA ## sample9 control bloodA ## sample10 treat bloodA ## sample11 treat bloodA ## sample12 treat bloodA This time we have two factors of interest, and we want to model both with an interaction (i.e. we assume that bloodA and bloodO samples may respond differently to treat/control). We define our design accordingly and fit the model: design ( dds ) <- ~ 1 + bloodtype + condition + bloodtype : condition dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"bloodtype_bloodA_vs_bloodO\" ## [3] \"condition_treat_vs_control\" \"bloodtypebloodA.conditiontreat\" Because we have two factors and an interaction, the number of comparisons we can do is larger. Using our three-step approach from the model matrix, we do things exactly as we\u2019ve been doing so far: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) # Define coefficient vectors for each condition bloodO_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"control\" , ]) bloodO_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"treat\" , ]) bloodA_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"control\" , ]) bloodA_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"treat\" , ]) We are now ready to define any contrast of interest from these vectors (for completeness we show the equivalent syntax using the coefficient\u2019s names from DESeq). bloodA vs bloodO (in the control): res1 <- results ( dds , contrast = bloodA_control - bloodO_control ) # or equivalently res2 <- results ( dds , contrast = list ( \"bloodtype_bloodA_vs_bloodO\" )) bloodA vs bloodO (in the treatment): res1 <- results ( dds , contrast = bloodO_treat - bloodA_treat ) # or equivalently res2 <- results ( dds , contrast = list ( c ( \"bloodtype_bloodA_vs_bloodO\" , \"bloodtypebloodA.conditiontreat\" ))) treat vs control (for bloodtypes O): res1 <- results ( dds , contrast = bloodO_treat - bloodO_control ) # or equivalently res2 <- results ( dds , contrast = list ( c ( \"condition_treat_vs_control\" ))) treat vs control (for bloodtypes A): res1 <- results ( dds , contrast = bloodA_treat - bloodA_control ) # or equivalently res2 <- results ( dds , contrast = list ( c ( \"condition_treat_vs_control\" , \"bloodtypebloodA.conditiontreat\" ))) Interaction between bloodtype and condition I.e. do bloodAs and bloodOs respond differently to the treatment? res1 <- results ( dds , contrast = ( bloodA_treat - bloodA_control ) - ( bloodO_treat - bloodO_control )) # or equivalently res2 <- results ( dds , contrast = list ( \"bloodtypebloodA.conditiontreat\" )) In conclusion, although we can define these contrasts using DESeq coefficient names, it is somewhat more explicit (and perhaps intuitive?) what it is we\u2019re comparing using matrix-based contrasts. Three factors, with nesting \u00b6 # simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 24 , betaSD = 2 ) dds $ bloodtype <- factor ( rep ( c ( \"bloodA\" , \"bloodO\" ), each = 12 )) dds $ bloodtype <- relevel ( dds $ bloodtype , \"bloodO\" ) dds $ patient <- factor ( rep ( LETTERS [ 1 : 4 ], each = 6 )) dds $ condition <- factor ( rep ( c ( \"treat\" , \"control\" ), 12 )) dds <- dds [, order ( dds $ bloodtype , dds $ patient , dds $ condition )] colnames ( dds ) <- paste0 ( \"sample\" , 1 : ncol ( dds )) First let\u2019s look at our sample information: colData ( dds ) ## DataFrame with 24 rows and 3 columns ## condition bloodtype patient ## <factor> <factor> <factor> ## sample1 control bloodO C ## sample2 control bloodO C ## sample3 control bloodO C ## sample4 treat bloodO C ## sample5 treat bloodO C ## ... ... ... ... ## sample20 control bloodA B ## sample21 control bloodA B ## sample22 treat bloodA B ## sample23 treat bloodA B ## sample24 treat bloodA B Now we have three factors, but patient is nested within bloodtype (i.e. a patient is either bloodA or bloodO, it cannot be both). Therefore, bloodtype is a linear combination with patient (or, another way to think about it is that bloodtype is redundant with patient). Because of this, we will define our design without including \u201cbloodtype\u201d, although later we can compare groups of patient of the same bloodtype with each other. design ( dds ) <- ~ 1 + patient + condition + patient : condition dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"patient_B_vs_A\" ## [3] \"patient_C_vs_A\" \"patient_D_vs_A\" ## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\" ## [7] \"patientC.conditiontreat\" \"patientD.conditiontreat\" Now it\u2019s harder to define contrasts between groups of patient of the same bloodtype using DESeq\u2019s coefficient names (although still possible). But using the model matrix approach, we do it in exactly the same way we have done so far! Again, let\u2019s define our groups from the model matrix: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) # define coefficient vectors for each group bloodO_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"control\" , ]) bloodA_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"control\" , ]) bloodO_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"treat\" , ]) bloodA_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"treat\" , ]) It\u2019s worth looking at some of these vectors, to see that they are composed of weighted coefficients from different patient. For example, for \u201cbloodO\u201d patient, we have equal contribution from \u201cpatientC\u201d and \u201cpatientD\u201d: bloodO_control ## (Intercept) patientB patientC ## 1.0 0.0 0.5 ## patientD conditiontreat patientB:conditiontreat ## 0.5 0.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.0 0.0 And so, when we define our contrasts, each patient will be correctly weighted: bloodO_treat - bloodO_control ## (Intercept) patientB patientC ## 0.0 0.0 0.0 ## patientD conditiontreat patientB:conditiontreat ## 0.0 1.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.5 0.5 We can set our contrasts in exactly the same way as we did in the previous section (for completeness, we also give the contrasts using DESeq\u2019s named coefficients). bloodA vs bloodO (in the control): res1_bloodA_bloodO_control <- results ( dds , contrast = bloodA_control - bloodO_control ) # or equivalently res2_bloodA_bloodO_control <- results ( dds , contrast = list ( c ( \"patient_B_vs_A\" ), # Blood type A c ( \"patient_C_vs_A\" , # Blood type O \"patient_D_vs_A\" ))) # Blood type O bloodA vs bloodO (in the treat): res1_bloodO_bloodA_treat <- results ( dds , contrast = bloodO_treat - bloodA_treat ) # or equivalently res2_bloodO_bloodA_treat <- results ( dds , contrast = list ( c ( \"patient_B_vs_A\" , # Blood type A \"patientB.conditiontreat\" ), # Interaction of patient B with treatment c ( \"patient_C_vs_A\" , # Blood type O \"patient_D_vs_A\" , # Blood type O \"patientC.conditiontreat\" , # Interaction of patient C with treatment \"patientD.conditiontreat\" ))) # Interaction of patient B with treatment And so on, for other contrasts of interest\u2026 Extra: imbalanced design \u00b6 Let\u2019s take our previous example, but drop one of the samples from the data, so that we only have 2 replicates for it. dds <- dds [, -1 ] # drop one of the patient C samples dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"patient_B_vs_A\" ## [3] \"patient_C_vs_A\" \"patient_D_vs_A\" ## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\" ## [7] \"patientC.conditiontreat\" \"patientD.conditiontreat\" Define our model matrix and coefficient vectors: mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## (Intercept) patientB patientC patientD conditiontreat ## sample2 1 0 1 0 0 ## sample3 1 0 1 0 0 ## sample4 1 0 1 0 1 ## sample5 1 0 1 0 1 ## sample6 1 0 1 0 1 ## sample7 1 0 0 1 0 ## sample8 1 0 0 1 0 ## sample9 1 0 0 1 0 ## sample10 1 0 0 1 1 ## sample11 1 0 0 1 1 ## sample12 1 0 0 1 1 ## sample13 1 0 0 0 0 ## sample14 1 0 0 0 0 ## sample15 1 0 0 0 0 ## sample16 1 0 0 0 1 ## sample17 1 0 0 0 1 ## sample18 1 0 0 0 1 ## sample19 1 1 0 0 0 ## sample20 1 1 0 0 0 ## sample21 1 1 0 0 0 ## sample22 1 1 0 0 1 ## sample23 1 1 0 0 1 ## sample24 1 1 0 0 1 ## patientB:conditiontreat patientC:conditiontreat ## sample2 0 0 ## sample3 0 0 ## sample4 0 1 ## sample5 0 1 ## sample6 0 1 ## sample7 0 0 ## sample8 0 0 ## sample9 0 0 ## sample10 0 0 ## sample11 0 0 ## sample12 0 0 ## sample13 0 0 ## sample14 0 0 ## sample15 0 0 ## sample16 0 0 ## sample17 0 0 ## sample18 0 0 ## sample19 0 0 ## sample20 0 0 ## sample21 0 0 ## sample22 1 0 ## sample23 1 0 ## sample24 1 0 ## patientD:conditiontreat ## sample2 0 ## sample3 0 ## sample4 0 ## sample5 0 ## sample6 0 ## sample7 0 ## sample8 0 ## sample9 0 ## sample10 1 ## sample11 1 ## sample12 1 ## sample13 0 ## sample14 0 ## sample15 0 ## sample16 0 ## sample17 0 ## sample18 0 ## sample19 0 ## sample20 0 ## sample21 0 ## sample22 0 ## sample23 0 ## sample24 0 ## attr(,\"assign\") ## [1] 0 1 1 1 2 3 3 3 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$patient ## [1] \"contr.treatment\" ## ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" # define coefficient vectors for each group bloodO_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"control\" , ]) bloodA_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"control\" , ]) bloodO_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"treat\" , ]) bloodA_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"treat\" , ]) Now let\u2019s check what happens to the bloodO_control group: bloodO_control ## (Intercept) patientB patientC ## 1.0 0.0 0.4 ## patientD conditiontreat patientB:conditiontreat ## 0.6 0.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.0 0.0 Notice that whereas before \u201cpatientC\u201d and \u201cpatientD\u201d had each a weight of 0.5, now they have different weights. That\u2019s because for patientC there\u2019s only 2 replicates. So, we have a total of 5 bloodtype O individuals in the control (2 from patient C and 3 from D). Therefore, when we calculate the average coefficients for bloodOs, we need to do it as 0.4 x patientC + 0.6 x patientD. The nice thing about this approach is that we do not need to worry about any of this, the weights come from our colMeans() call automatically. And now, any contrasts that we make will take these weights into account: # bloodA vs bloodO (in the control) bloodA_control - bloodO_control ## (Intercept) patientB patientC ## 0.0 0.5 -0.4 ## patientD conditiontreat patientB:conditiontreat ## -0.6 0.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.0 0.0 # interaction ( bloodA_treat - bloodA_control ) - ( bloodO_treat - bloodO_control ) ## (Intercept) patientB patientC ## 0.0 0.0 -0.1 ## patientD conditiontreat patientB:conditiontreat ## 0.1 0.0 0.5 ## patientC:conditiontreat patientD:conditiontreat ## -0.5 -0.5 Part of this lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . In addition, we would like to thank Hugo Tavares from the Bioinformatics Training Facility of the University of Cambridge.","title":"Design formula"},{"location":"07_extra_contrast_design.html#learning-objectives","text":"Demonstrate the use of the design formula with simple and complex designs Construct R code to execute the differential expression analysis workflow with DESeq2","title":"Learning Objectives"},{"location":"07_extra_contrast_design.html#differential-expression-analysis-with-deseq2","text":"The final step in the differential expression analysis workflow is fitting the raw counts to the NB model and performing the statistical test for differentially expressed genes. In this step we essentially want to determine whether the mean expression levels of different sample groups are significantly different. Image credit: Paul Pavlidis, UBC The DESeq2 paper was published in 2014, but the package is continually updated and available for use in R through Bioconductor. It builds on good ideas for dispersion estimation and use of Generalized Linear Models from the DSS and edgeR methods. Differential expression analysis with DESeq2 involves multiple steps as displayed in the flowchart below in blue. Briefly, DESeq2 will model the raw counts, using normalization factors (size factors) to account for differences in library depth. Then, it will estimate the gene-wise dispersions and shrink these estimates to generate more accurate estimates of dispersion to model the counts. Finally, DESeq2 will fit the negative binomial model and perform hypothesis testing using the Wald test or Likelihood Ratio Test. NOTE: DESeq2 is actively maintained by the developers and continuously being updated. As such, it is important that you note the version you are working with. Recently, there have been some rather big changes implemented that impact the output. To find out more detail about the specific modifications made to methods described in the original 2014 paper , take a look at this section in the DESeq2 vignette . Additional details on the statistical concepts underlying DESeq2 are elucidated nicely in Rafael Irizarry\u2019s materials for the EdX course, \u201cData Analysis for the Life Sciences Series\u201d. ## Running DESeq2 Prior to performing the differential expression analysis, it is a good idea to know what sources of variation are present in your data, either by exploration during the QC and/or prior knowledge. Once you know the major sources of variation, you can remove them prior to analysis or control for them in the statistical model by including them in your design formula .","title":"Differential expression analysis with DESeq2"},{"location":"07_extra_contrast_design.html#design-formula","text":"A design formula tells the statistical software the known sources of variation to control for, as well as, the factor of interest to test for during differential expression testing. For example, if you know that sex is a significant source of variation in your data, then sex should be included in your model. The design formula should have all of the factors in your metadata that account for major sources of variation in your data. The last factor entered in the formula should be the condition of interest. For example, suppose you have the following metadata: If you want to examine the expression differences between condition , and you know that major sources of variation include bloodtype and patient , then your design formula would be: design = ~ bloodtype + patient + condition The tilde ( ~ ) should always precede your factors and tells DESeq2 to model the counts using the following formula. Note the factors included in the design formula need to match the column names in the metadata . In this tutorial we show a general and flexible way to define contrasts, and is often useful for more complex contrasts or when the design of the experiment is imbalanced (e.g. different number of replicates in each group). Although we focus on DESeq2 , the approach can also be used with the other popular package edgeR . Each section below covers a particular experimental design, from simpler to more complex ones. The first chunk of code in each section is to simulate data, which has no particular meaning and is only done in order to have a DESeqDataSet object with the right kind of variables for each example. In practice, users can ignore this step as they should have created a DESeqDataSet object from their own data following the instructions in the vignette .","title":"Design formula"},{"location":"07_extra_contrast_design.html#one-factor-two-levels","text":"# simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 6 , betaSD = 2 ) dds $ condition <- factor ( rep ( c ( \"control\" , \"treat\" ), each = 3 )) First we can look at our sample information: colData ( dds ) ## DataFrame with 6 rows and 1 column ## condition ## <factor> ## sample1 control ## sample2 control ## sample3 control ## sample4 treat ## sample5 treat ## sample6 treat Our factor of interest is condition and so we define our design and run the DESeq model fitting routine: design ( dds ) <- ~ 1 + condition # or just `~ condition` dds <- DESeq ( dds ) # equivalent to edgeR::glmFit() Then check what coefficients DESeq estimated: resultsNames ( dds ) ## [1] \"Intercept\" \"condition_treat_vs_control\" We can see that we have a coefficient for our intercept and coefficient for the effect of treat (i.e. differences between treat versus control). Using the more standard syntax, we can obtain the results for the effect of treat as such: res1 <- results ( dds , contrast = list ( \"condition_treat_vs_control\" )) res1 ## log2 fold change (MLE): condition_treat_vs_control effect ## Wald test p-value: condition_treat_vs_control effect ## DataFrame with 1000 rows and 6 columns ## baseMean log2FoldChange lfcSE stat pvalue ## <numeric> <numeric> <numeric> <numeric> <numeric> ## gene1 40.90941 1.267525859 0.574144 2.207679752 0.0272666 ## gene2 12.21876 -0.269917301 1.111127 -0.242922069 0.8080658 ## gene3 1.91439 -3.538133611 2.564464 -1.379677442 0.1676860 ## gene4 10.24472 0.954811627 1.166408 0.818591708 0.4130194 ## gene5 13.16824 0.000656519 0.868780 0.000755679 0.9993971 ## ... ... ... ... ... ... ## gene996 40.43827 -1.0291276 0.554587 -1.855664 0.063501471 ## gene997 52.88360 0.0622133 0.561981 0.110704 0.911851377 ## gene998 73.06582 1.3271896 0.576695 2.301373 0.021370581 ## gene999 8.87701 -5.8385374 1.549471 -3.768084 0.000164506 ## gene1000 37.06533 1.2669314 0.602010 2.104501 0.035334764 ## padj ## <numeric> ## gene1 0.0712378 ## gene2 0.8779871 ## gene3 0.2943125 ## gene4 0.5692485 ## gene5 0.9996728 ## ... ... ## gene996 0.138827354 ## gene997 0.948279388 ## gene998 0.059599481 ## gene999 0.000914882 ## gene1000 0.087737235 The above is a simple way to obtain the results of interest. But it is worth understanding how DESeq is getting to these results by looking at the model\u2019s matrix. DESeq defines the model matrix using base R functionality: model.matrix ( design ( dds ), colData ( dds )) ## (Intercept) conditiontreat ## sample1 1 0 ## sample2 1 0 ## sample3 1 0 ## sample4 1 1 ## sample5 1 1 ## sample6 1 1 ## attr(,\"assign\") ## [1] 0 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" We can see that R coded condition as a dummy variable, with an intercept (common to all samples) and a \u201cconditiontreat\u201d variable, which adds the effect of treat to samples 4-6. We can actually set our contrasts in DESeq2::results() using a numeric vector. The way it works is to define a vector of \u201cweights\u201d for the coefficient(s) we want to test for. In this case, we have (Intercept) and conditiontreat as our coefficients (see model matrix above), and we want to test for the effect of treat, so our contrast vector would be c(0, 1) . In other words, we don\u2019t care about the value of (Intercept) (so it has a weight of 0), and we\u2019re only interested in the effect of treat (so we give it a weight of 1). In this case the design is very simple, so we could define our contrast vector \u201cmanually\u201d. But for complex designs this can get more difficult to do, so it\u2019s worth mentioning the general way in which we can define this. For any contrast of interest, we can follow three steps: Get the model matrix Subset the matrix for each group of interest and calculate its column means - this results in a vector of coefficients for each group Subtract the group vectors from each other according to the comparison we\u2019re interested in Let\u2019s see this example in action: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## (Intercept) conditiontreat ## sample1 1 0 ## sample2 1 0 ## sample3 1 0 ## sample4 1 1 ## sample5 1 1 ## sample6 1 1 ## attr(,\"assign\") ## [1] 0 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" # calculate the vector of coefficient weights in the treat treat <- colMeans ( mod_mat [ dds $ condition == \"treat\" , ]) treat ## (Intercept) conditiontreat ## 1 1 # calculate the vector of coefficient weights in the control control <- colMeans ( mod_mat [ dds $ condition == \"control\" , ]) control ## (Intercept) conditiontreat ## 1 0 # The contrast we are interested in is the difference between treat and control treat - control ## (Intercept) conditiontreat ## 0 1 That last step is where we define our contrast vector, and we can give this directly to the results function: # get the results for this contrast res2 <- results ( dds , contrast = treat - control ) This gives us exactly the same results as before, which we can check for example by plotting the log-fold-changes between the first and second approach: plot ( res1 $ log2FoldChange , res2 $ log2FoldChange )","title":"One factor, two levels"},{"location":"07_extra_contrast_design.html#recoding-the-design","text":"Often, we can use different model matrices that essentially correspond to the same design. For example, we could recode our design above by removing the intercept: design ( dds ) <- ~ 0 + condition dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"conditioncontrol\" \"conditiontreat\" In this case we get a coefficient corresponding to the average expression in control and the average expression in the treat (rather than the difference between treat and control). If we use the same contrast trick as before (using the model matrix), we can see the result is the same: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## conditioncontrol conditiontreat ## sample1 1 0 ## sample2 1 0 ## sample3 1 0 ## sample4 0 1 ## sample5 0 1 ## sample6 0 1 ## attr(,\"assign\") ## [1] 1 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" # calculate weights for coefficients in each condition treat <- colMeans ( mod_mat [ which ( dds $ condition == \"treat\" ), ]) control <- colMeans ( mod_mat [ which ( dds $ condition == \"control\" ), ]) # get the results for our contrast res3 <- results ( dds , contrast = treat - control ) Again, the results are essentially the same: plot ( res1 $ log2FoldChange , res3 $ log2FoldChange ) In theory there\u2019s no difference between these two ways of defining our design. The design with an intercept is more common, but for the purposes of understanding what\u2019s going on, it\u2019s sometimes easier to look at models without intercept.","title":"Recoding the design"},{"location":"07_extra_contrast_design.html#one-factor-three-levels","text":"# simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 9 , betaSD = 2 ) dds $ condition <- NULL dds $ bloodtype <- factor ( rep ( c ( \"bloodA\" , \"bloodB\" , \"bloodO\" ), each = 3 )) dds $ bloodtype <- relevel ( dds $ bloodtype , \"bloodO\" ) First we can look at our sample information: colData ( dds ) ## DataFrame with 9 rows and 1 column ## bloodtype ## <factor> ## sample1 bloodA ## sample2 bloodA ## sample3 bloodA ## sample4 bloodB ## sample5 bloodB ## sample6 bloodB ## sample7 bloodO ## sample8 bloodO ## sample9 bloodO As in the previous example, we only have one factor of interest, bloodtype , and so we define our design and run the DESeq as before: design ( dds ) <- ~ 1 + bloodtype dds <- DESeq ( dds ) # check the coefficients estimated by DEseq resultsNames ( dds ) ## [1] \"Intercept\" \"bloodtype_bloodA_vs_bloodO\" ## [3] \"bloodtype_bloodB_vs_bloodO\" We see that now we have 3 coefficients: \u201cIntercept\u201d corresponds to bloodO bloodtype (our reference level) \u201cbloodtype_bloodA_vs_bloodO\u201d corresponds to the difference between the reference level and bloodA \u201cbloodtype_bloodB_vs_bloodO\u201d corresponds to the difference between the reference level and bloodB We could obtain the difference between bloodO and any of the two bloodtypes easily: res1_bloodA_bloodO <- results ( dds , contrast = list ( \"bloodtype_bloodA_vs_bloodO\" )) res1_bloodB_bloodO <- results ( dds , contrast = list ( \"bloodtype_bloodB_vs_bloodO\" )) For comparing bloodA vs bloodB, however, we need to compare two coefficients with each other to check whether they are themselves different (check the slide to see the illustration). This is how the standard DESeq syntax would be: res1_bloodA_bloodB <- results ( dds , contrast = list ( \"bloodtype_bloodA_vs_bloodO\" , \"bloodtype_bloodB_vs_bloodO\" )) However, following our three steps detailed in the first section, we can define our comparisons from the design matrix: # define the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## (Intercept) bloodtypebloodA bloodtypebloodB ## sample1 1 1 0 ## sample2 1 1 0 ## sample3 1 1 0 ## sample4 1 0 1 ## sample5 1 0 1 ## sample6 1 0 1 ## sample7 1 0 0 ## sample8 1 0 0 ## sample9 1 0 0 ## attr(,\"assign\") ## [1] 0 1 1 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$bloodtype ## [1] \"contr.treatment\" # calculate coefficient vectors for each group bloodA <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" , ]) bloodB <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodB\" , ]) bloodO <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" , ]) And we can now define any contrasts we want: # obtain results for each pairwise contrast res2_bloodA_bloodO <- results ( dds , contrast = bloodA - bloodO ) res2_bloodB_bloodO <- results ( dds , contrast = bloodB - bloodO ) res2_bloodA_bloodB <- results ( dds , contrast = bloodA - bloodB ) # plot the results from the two approaches to check that they are identical plot ( res1_bloodA_bloodO $ log2FoldChange , res2_bloodA_bloodO $ log2FoldChange ) plot ( res1_bloodB_bloodO $ log2FoldChange , res2_bloodB_bloodO $ log2FoldChange ) plot ( res1_bloodA_bloodB $ log2FoldChange , res2_bloodA_bloodB $ log2FoldChange )","title":"One factor, three levels"},{"location":"07_extra_contrast_design.html#a-and-b-against-o","text":"With this approach, we could even define a more unusual contrast, for example to find genes that differ between A and B against and O samples: # define vector of coefficients for A_B samples A_B <- colMeans ( mod_mat [ dds $ bloodtype %in% c ( \"bloodA\" , \"bloodB\" ),]) # Our contrast of interest is A_B - bloodO ## (Intercept) bloodtypebloodA bloodtypebloodB ## 0.0 0.5 0.5 Notice the contrast vector in this case assigns a \u201cweight\u201d of 0.5 to each of bloodtypebloodA and bloodtypebloodB . This is equivalent to saying that we want to consider the average of bloodA and bloodB expression. In fact, we could have also defined our contrast vector like this: # average of bloodA and bloodB minus bloodO ( bloodA + bloodB ) / 2 - bloodO ## (Intercept) bloodtypebloodA bloodtypebloodB ## 0.0 0.5 0.5 To obtain our results, we use the results() function as before: # get the results between A_B and bloodA res2_AB <- results ( dds , contrast = A_B - bloodO )","title":"A and B against O"},{"location":"07_extra_contrast_design.html#extra-why-not-define-a-new-group-in-our-design-matrix","text":"For this last example (A_B vs bloodO), we may have considered creating a new variable in our column data: dds $ A_B <- factor ( dds $ bloodtype %in% c ( \"bloodA\" , \"bloodB\" )) colData ( dds ) ## DataFrame with 9 rows and 3 columns ## bloodtype sizeFactor A_B ## <factor> <numeric> <factor> ## sample1 bloodA 0.972928 TRUE ## sample2 bloodA 0.985088 TRUE ## sample3 bloodA 0.960749 TRUE ## sample4 bloodB 0.916582 TRUE ## sample5 bloodB 0.936918 TRUE ## sample6 bloodB 1.137368 TRUE ## sample7 bloodO 1.071972 FALSE ## sample8 bloodO 1.141490 FALSE ## sample9 bloodO 1.140135 FALSE and then re-run DESeq with a new design: design ( dds ) <- ~ 1 + A_B dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"A_B_TRUE_vs_FALSE\" res1_A_B <- results ( dds , contrast = list ( \"A_B_TRUE_vs_FALSE\" )) However, in this model the gene dispersion is estimated together for bloodA and bloodB samples as if they were replicates of each other, which may result in inflated/deflated estimates. Instead, our approach above estimates the error within each of those groups. To check the difference one could compare the two approaches visually: # compare the log-fold-changes between the two approaches plot ( res1_A_B $ log2FoldChange , res2_AB $ log2FoldChange ) abline ( 0 , 1 , col = \"brown\" , lwd = 2 ) # compare the errors between the two approaches plot ( res1_A_B $ lfcSE , res2_AB $ lfcSE ) abline ( 0 , 1 , col = \"brown\" , lwd = 2 )","title":"Extra: why not define a new group in our design matrix?"},{"location":"07_extra_contrast_design.html#two-factors-with-interaction","text":"# simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 12 , betaSD = 2 ) dds $ bloodtype <- factor ( rep ( c ( \"bloodO\" , \"bloodA\" ), each = 6 )) dds $ bloodtype <- relevel ( dds $ bloodtype , \"bloodO\" ) dds $ condition <- factor ( rep ( c ( \"treat\" , \"control\" ), 6 )) dds <- dds [, order ( dds $ bloodtype , dds $ condition )] colnames ( dds ) <- paste0 ( \"sample\" , 1 : ncol ( dds )) First let\u2019s look at our sample information: colData ( dds ) ## DataFrame with 12 rows and 2 columns ## condition bloodtype ## <factor> <factor> ## sample1 control bloodO ## sample2 control bloodO ## sample3 control bloodO ## sample4 treat bloodO ## sample5 treat bloodO ## ... ... ... ## sample8 control bloodA ## sample9 control bloodA ## sample10 treat bloodA ## sample11 treat bloodA ## sample12 treat bloodA This time we have two factors of interest, and we want to model both with an interaction (i.e. we assume that bloodA and bloodO samples may respond differently to treat/control). We define our design accordingly and fit the model: design ( dds ) <- ~ 1 + bloodtype + condition + bloodtype : condition dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"bloodtype_bloodA_vs_bloodO\" ## [3] \"condition_treat_vs_control\" \"bloodtypebloodA.conditiontreat\" Because we have two factors and an interaction, the number of comparisons we can do is larger. Using our three-step approach from the model matrix, we do things exactly as we\u2019ve been doing so far: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) # Define coefficient vectors for each condition bloodO_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"control\" , ]) bloodO_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"treat\" , ]) bloodA_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"control\" , ]) bloodA_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"treat\" , ]) We are now ready to define any contrast of interest from these vectors (for completeness we show the equivalent syntax using the coefficient\u2019s names from DESeq). bloodA vs bloodO (in the control): res1 <- results ( dds , contrast = bloodA_control - bloodO_control ) # or equivalently res2 <- results ( dds , contrast = list ( \"bloodtype_bloodA_vs_bloodO\" )) bloodA vs bloodO (in the treatment): res1 <- results ( dds , contrast = bloodO_treat - bloodA_treat ) # or equivalently res2 <- results ( dds , contrast = list ( c ( \"bloodtype_bloodA_vs_bloodO\" , \"bloodtypebloodA.conditiontreat\" ))) treat vs control (for bloodtypes O): res1 <- results ( dds , contrast = bloodO_treat - bloodO_control ) # or equivalently res2 <- results ( dds , contrast = list ( c ( \"condition_treat_vs_control\" ))) treat vs control (for bloodtypes A): res1 <- results ( dds , contrast = bloodA_treat - bloodA_control ) # or equivalently res2 <- results ( dds , contrast = list ( c ( \"condition_treat_vs_control\" , \"bloodtypebloodA.conditiontreat\" ))) Interaction between bloodtype and condition I.e. do bloodAs and bloodOs respond differently to the treatment? res1 <- results ( dds , contrast = ( bloodA_treat - bloodA_control ) - ( bloodO_treat - bloodO_control )) # or equivalently res2 <- results ( dds , contrast = list ( \"bloodtypebloodA.conditiontreat\" )) In conclusion, although we can define these contrasts using DESeq coefficient names, it is somewhat more explicit (and perhaps intuitive?) what it is we\u2019re comparing using matrix-based contrasts.","title":"Two factors with interaction"},{"location":"07_extra_contrast_design.html#three-factors-with-nesting","text":"# simulate data dds <- makeExampleDESeqDataSet ( n = 1000 , m = 24 , betaSD = 2 ) dds $ bloodtype <- factor ( rep ( c ( \"bloodA\" , \"bloodO\" ), each = 12 )) dds $ bloodtype <- relevel ( dds $ bloodtype , \"bloodO\" ) dds $ patient <- factor ( rep ( LETTERS [ 1 : 4 ], each = 6 )) dds $ condition <- factor ( rep ( c ( \"treat\" , \"control\" ), 12 )) dds <- dds [, order ( dds $ bloodtype , dds $ patient , dds $ condition )] colnames ( dds ) <- paste0 ( \"sample\" , 1 : ncol ( dds )) First let\u2019s look at our sample information: colData ( dds ) ## DataFrame with 24 rows and 3 columns ## condition bloodtype patient ## <factor> <factor> <factor> ## sample1 control bloodO C ## sample2 control bloodO C ## sample3 control bloodO C ## sample4 treat bloodO C ## sample5 treat bloodO C ## ... ... ... ... ## sample20 control bloodA B ## sample21 control bloodA B ## sample22 treat bloodA B ## sample23 treat bloodA B ## sample24 treat bloodA B Now we have three factors, but patient is nested within bloodtype (i.e. a patient is either bloodA or bloodO, it cannot be both). Therefore, bloodtype is a linear combination with patient (or, another way to think about it is that bloodtype is redundant with patient). Because of this, we will define our design without including \u201cbloodtype\u201d, although later we can compare groups of patient of the same bloodtype with each other. design ( dds ) <- ~ 1 + patient + condition + patient : condition dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"patient_B_vs_A\" ## [3] \"patient_C_vs_A\" \"patient_D_vs_A\" ## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\" ## [7] \"patientC.conditiontreat\" \"patientD.conditiontreat\" Now it\u2019s harder to define contrasts between groups of patient of the same bloodtype using DESeq\u2019s coefficient names (although still possible). But using the model matrix approach, we do it in exactly the same way we have done so far! Again, let\u2019s define our groups from the model matrix: # get the model matrix mod_mat <- model.matrix ( design ( dds ), colData ( dds )) # define coefficient vectors for each group bloodO_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"control\" , ]) bloodA_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"control\" , ]) bloodO_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"treat\" , ]) bloodA_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"treat\" , ]) It\u2019s worth looking at some of these vectors, to see that they are composed of weighted coefficients from different patient. For example, for \u201cbloodO\u201d patient, we have equal contribution from \u201cpatientC\u201d and \u201cpatientD\u201d: bloodO_control ## (Intercept) patientB patientC ## 1.0 0.0 0.5 ## patientD conditiontreat patientB:conditiontreat ## 0.5 0.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.0 0.0 And so, when we define our contrasts, each patient will be correctly weighted: bloodO_treat - bloodO_control ## (Intercept) patientB patientC ## 0.0 0.0 0.0 ## patientD conditiontreat patientB:conditiontreat ## 0.0 1.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.5 0.5 We can set our contrasts in exactly the same way as we did in the previous section (for completeness, we also give the contrasts using DESeq\u2019s named coefficients). bloodA vs bloodO (in the control): res1_bloodA_bloodO_control <- results ( dds , contrast = bloodA_control - bloodO_control ) # or equivalently res2_bloodA_bloodO_control <- results ( dds , contrast = list ( c ( \"patient_B_vs_A\" ), # Blood type A c ( \"patient_C_vs_A\" , # Blood type O \"patient_D_vs_A\" ))) # Blood type O bloodA vs bloodO (in the treat): res1_bloodO_bloodA_treat <- results ( dds , contrast = bloodO_treat - bloodA_treat ) # or equivalently res2_bloodO_bloodA_treat <- results ( dds , contrast = list ( c ( \"patient_B_vs_A\" , # Blood type A \"patientB.conditiontreat\" ), # Interaction of patient B with treatment c ( \"patient_C_vs_A\" , # Blood type O \"patient_D_vs_A\" , # Blood type O \"patientC.conditiontreat\" , # Interaction of patient C with treatment \"patientD.conditiontreat\" ))) # Interaction of patient B with treatment And so on, for other contrasts of interest\u2026","title":"Three factors, with nesting"},{"location":"07_extra_contrast_design.html#extra-imbalanced-design","text":"Let\u2019s take our previous example, but drop one of the samples from the data, so that we only have 2 replicates for it. dds <- dds [, -1 ] # drop one of the patient C samples dds <- DESeq ( dds ) resultsNames ( dds ) ## [1] \"Intercept\" \"patient_B_vs_A\" ## [3] \"patient_C_vs_A\" \"patient_D_vs_A\" ## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\" ## [7] \"patientC.conditiontreat\" \"patientD.conditiontreat\" Define our model matrix and coefficient vectors: mod_mat <- model.matrix ( design ( dds ), colData ( dds )) mod_mat ## (Intercept) patientB patientC patientD conditiontreat ## sample2 1 0 1 0 0 ## sample3 1 0 1 0 0 ## sample4 1 0 1 0 1 ## sample5 1 0 1 0 1 ## sample6 1 0 1 0 1 ## sample7 1 0 0 1 0 ## sample8 1 0 0 1 0 ## sample9 1 0 0 1 0 ## sample10 1 0 0 1 1 ## sample11 1 0 0 1 1 ## sample12 1 0 0 1 1 ## sample13 1 0 0 0 0 ## sample14 1 0 0 0 0 ## sample15 1 0 0 0 0 ## sample16 1 0 0 0 1 ## sample17 1 0 0 0 1 ## sample18 1 0 0 0 1 ## sample19 1 1 0 0 0 ## sample20 1 1 0 0 0 ## sample21 1 1 0 0 0 ## sample22 1 1 0 0 1 ## sample23 1 1 0 0 1 ## sample24 1 1 0 0 1 ## patientB:conditiontreat patientC:conditiontreat ## sample2 0 0 ## sample3 0 0 ## sample4 0 1 ## sample5 0 1 ## sample6 0 1 ## sample7 0 0 ## sample8 0 0 ## sample9 0 0 ## sample10 0 0 ## sample11 0 0 ## sample12 0 0 ## sample13 0 0 ## sample14 0 0 ## sample15 0 0 ## sample16 0 0 ## sample17 0 0 ## sample18 0 0 ## sample19 0 0 ## sample20 0 0 ## sample21 0 0 ## sample22 1 0 ## sample23 1 0 ## sample24 1 0 ## patientD:conditiontreat ## sample2 0 ## sample3 0 ## sample4 0 ## sample5 0 ## sample6 0 ## sample7 0 ## sample8 0 ## sample9 0 ## sample10 1 ## sample11 1 ## sample12 1 ## sample13 0 ## sample14 0 ## sample15 0 ## sample16 0 ## sample17 0 ## sample18 0 ## sample19 0 ## sample20 0 ## sample21 0 ## sample22 0 ## sample23 0 ## sample24 0 ## attr(,\"assign\") ## [1] 0 1 1 1 2 3 3 3 ## attr(,\"contrasts\") ## attr(,\"contrasts\")$patient ## [1] \"contr.treatment\" ## ## attr(,\"contrasts\")$condition ## [1] \"contr.treatment\" # define coefficient vectors for each group bloodO_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"control\" , ]) bloodA_control <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"control\" , ]) bloodO_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodO\" & dds $ condition == \"treat\" , ]) bloodA_treat <- colMeans ( mod_mat [ dds $ bloodtype == \"bloodA\" & dds $ condition == \"treat\" , ]) Now let\u2019s check what happens to the bloodO_control group: bloodO_control ## (Intercept) patientB patientC ## 1.0 0.0 0.4 ## patientD conditiontreat patientB:conditiontreat ## 0.6 0.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.0 0.0 Notice that whereas before \u201cpatientC\u201d and \u201cpatientD\u201d had each a weight of 0.5, now they have different weights. That\u2019s because for patientC there\u2019s only 2 replicates. So, we have a total of 5 bloodtype O individuals in the control (2 from patient C and 3 from D). Therefore, when we calculate the average coefficients for bloodOs, we need to do it as 0.4 x patientC + 0.6 x patientD. The nice thing about this approach is that we do not need to worry about any of this, the weights come from our colMeans() call automatically. And now, any contrasts that we make will take these weights into account: # bloodA vs bloodO (in the control) bloodA_control - bloodO_control ## (Intercept) patientB patientC ## 0.0 0.5 -0.4 ## patientD conditiontreat patientB:conditiontreat ## -0.6 0.0 0.0 ## patientC:conditiontreat patientD:conditiontreat ## 0.0 0.0 # interaction ( bloodA_treat - bloodA_control ) - ( bloodO_treat - bloodO_control ) ## (Intercept) patientB patientC ## 0.0 0.0 -0.1 ## patientD conditiontreat patientB:conditiontreat ## 0.1 0.0 0.5 ## patientC:conditiontreat patientD:conditiontreat ## -0.5 -0.5 Part of this lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . In addition, we would like to thank Hugo Tavares from the Bioinformatics Training Facility of the University of Cambridge.","title":"Extra: imbalanced design"},{"location":"07a_DEA.html","text":"Approximate time: 60 minutes Learning Objectives \u00b6 Explain the different steps involved in running DESeq() Examine size factors and undertand the source of differences Inspect gene-level dispersion estimates Recognize the importance of dispersion during differential expression analysis DESeq2 differential gene expression analysis workflow \u00b6 Previously, we created the DESeq2 object using the appropriate design formula. # DO NOT RUN ## Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( data , colData = meta , design = ~ sampletype ) Then, to run the actual differential expression analysis, we use a single call to the function DESeq() . ## Run analysis dds <- DESeq ( dds ) And with that we completed the entire workflow for the differential gene expression analysis with DESeq2! The DESeq() function performs a default analysis through the following steps: Estimation of size factors: estimateSizeFactors() Estimation of dispersion: estimateDispersions() Negative Binomial GLM fitting and Wald statistics: nbinomWaldTest() We will be taking a detailed look at each of these steps to better understand how DESeq2 is performing the statistical analysis and what metrics we should examine to explore the quality of our analysis. Step 1: Estimate size factors \u00b6 The first step in the differential expression analysis is to estimate the size factors, which is exactly what we already did to normalize the raw counts. DESeq2 will automatically estimate the size factors when performing the differential expression analysis. However, if you have already generated the size factors using estimateSizeFactors() , as we did earlier, then DESeq2 will use these values. To normalize the count data, DESeq2 calculates size factors for each sample using the median of ratios method discussed previously in the \u2018Count normalization\u2019 lesson. MOV10 DE analysis: examining the size factors \u00b6 Let\u2019s take a quick look at size factor values we have for each sample: ## Check the size factors sizeFactors(dds) Irrel_kd_1 Irrel_kd_2 Irrel_kd_3 Mov10_kd_2 Mov10_kd_3 Mov10_oe_1 Mov10_oe_2 1.1149694 0.9606733 0.7492240 1.5633640 0.9359695 1.2262649 1.1405026 Mov10_oe_3 0.6542030 These numbers should be identical to those we generated initially when we had run the function estimateSizeFactors(dds) . Take a look at the total number of reads for each sample: ## Total number of raw counts per sample colSums ( counts ( dds )) How do the numbers correlate with the size factor? We see that the larger size factors correspond to the samples with higher sequencing depth, which makes sense, because to generate our normalized counts we need to divide the counts by the size factors. This accounts for the differences in sequencing depth between samples. Now take a look at the total depth after normalization using: ## Total number of normalized counts per sample colSums ( counts ( dds , normalized = T )) How do the values across samples compare with the total counts taken for each sample? You might have expected the counts to be the exact same across the samples after normalization. However, DESeq2 also accounts for RNA composition during the normalization procedure. By using the median ratio value for the size factor, DESeq2 should not be biased to a large number of counts sucked up by a few DE genes; however, this may lead to the size factors being quite different than what would be anticipated just based on sequencing depth. Step 2: Estimate gene-wise dispersion \u00b6 The next step in the differential expression analysis is the estimation of gene-wise dispersions. Before we get into the details, we should have a good idea about what dispersion is referring to in DESeq2. In RNA-seq count data, we know: To determine differentially expressed genes, we need to identify genes that have significantly different mean expression between groups given the variation within the groups (between replicates). The variation within group (between replicates) needs to account for the fact that variance increases with the mean expression, as shown in the plot below (each black dot is a gene). To accurately identify DE genes, DESeq2 needs to account for the relationship between the variance and mean. We don\u2019t want all of our DE genes to be genes with low counts because the variance is lower for lowly expressed genes. Instead of using variance as the measure of variation in the data ( since variance correlates with gene expression level ), DESeq2 uses a measure of variation called dispersion, which accounts for a gene\u2019s variance and mean expression level . Dispersion is calculated by Var = \u03bc + \u03b1*\u03bc^2 , where \u03b1 = dispersion, Var = variance, and \u03bc = mean, giving the relationship: Effect on dispersion Variance increases Dispersion increases Mean expression increases Dispersion decreases For genes with moderate to high count values, the square root of dispersion will be equal to the coefficient of variation. So 0.01 dispersion means 10% variation around the mean expected across biological replicates. The dispersion estimates for genes with the same mean will differ only based on their variance. Therefore, the dispersion estimates reflect the variance in gene expression for a given mean value. In the plot below, each black dot is a gene, and the dispersion is plotted against the mean expression (across within-group replicates) for each gene. How does the dispersion relate to our model? To accurately model sequencing counts, we need to generate accurate estimates of within-group variation (variation between replicates of the same sample group) for each gene. With only a few (3-6) replicates per group, the estimates of variation for each gene are often unreliable . To address this problem, DESeq2 shares information across genes to generate more accurate estimates of variation based on the mean expression level of the gene using a method called \u2018shrinkage\u2019. DESeq2 assumes that genes with similar expression levels should have similar dispersion. Estimating the dispersion for each gene separately: DESeq2 estimates the dispersion for each gene based on the gene\u2019s expression level (mean counts of within-group replicates) and variance. Step 3a: Fit curve to gene-wise dispersion estimates \u00b6 The next step in the workflow is to fit a curve to the gene-wise dispersion estimates. The idea behind fitting a curve to the data is that different genes will have different scales of biological variability, but, across all genes, there will be a distribution of reasonable estimates of dispersion. This curve is displayed as a red line in the figure below, which plots the estimate for the expected dispersion value for genes of a given expression strength . Each black dot is a gene with an associated mean expression level and maximum likelihood estimation (MLE) of the dispersion (Step 1). Step 3b: Shrink gene-wise dispersion estimates toward the values predicted by the curve \u00b6 The next step in the workflow is to shrink the gene-wise dispersion estimates toward the expected dispersion values. The curve allows for more accurate identification of differentially expressed genes when sample sizes are small, and the strength of the shrinkage for each gene depends on : how close gene dispersions are from the curve sample size (more samples = less shrinkage) This shrinkage method is particularly important to reduce false positives in the differential expression analysis. Genes with low dispersion estimates are shrunken towards the curve, and the more accurate, higher shrunken values are output for fitting of the model and differential expression testing. These shrunken estimates represent the within-group variation that is needed to determine whether the gene expression across groups is significantly different. Dispersion estimates that are slightly above the curve are also shrunk toward the curve for better dispersion estimation; however, genes with extremely high dispersion values are not . This is due to the likelihood that the gene does not follow the modeling assumptions and has higher variability than others for biological or technical reasons [ 1 ]. Shrinking the values toward the curve could result in false positives, so these values are not shrunken. These genes are shown surrounded by blue circles below. This is a good plot to evaluate whether your data is a good fit for the DESeq2 model. You expect your data to generally scatter around the curve, with the dispersion decreasing with increasing mean expression levels. If you see a cloud or different shapes, then you might want to explore your data more to see if you have contamination or outlier samples. Note how much shrinkage you get across the whole range of means in the plotDispEsts() plot for any experiment with low degrees of freedom. Examples of worrisome dispersion plots are shown below: The plot below shows a cloud of dispersion values, which do not generally follow the curve. This would suggest a bad fit of the data to the model. The next plot shows the dispersion values initially decreasing, then increasing with larger expression values. The larger mean expression values should not have larger dispersions based on our expectations - we expect decreasing dispersions with increasing mean. This indicates that there is less variation for more highly expressed genes than expected. This also indicates that there could be an outlier sample or contamination present in our analysis. MOV10 DE analysis: exploring the dispersion estimates and assessing model fit \u00b6 Let\u2019s take a look at the dispersion estimates for our MOV10 data: ## Plot dispersion estimates plotDispEsts ( dds ) Since we have a small sample size, for many genes we see quite a bit of shrinkage. Do you think our data are a good fit for the model? We see a nice decrease in dispersion with increasing mean expression, which is good. We also see the dispersion estimates generally surround the curve, which is also expected. Overall, this plot looks good. We do see strong shrinkage, which is likely due to the fact that we have only two replicates for one of our sample groups. The more replicates we have, the less shrinkage is applied to the dispersion estimates, and the more DE genes are able to be identified. We would generally recommend having at least 4 biological replicates per condition for better estimation of variation. Exercise Given the dispersion plot below, would you have any concerns regarding the fit of your data to the model? If not, what aspects of the plot makes you feel confident about your data? If so, what are your concerns? What would you do to address them? This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . Some materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website","title":"DESeq2"},{"location":"07a_DEA.html#learning-objectives","text":"Explain the different steps involved in running DESeq() Examine size factors and undertand the source of differences Inspect gene-level dispersion estimates Recognize the importance of dispersion during differential expression analysis","title":"Learning Objectives"},{"location":"07a_DEA.html#deseq2-differential-gene-expression-analysis-workflow","text":"Previously, we created the DESeq2 object using the appropriate design formula. # DO NOT RUN ## Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( data , colData = meta , design = ~ sampletype ) Then, to run the actual differential expression analysis, we use a single call to the function DESeq() . ## Run analysis dds <- DESeq ( dds ) And with that we completed the entire workflow for the differential gene expression analysis with DESeq2! The DESeq() function performs a default analysis through the following steps: Estimation of size factors: estimateSizeFactors() Estimation of dispersion: estimateDispersions() Negative Binomial GLM fitting and Wald statistics: nbinomWaldTest() We will be taking a detailed look at each of these steps to better understand how DESeq2 is performing the statistical analysis and what metrics we should examine to explore the quality of our analysis.","title":"DESeq2 differential gene expression analysis workflow"},{"location":"07a_DEA.html#step-1-estimate-size-factors","text":"The first step in the differential expression analysis is to estimate the size factors, which is exactly what we already did to normalize the raw counts. DESeq2 will automatically estimate the size factors when performing the differential expression analysis. However, if you have already generated the size factors using estimateSizeFactors() , as we did earlier, then DESeq2 will use these values. To normalize the count data, DESeq2 calculates size factors for each sample using the median of ratios method discussed previously in the \u2018Count normalization\u2019 lesson.","title":"Step 1: Estimate size factors"},{"location":"07a_DEA.html#mov10-de-analysis-examining-the-size-factors","text":"Let\u2019s take a quick look at size factor values we have for each sample: ## Check the size factors sizeFactors(dds) Irrel_kd_1 Irrel_kd_2 Irrel_kd_3 Mov10_kd_2 Mov10_kd_3 Mov10_oe_1 Mov10_oe_2 1.1149694 0.9606733 0.7492240 1.5633640 0.9359695 1.2262649 1.1405026 Mov10_oe_3 0.6542030 These numbers should be identical to those we generated initially when we had run the function estimateSizeFactors(dds) . Take a look at the total number of reads for each sample: ## Total number of raw counts per sample colSums ( counts ( dds )) How do the numbers correlate with the size factor? We see that the larger size factors correspond to the samples with higher sequencing depth, which makes sense, because to generate our normalized counts we need to divide the counts by the size factors. This accounts for the differences in sequencing depth between samples. Now take a look at the total depth after normalization using: ## Total number of normalized counts per sample colSums ( counts ( dds , normalized = T )) How do the values across samples compare with the total counts taken for each sample? You might have expected the counts to be the exact same across the samples after normalization. However, DESeq2 also accounts for RNA composition during the normalization procedure. By using the median ratio value for the size factor, DESeq2 should not be biased to a large number of counts sucked up by a few DE genes; however, this may lead to the size factors being quite different than what would be anticipated just based on sequencing depth.","title":"MOV10 DE analysis: examining the size factors"},{"location":"07a_DEA.html#step-2-estimate-gene-wise-dispersion","text":"The next step in the differential expression analysis is the estimation of gene-wise dispersions. Before we get into the details, we should have a good idea about what dispersion is referring to in DESeq2. In RNA-seq count data, we know: To determine differentially expressed genes, we need to identify genes that have significantly different mean expression between groups given the variation within the groups (between replicates). The variation within group (between replicates) needs to account for the fact that variance increases with the mean expression, as shown in the plot below (each black dot is a gene). To accurately identify DE genes, DESeq2 needs to account for the relationship between the variance and mean. We don\u2019t want all of our DE genes to be genes with low counts because the variance is lower for lowly expressed genes. Instead of using variance as the measure of variation in the data ( since variance correlates with gene expression level ), DESeq2 uses a measure of variation called dispersion, which accounts for a gene\u2019s variance and mean expression level . Dispersion is calculated by Var = \u03bc + \u03b1*\u03bc^2 , where \u03b1 = dispersion, Var = variance, and \u03bc = mean, giving the relationship: Effect on dispersion Variance increases Dispersion increases Mean expression increases Dispersion decreases For genes with moderate to high count values, the square root of dispersion will be equal to the coefficient of variation. So 0.01 dispersion means 10% variation around the mean expected across biological replicates. The dispersion estimates for genes with the same mean will differ only based on their variance. Therefore, the dispersion estimates reflect the variance in gene expression for a given mean value. In the plot below, each black dot is a gene, and the dispersion is plotted against the mean expression (across within-group replicates) for each gene. How does the dispersion relate to our model? To accurately model sequencing counts, we need to generate accurate estimates of within-group variation (variation between replicates of the same sample group) for each gene. With only a few (3-6) replicates per group, the estimates of variation for each gene are often unreliable . To address this problem, DESeq2 shares information across genes to generate more accurate estimates of variation based on the mean expression level of the gene using a method called \u2018shrinkage\u2019. DESeq2 assumes that genes with similar expression levels should have similar dispersion. Estimating the dispersion for each gene separately: DESeq2 estimates the dispersion for each gene based on the gene\u2019s expression level (mean counts of within-group replicates) and variance.","title":"Step 2: Estimate gene-wise dispersion"},{"location":"07a_DEA.html#step-3a-fit-curve-to-gene-wise-dispersion-estimates","text":"The next step in the workflow is to fit a curve to the gene-wise dispersion estimates. The idea behind fitting a curve to the data is that different genes will have different scales of biological variability, but, across all genes, there will be a distribution of reasonable estimates of dispersion. This curve is displayed as a red line in the figure below, which plots the estimate for the expected dispersion value for genes of a given expression strength . Each black dot is a gene with an associated mean expression level and maximum likelihood estimation (MLE) of the dispersion (Step 1).","title":"Step 3a: Fit curve to gene-wise dispersion estimates"},{"location":"07a_DEA.html#step-3b-shrink-gene-wise-dispersion-estimates-toward-the-values-predicted-by-the-curve","text":"The next step in the workflow is to shrink the gene-wise dispersion estimates toward the expected dispersion values. The curve allows for more accurate identification of differentially expressed genes when sample sizes are small, and the strength of the shrinkage for each gene depends on : how close gene dispersions are from the curve sample size (more samples = less shrinkage) This shrinkage method is particularly important to reduce false positives in the differential expression analysis. Genes with low dispersion estimates are shrunken towards the curve, and the more accurate, higher shrunken values are output for fitting of the model and differential expression testing. These shrunken estimates represent the within-group variation that is needed to determine whether the gene expression across groups is significantly different. Dispersion estimates that are slightly above the curve are also shrunk toward the curve for better dispersion estimation; however, genes with extremely high dispersion values are not . This is due to the likelihood that the gene does not follow the modeling assumptions and has higher variability than others for biological or technical reasons [ 1 ]. Shrinking the values toward the curve could result in false positives, so these values are not shrunken. These genes are shown surrounded by blue circles below. This is a good plot to evaluate whether your data is a good fit for the DESeq2 model. You expect your data to generally scatter around the curve, with the dispersion decreasing with increasing mean expression levels. If you see a cloud or different shapes, then you might want to explore your data more to see if you have contamination or outlier samples. Note how much shrinkage you get across the whole range of means in the plotDispEsts() plot for any experiment with low degrees of freedom. Examples of worrisome dispersion plots are shown below: The plot below shows a cloud of dispersion values, which do not generally follow the curve. This would suggest a bad fit of the data to the model. The next plot shows the dispersion values initially decreasing, then increasing with larger expression values. The larger mean expression values should not have larger dispersions based on our expectations - we expect decreasing dispersions with increasing mean. This indicates that there is less variation for more highly expressed genes than expected. This also indicates that there could be an outlier sample or contamination present in our analysis.","title":"Step 3b: Shrink gene-wise dispersion estimates toward the values predicted by the curve"},{"location":"07a_DEA.html#mov10-de-analysis-exploring-the-dispersion-estimates-and-assessing-model-fit","text":"Let\u2019s take a look at the dispersion estimates for our MOV10 data: ## Plot dispersion estimates plotDispEsts ( dds ) Since we have a small sample size, for many genes we see quite a bit of shrinkage. Do you think our data are a good fit for the model? We see a nice decrease in dispersion with increasing mean expression, which is good. We also see the dispersion estimates generally surround the curve, which is also expected. Overall, this plot looks good. We do see strong shrinkage, which is likely due to the fact that we have only two replicates for one of our sample groups. The more replicates we have, the less shrinkage is applied to the dispersion estimates, and the more DE genes are able to be identified. We would generally recommend having at least 4 biological replicates per condition for better estimation of variation. Exercise Given the dispersion plot below, would you have any concerns regarding the fit of your data to the model? If not, what aspects of the plot makes you feel confident about your data? If so, what are your concerns? What would you do to address them? This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . Some materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website","title":"MOV10 DE analysis: exploring the dispersion estimates and assessing model fit"},{"location":"07b_hypothesis_testing.html","text":"Approximate time: 90 minutes Learning Objectives \u00b6 Describe the process of model fitting Compare two methods for hypothesis testing (Wald test vs. LRT) Discuss the steps required to generate a results table for pairwise comparisons (Wald test) Recognize the importance of multiple test correction Identify different methods for multiple test correction Summarize the different levels of gene filtering Evaluate the number of differentially expressed genes produced for each comparison Construct R objects containing significant genes from each comparison DESeq2: Model fitting and Hypothesis testing \u00b6 The final step in the DESeq2 workflow is taking the counts for each gene and fitting it to the model and testing for differential expression. Generalized Linear Model \u00b6 As described earlier , the count data generated by RNA-seq exhibits overdispersion (variance > mean) and the statistical distribution used to model the counts needs to account for this. As such, DESeq2 uses a negative binomial distribution to model the RNA-seq counts using the equation below : The two parameters required are the size factor, and the dispersion estimate . Next, a generalized linear model (GLM) of the NB family is used to fit the data. Modeling is a mathematically formalized way to approximate how the data behaves given a set of parameters. \u201c In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\u201d ( Wikipedia ) . After the model is fit, coefficients are estimated for each sample group along with their standard error. The coefficents are the estimates for the log2 fold changes , and will be used as input for hypothesis testing. Hypothesis testing \u00b6 The first step in hypothesis testing is to set up a null hypothesis for each gene. In our case, the null hypothesis is that there is no differential expression across the two sample groups (LFC == 0) . Notice that we can do this without observing any data, because it is based on a thought experiment. Second, we use a statistical test to determine if based on the observed data, the null hypothesis can be rejected. Wald test \u00b6 In DESeq2, the Wald test is the default used for hypothesis testing when comparing two groups . The Wald test is a test usually performed on parameters that have been estimated by maximum likelihood. In our case we are testing each gene model coefficient (LFC) which was derived using parameters like dispersion, which were estimated using maximum likelihood. DESeq2 implements the Wald test by: * Taking the LFC and dividing it by its standard error, resulting in a z-statistic * The z-statistic is compared to a standard normal distribution, and a p-value is computed reporting the probability that a z-statistic at least as extreme as the observed value would be selected at random * If the p-value is small we reject the null hypothesis and state that there is evidence against the null (i.e. the gene is differentially expressed). The model fit and Wald test were already run previously as part of the DESeq() function : ## DO NOT RUN THIS CODE ## Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( data [, -1 ], rowData = data [, 1 ], colData = meta , design = ~ sampletype ) ## Run analysis dds <- DESeq ( dds ) Likelihood ratio test (LRT) \u00b6 An alternative to pair-wise comparisons is to analyze all levels of a factor at once . By default, the Wald test is used to generate the results table, but DESeq2 also offers the LRT which is used to identify any genes that show change in expression across the different levels. This type of test can be especially useful in analyzing time course experiments. How does this compare to the Wald test? The Wald test evaluates whether a gene\u2019s expression is up- or down-regulated in one group compared to another, while the LRT identifies genes that are changing in expression in any combination across the different sample groups . For example: Gene expression is equal between Control and Overexpression, but is less in Knockdown Gene expression of Overexpression is lower than Control and Control is lower than Knockdown Gene expression of Knockdown is equal to Control, but less than Overexpression. Using LRT, we are evaluating the null hypothesis that the full model fits just as well as the reduced model . If we reject the null hypothesis, this suggests that there is a significant amount of variation explained by the full model (and our main factor of interest), therefore the gene is differentially expressed across the different levels. Generally, this test will result in a larger number of genes than the individual pair-wise comparisons. However, while the LRT is a test of significance for differences of any level of the factor, one should not expect it to be exactly equal to the union of sets of genes using Wald tests (although we do expect a majority overlap). To use the LRT, we use the DESeq() function but this time adding two arguments: specifying that we want to use the LRT test the \u2018reduced\u2019 model # The full model was specified previously with the `design = ~ sampletype`: # dds <- DESeqDataSetFromMatrix(data, colData = meta, ~ sampletype) # Likelihood ratio test dds_lrt <- DESeq ( dds , test = \"LRT\" , reduced = ~ 1 ) Since our \u2018full\u2019 model only has one factor ( sampletype ), the \u2018reduced\u2019 model (removing that factor) is just the intercept (~ 1). You will find that similar columns are reported for the LRT test. One thing to note is, even though there are fold changes present they are not directly associated with the actual hypothesis test. **Time course analyses with LRT** The LRT test can be especially helpful when performing time course analyses. We can use the LRT to explore whether there are any significant differences in treatment effect between any of the timepoints. For have an experiment looking at the effect of treatment over time on mice of two different genotypes. We could use a design formula for our \u2018full model\u2019 that would include the major sources of variation in our data: genotype, treatment, time, and our main condition of interest, which is the difference in the effect of treatment over time (treatment:time). full_model <- ~ genotype + treatment + time + treatment : time To perform the LRT test, we can determine all genes that have significant differences in expression between treatments across time points by giving the \u2018reduced model\u2019 without the treatment:time term: reduced_model <- ~ genotype + treatment + time Then, we could run our test by using the following code: dds_lrt <- DESeqDataSetFromMatrix ( countData = data , colData = metadata , design = ~ genotype + treatment + time + treatment : time ) dds_lrt_time <- DESeq ( dds_lrt , test = \"LRT\" , reduced = ~ genotype + treatment + time ) This analysis will not return genes where the treatment effect does not change over time, even though the genes may be differentially expressed between groups at a particular time point, as shown in the figure below: knitr :: include_graphics ( \"./img/07b_hypothesis_testing/lrt_time_nodiff.png\" ) The significant DE genes will represent those genes that have differences in the effect of treatment over time, an example is displayed in the figure below: knitr :: include_graphics ( \"./img/07b_hypothesis_testing/lrt_time_yesdiff.png\" ) Exercise You are studying brain maturation and growth patterns in mouse cortex and have obtained RNA-seq data for a total of 31 mice. These samples were acquired at 9 developmental stages during the postnatal period of 2-40 days of growth, with at least three replicates at each stage. You also have sex information for these mice (16 males and 15 females). What is an appropriate hypothesis test if you are testing for expression differences across the developmental stages? Provide the line of code used to create the dds object. Provide the line of code used to run DESeq2. The results of the differential expression analysis run identifies a group of genes that spike in expression between the first and second timepoints with no change in expression thereafter. How would we go about obtaining fold changes for these genes? Multiple test correction \u00b6 Regardless of whether we use the Wald test or the LRT, each gene that has been tested will be associated with a p-value. It is this result which we use to determine which genes are considered significantly differentially expressed. However, we cannot use the p-value directly. What does the p-value mean? \u00b6 A gene with a significance cut-off of p \\< 0.05, means there is a 5% chance it is a false positive. For example, if we test 20,000 genes for differential expression, at p \\< 0.05 we would expect to find 1,000 DE genes by chance. If we found 3000 genes to be differentially expressed total, roughly one third of our genes are false positives! We would not want to sift through our \u201csignificant\u201d genes to identify which ones are true positives. Since each p-value is the result of a single test (single gene). The more genes we test, the more we inflate the false positive rate. This is the multiple testing problem. Correcting the p-value for multiple testing \u00b6 There are a few common approaches for multiple test correction: Bonferroni: The adjusted p-value is calculated by: p-value * m (m = total number of tests). This is a very conservative approach with a high probability of false negatives , so is generally not recommended. FDR/Benjamini-Hochberg: Benjamini and Hochberg (1995) defined the concept of False Discovery Rate (FDR) and created an algorithm to control the expected FDR below a specified level given a list of independent p-values. More info about BH . Q-value / Storey method: The minimum FDR that can be attained when calling that feature significant. For example, if gene X has a q-value of 0.013 it means that 1.3% of genes that show p-values at least as small as gene X are false positives. DESeq2 helps reduce the number of genes tested by removing those genes unlikely to be significantly DE prior to testing, such as those with low number of counts and outlier samples ( see below ). However, multiple test correction is also implemented to reduce the False Discovery Rate using an interpretation of the Benjamini-Hochberg procedure. So what does FDR \\< 0.05 mean? By setting the FDR cutoff to \\< 0.05, we\u2019re saying that the proportion of false positives we expect amongst our differentially expressed genes is 5%. For example, if you call 500 genes as differentially expressed with an FDR cutoff of 0.05, you expect 25 of them to be false positives. Exploring Results (Wald test) \u00b6 By default DESeq2 uses the Wald test to identify genes that are differentially expressed between two sample groups. Given the factor(s) used in the design formula, and how many factor levels are present, we can extract results for a number of different comparisons. Here, we will walk you through how to obtain results from the dds object and provide some explanations on how to interpret them. NOTE: The Wald test can also be used with continuous variables . If the variable of interest provided in the design formula is continuous-valued, then the reported log2FoldChange is per unit of change of that variable. Specifying contrasts \u00b6 In our dataset, we have three sample groups so we can make three possible pairwise comparisons: Control vs. Mov10 overexpression Control vs. Mov10 knockdown Mov10 knockdown vs. Mov10 overexpression We are really only interested in #1 and #2 from above . When we intially created our dds object we had provided ~ sampletype as our design formula, indicating that sampletype is our main factor of interest. To indicate which two sample groups we are interested in comparing, we need to specify contrasts . The contrasts are used as input to the DESeq2 results() function to extract the desired results. Contrasts can be specified in two different ways (with the first method more commonly used): Contrasts can be supplied as a character vector with exactly three elements : the name of the factor (of interest) in the design formula, the name of the two factors levels to compare. The factor level given last is the base level for the comparison. The syntax is given below: # DO NOT RUN! contrast <- c ( \"condition\" , \"level_to_compare\" , \"base_level\" ) results ( dds , contrast = contrast ) Contrasts can be given as a list of 2 character vectors : the names of the fold changes for the level of ineterest, and the names of the fold changes for the base level. These names should match identically to the elements of resultsNames(object) . This method can be useful for combining interaction terms and main effects. # DO NOT RUN! resultsNames ( dds ) # to see what names to use contrast <- list ( resultsNames ( dds )[ 1 ], resultsNames ( dds )[ 2 ]) results ( dds , contrast = contrast ) Alternatively, if you only had two factor levels you could do nothing and not worry about specifying contrasts (i.e. results(dds) ). In this case, DESeq2 will choose what your base factor level based on alphabetical order of the levels. To start, we want to evaluate expression changes between the MOV10 overexpression samples and the control samples . As such we will use the first method for specifcying contrasts and create a character vector: ## Define contrasts for MOV10 overexpression contrast_oe <- c ( \"sampletype\" , \"MOV10_overexpression\" , \"control\" ) Does it matter what I choose to be my base level? \u00b6 Yes, it does matter. Deciding what level is the base level will determine how to interpret the fold change that is reported. So for example, if we observe a log2 fold change of -2 this would mean the gene expression is lower in factor level of interest relative to the base level. Thus, if leaving it up to DESeq2 to decide on the contrasts be sure to check that the alphabetical order coincides with the fold change direction you are anticipating. The results table \u00b6 Now that we have our contrast created, we can use it as input to the results() function. Let\u2019s take a quick look at the help manual for the function: ? results You will see we have the option to provide a wide array of arguments and tweak things from the defaults as needed. As we go through the lesson we will keep coming back to the help documentation to discuss some arguments that are good to know about. ## Extract results for MOV10 overexpression vs control res_tableOE <- results ( dds , contrast = contrast_oe , alpha = 0.05 ) NOTE: For our analysis, in addition to the contrast argument we will also provide a value of 0.05 for the alpha argument. We will describe this in more detail when we talk about gene-level filtering. The results table that is returned to us is a DESeqResults object , which is a simple subclass of DataFrame. In many ways it can be treated like a dataframe (i.e when accessing/subsetting data), however it is important to recognize that there are differences for downstream steps like visualization. # Check what type of object is returned class ( res_tableOE ) Now let\u2019s take a look at what information is stored in the results: # What is stored in results? res_tableOE %>% data.frame () %>% head () We have six columns of information reported for each gene (row). We can use the mcols() function to extract information on what the values stored in each column represent: # Get information on each column in results mcols ( res_tableOE , use.names = T ) baseMean : mean of normalized counts for all samples log2FoldChange : log2 fold change lfcSE : standard error stat : Wald statistic pvalue : Wald test p-value padj : BH adjusted p-values P-values \u00b6 The p-value is a probability value used to determine whether there is evidence to reject the null hypothesis. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis . The padj column in the results table represents the p-value adjusted for multiple testing, and is the most important column of the results. Typically, a threshold such as padj \\< 0.05 is a good starting point for identifying significant genes. The default method for multiple test correction in DESeq2 is the FDR. Other methods can be used with the pAdjustMethod argument in the results() function. Gene-level filtering \u00b6 Let\u2019s take a closer look at our results table. As we scroll through it, you will notice that for selected genes there are NA values in the pvalue and padj columns . What does this mean? The missing values represent genes that have undergone filtering as part of the DESeq() function. Prior to differential expression analysis it is beneficial to omit genes that have little or no chance of being detected as differentially expressed. This will increase the power to detect differentially expressed genes. DESeq2 does not physically remove any genes from the original counts matrix, and so all genes will be present in your results table. The genes omitted by DESeq2 meet one of the three filtering criteria outlined below : 1. Genes with zero counts in all samples If within a row, all samples have zero counts there is no expression information and therefore these genes are not tested. # Filter genes by zero expression res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% dplyr :: filter ( baseMean == 0 ) %>% head () The baseMean column for these genes will be zero, and the log2 fold change estimates, p-value and adjusted p-value will all be set to NA. 2. Genes with an extreme count outlier The DESeq() function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook\u2019s distance. Cook\u2019s distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook\u2019s distance is intended to indicate an outlier count. Genes which contain a Cook\u2019s distance above a threshold are flagged, however at least 3 replicates are required for flagging, as it is difficult to judge which sample might be an outlier with only 2 replicates. We can turn off this filtering by using the cooksCutoff argument in the results() function. # Filter genes that have an extreme outlier res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% dplyr :: filter ( is.na ( pvalue ) & is.na ( padj ) & baseMean > 0 ) %>% head () If a gene contains a sample with an extreme count outlier then the p-value and adjusted p-value will be set to NA. 3. Genes with a low mean normalized counts DESeq2 defines a low mean threshold, that is empirically determined from your data, in which the fraction of significant genes can be increased by reducing the number of genes that are considered for multiple testing. This is based on the notion that genes with very low counts are not likely to see significant differences typically due to high dispersion. Image courtesy of slideshare presentation from Joachim Jacob, 2014. At a user-specified value ( alpha = 0.05 ), DESeq2 evaluates the change in the number of significant genes as it filters out increasingly bigger portions of genes based on their mean counts, as shown in the figure above. The point at which the number of significant genes reaches a peak is the low mean threshold that is used to filter genes that undergo multiple testing. There is also an argument to turn off the filtering off by setting independentFiltering = F . # Filter genes below the low mean threshold res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% dplyr :: filter ( ! is.na ( pvalue ) & is.na ( padj ) & baseMean > 0 ) %>% head () If a gene is filtered by independent filtering, then only the adjusted p-value will be set to NA. NOTE: DESeq2 will perform the filtering outlined above by default; however other DE tools, such as EdgeR will not. Filtering is a necessary step, even if you are using limma-voom and/or edgeR\u2019s quasi-likelihood methods. Be sure to follow pre-filtering steps when using other tools, as outlined in their user guides found on Bioconductor as they generally perform much better. Fold changes \u00b6 Another important column in the results table, is the log2FoldChange . With large significant gene lists it can be hard to extract meaningful biological relevance. To help increase stringency, one can also add a fold change threshold . Keep in mind when setting that value that we are working with log2 fold changes, so a cutoff of log2FoldChange \\< 1 would translate to an actual fold change of 2. An alternative approach to add the fold change threshold: \u00b6 The results() function has an option to add a fold change threshold using the lfcThrehsold argument. This method is more statistically motivated, and is recommended when you want a more confident set of genes based on a certain fold-change. It actually performs a statistical test against the desired threshold, by performing a two-tailed test for log2 fold changes greater than the absolute value specified. The user can change the alternative hypothesis using altHypothesis and perform two one-tailed tests as well. This is a more conservative approach, so expect to retrieve a much smaller set of genes! The fold changes reported in the results table are calculated by: log2 ( normalized_counts_group1 / normalized_counts_group2 ) Summarizing results \u00b6 To summarize the results table, a handy function in DESeq2 is summary() . Confusingly it has the same name as the function used to inspect data frames. This function, when called with a DESeq results table as input, will summarize the results using a default threshold of padj \\< 0.1. However, since we had set the alpha argument to 0.05 when creating our results table threshold: FDR \\< 0.05 (padj/FDR is used even though the output says p-value < 0.05 ). Let\u2019s start with the OE vs control results: ## Summarize results summary ( res_tableOE , alpha = 0.05 ) In addition to the number of genes up- and down-regulated at the default threshold, the function also reports the number of genes that were tested (genes with non-zero total read count), and the number of genes not included in multiple test correction due to a low mean count . Extracting significant differentially expressed genes \u00b6 Let\u2019s first create variables that contain our threshold criteria. We will only be using the adjusted p-values in our criteria: ### Set thresholds padj.cutoff <- 0.05 We can easily subset the results table to only include those that are significant using the dplyr::filter() function, but first we will convert the results table into a tibble: # Create a tibble of results and add gene symbols to new object res_tableOE_tb <- res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% relocate ( gene , .before = baseMean ) Now we can subset that table to only keep the significant genes using our pre-defined thresholds: # Subset the tibble to keep only significant genes sigOE <- res_tableOE_tb %>% dplyr :: filter ( padj < padj.cutoff ) # Take a quick look at this tibble sigOE Now that we have extracted the significant results, we are ready for visualization! Excercise MOV10 Differential Expression Analysis: Control versus Knockdown Now that we have results for the overexpression results, do the same for the Control vs. Knockdown samples . Create a contrast vector called contrast_kd . Use contrast vector in the results() to extract a results table and store that to a variable called res_tableKD . Using a p-adjusted threshold of 0.05 ( padj.cutoff < 0.05 ), subset res_tableKD to report the number of genes that are up- and down-regulated in Mov10_knockdown compared to control. How many genes are differentially expressed in the Knockdown compared to Control? How does this compare to the overexpression significant gene list (in terms of numbers)? This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . Some materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website","title":"Hypothesis testing"},{"location":"07b_hypothesis_testing.html#learning-objectives","text":"Describe the process of model fitting Compare two methods for hypothesis testing (Wald test vs. LRT) Discuss the steps required to generate a results table for pairwise comparisons (Wald test) Recognize the importance of multiple test correction Identify different methods for multiple test correction Summarize the different levels of gene filtering Evaluate the number of differentially expressed genes produced for each comparison Construct R objects containing significant genes from each comparison","title":"Learning Objectives"},{"location":"07b_hypothesis_testing.html#deseq2-model-fitting-and-hypothesis-testing","text":"The final step in the DESeq2 workflow is taking the counts for each gene and fitting it to the model and testing for differential expression.","title":"DESeq2: Model fitting and Hypothesis testing"},{"location":"07b_hypothesis_testing.html#generalized-linear-model","text":"As described earlier , the count data generated by RNA-seq exhibits overdispersion (variance > mean) and the statistical distribution used to model the counts needs to account for this. As such, DESeq2 uses a negative binomial distribution to model the RNA-seq counts using the equation below : The two parameters required are the size factor, and the dispersion estimate . Next, a generalized linear model (GLM) of the NB family is used to fit the data. Modeling is a mathematically formalized way to approximate how the data behaves given a set of parameters. \u201c In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\u201d ( Wikipedia ) . After the model is fit, coefficients are estimated for each sample group along with their standard error. The coefficents are the estimates for the log2 fold changes , and will be used as input for hypothesis testing.","title":"Generalized Linear Model"},{"location":"07b_hypothesis_testing.html#hypothesis-testing","text":"The first step in hypothesis testing is to set up a null hypothesis for each gene. In our case, the null hypothesis is that there is no differential expression across the two sample groups (LFC == 0) . Notice that we can do this without observing any data, because it is based on a thought experiment. Second, we use a statistical test to determine if based on the observed data, the null hypothesis can be rejected.","title":"Hypothesis testing"},{"location":"07b_hypothesis_testing.html#wald-test","text":"In DESeq2, the Wald test is the default used for hypothesis testing when comparing two groups . The Wald test is a test usually performed on parameters that have been estimated by maximum likelihood. In our case we are testing each gene model coefficient (LFC) which was derived using parameters like dispersion, which were estimated using maximum likelihood. DESeq2 implements the Wald test by: * Taking the LFC and dividing it by its standard error, resulting in a z-statistic * The z-statistic is compared to a standard normal distribution, and a p-value is computed reporting the probability that a z-statistic at least as extreme as the observed value would be selected at random * If the p-value is small we reject the null hypothesis and state that there is evidence against the null (i.e. the gene is differentially expressed). The model fit and Wald test were already run previously as part of the DESeq() function : ## DO NOT RUN THIS CODE ## Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( data [, -1 ], rowData = data [, 1 ], colData = meta , design = ~ sampletype ) ## Run analysis dds <- DESeq ( dds )","title":"Wald test"},{"location":"07b_hypothesis_testing.html#likelihood-ratio-test-lrt","text":"An alternative to pair-wise comparisons is to analyze all levels of a factor at once . By default, the Wald test is used to generate the results table, but DESeq2 also offers the LRT which is used to identify any genes that show change in expression across the different levels. This type of test can be especially useful in analyzing time course experiments. How does this compare to the Wald test? The Wald test evaluates whether a gene\u2019s expression is up- or down-regulated in one group compared to another, while the LRT identifies genes that are changing in expression in any combination across the different sample groups . For example: Gene expression is equal between Control and Overexpression, but is less in Knockdown Gene expression of Overexpression is lower than Control and Control is lower than Knockdown Gene expression of Knockdown is equal to Control, but less than Overexpression. Using LRT, we are evaluating the null hypothesis that the full model fits just as well as the reduced model . If we reject the null hypothesis, this suggests that there is a significant amount of variation explained by the full model (and our main factor of interest), therefore the gene is differentially expressed across the different levels. Generally, this test will result in a larger number of genes than the individual pair-wise comparisons. However, while the LRT is a test of significance for differences of any level of the factor, one should not expect it to be exactly equal to the union of sets of genes using Wald tests (although we do expect a majority overlap). To use the LRT, we use the DESeq() function but this time adding two arguments: specifying that we want to use the LRT test the \u2018reduced\u2019 model # The full model was specified previously with the `design = ~ sampletype`: # dds <- DESeqDataSetFromMatrix(data, colData = meta, ~ sampletype) # Likelihood ratio test dds_lrt <- DESeq ( dds , test = \"LRT\" , reduced = ~ 1 ) Since our \u2018full\u2019 model only has one factor ( sampletype ), the \u2018reduced\u2019 model (removing that factor) is just the intercept (~ 1). You will find that similar columns are reported for the LRT test. One thing to note is, even though there are fold changes present they are not directly associated with the actual hypothesis test. **Time course analyses with LRT** The LRT test can be especially helpful when performing time course analyses. We can use the LRT to explore whether there are any significant differences in treatment effect between any of the timepoints. For have an experiment looking at the effect of treatment over time on mice of two different genotypes. We could use a design formula for our \u2018full model\u2019 that would include the major sources of variation in our data: genotype, treatment, time, and our main condition of interest, which is the difference in the effect of treatment over time (treatment:time). full_model <- ~ genotype + treatment + time + treatment : time To perform the LRT test, we can determine all genes that have significant differences in expression between treatments across time points by giving the \u2018reduced model\u2019 without the treatment:time term: reduced_model <- ~ genotype + treatment + time Then, we could run our test by using the following code: dds_lrt <- DESeqDataSetFromMatrix ( countData = data , colData = metadata , design = ~ genotype + treatment + time + treatment : time ) dds_lrt_time <- DESeq ( dds_lrt , test = \"LRT\" , reduced = ~ genotype + treatment + time ) This analysis will not return genes where the treatment effect does not change over time, even though the genes may be differentially expressed between groups at a particular time point, as shown in the figure below: knitr :: include_graphics ( \"./img/07b_hypothesis_testing/lrt_time_nodiff.png\" ) The significant DE genes will represent those genes that have differences in the effect of treatment over time, an example is displayed in the figure below: knitr :: include_graphics ( \"./img/07b_hypothesis_testing/lrt_time_yesdiff.png\" ) Exercise You are studying brain maturation and growth patterns in mouse cortex and have obtained RNA-seq data for a total of 31 mice. These samples were acquired at 9 developmental stages during the postnatal period of 2-40 days of growth, with at least three replicates at each stage. You also have sex information for these mice (16 males and 15 females). What is an appropriate hypothesis test if you are testing for expression differences across the developmental stages? Provide the line of code used to create the dds object. Provide the line of code used to run DESeq2. The results of the differential expression analysis run identifies a group of genes that spike in expression between the first and second timepoints with no change in expression thereafter. How would we go about obtaining fold changes for these genes?","title":"Likelihood ratio test (LRT)"},{"location":"07b_hypothesis_testing.html#multiple-test-correction","text":"Regardless of whether we use the Wald test or the LRT, each gene that has been tested will be associated with a p-value. It is this result which we use to determine which genes are considered significantly differentially expressed. However, we cannot use the p-value directly.","title":"Multiple test correction"},{"location":"07b_hypothesis_testing.html#what-does-the-p-value-mean","text":"A gene with a significance cut-off of p \\< 0.05, means there is a 5% chance it is a false positive. For example, if we test 20,000 genes for differential expression, at p \\< 0.05 we would expect to find 1,000 DE genes by chance. If we found 3000 genes to be differentially expressed total, roughly one third of our genes are false positives! We would not want to sift through our \u201csignificant\u201d genes to identify which ones are true positives. Since each p-value is the result of a single test (single gene). The more genes we test, the more we inflate the false positive rate. This is the multiple testing problem.","title":"What does the p-value mean?"},{"location":"07b_hypothesis_testing.html#correcting-the-p-value-for-multiple-testing","text":"There are a few common approaches for multiple test correction: Bonferroni: The adjusted p-value is calculated by: p-value * m (m = total number of tests). This is a very conservative approach with a high probability of false negatives , so is generally not recommended. FDR/Benjamini-Hochberg: Benjamini and Hochberg (1995) defined the concept of False Discovery Rate (FDR) and created an algorithm to control the expected FDR below a specified level given a list of independent p-values. More info about BH . Q-value / Storey method: The minimum FDR that can be attained when calling that feature significant. For example, if gene X has a q-value of 0.013 it means that 1.3% of genes that show p-values at least as small as gene X are false positives. DESeq2 helps reduce the number of genes tested by removing those genes unlikely to be significantly DE prior to testing, such as those with low number of counts and outlier samples ( see below ). However, multiple test correction is also implemented to reduce the False Discovery Rate using an interpretation of the Benjamini-Hochberg procedure. So what does FDR \\< 0.05 mean? By setting the FDR cutoff to \\< 0.05, we\u2019re saying that the proportion of false positives we expect amongst our differentially expressed genes is 5%. For example, if you call 500 genes as differentially expressed with an FDR cutoff of 0.05, you expect 25 of them to be false positives.","title":"Correcting the p-value for multiple testing"},{"location":"07b_hypothesis_testing.html#exploring-results-wald-test","text":"By default DESeq2 uses the Wald test to identify genes that are differentially expressed between two sample groups. Given the factor(s) used in the design formula, and how many factor levels are present, we can extract results for a number of different comparisons. Here, we will walk you through how to obtain results from the dds object and provide some explanations on how to interpret them. NOTE: The Wald test can also be used with continuous variables . If the variable of interest provided in the design formula is continuous-valued, then the reported log2FoldChange is per unit of change of that variable.","title":"Exploring Results (Wald test)"},{"location":"07b_hypothesis_testing.html#specifying-contrasts","text":"In our dataset, we have three sample groups so we can make three possible pairwise comparisons: Control vs. Mov10 overexpression Control vs. Mov10 knockdown Mov10 knockdown vs. Mov10 overexpression We are really only interested in #1 and #2 from above . When we intially created our dds object we had provided ~ sampletype as our design formula, indicating that sampletype is our main factor of interest. To indicate which two sample groups we are interested in comparing, we need to specify contrasts . The contrasts are used as input to the DESeq2 results() function to extract the desired results. Contrasts can be specified in two different ways (with the first method more commonly used): Contrasts can be supplied as a character vector with exactly three elements : the name of the factor (of interest) in the design formula, the name of the two factors levels to compare. The factor level given last is the base level for the comparison. The syntax is given below: # DO NOT RUN! contrast <- c ( \"condition\" , \"level_to_compare\" , \"base_level\" ) results ( dds , contrast = contrast ) Contrasts can be given as a list of 2 character vectors : the names of the fold changes for the level of ineterest, and the names of the fold changes for the base level. These names should match identically to the elements of resultsNames(object) . This method can be useful for combining interaction terms and main effects. # DO NOT RUN! resultsNames ( dds ) # to see what names to use contrast <- list ( resultsNames ( dds )[ 1 ], resultsNames ( dds )[ 2 ]) results ( dds , contrast = contrast ) Alternatively, if you only had two factor levels you could do nothing and not worry about specifying contrasts (i.e. results(dds) ). In this case, DESeq2 will choose what your base factor level based on alphabetical order of the levels. To start, we want to evaluate expression changes between the MOV10 overexpression samples and the control samples . As such we will use the first method for specifcying contrasts and create a character vector: ## Define contrasts for MOV10 overexpression contrast_oe <- c ( \"sampletype\" , \"MOV10_overexpression\" , \"control\" )","title":"Specifying contrasts"},{"location":"07b_hypothesis_testing.html#does-it-matter-what-i-choose-to-be-my-base-level","text":"Yes, it does matter. Deciding what level is the base level will determine how to interpret the fold change that is reported. So for example, if we observe a log2 fold change of -2 this would mean the gene expression is lower in factor level of interest relative to the base level. Thus, if leaving it up to DESeq2 to decide on the contrasts be sure to check that the alphabetical order coincides with the fold change direction you are anticipating.","title":"Does it matter what I choose to be my base level?"},{"location":"07b_hypothesis_testing.html#the-results-table","text":"Now that we have our contrast created, we can use it as input to the results() function. Let\u2019s take a quick look at the help manual for the function: ? results You will see we have the option to provide a wide array of arguments and tweak things from the defaults as needed. As we go through the lesson we will keep coming back to the help documentation to discuss some arguments that are good to know about. ## Extract results for MOV10 overexpression vs control res_tableOE <- results ( dds , contrast = contrast_oe , alpha = 0.05 ) NOTE: For our analysis, in addition to the contrast argument we will also provide a value of 0.05 for the alpha argument. We will describe this in more detail when we talk about gene-level filtering. The results table that is returned to us is a DESeqResults object , which is a simple subclass of DataFrame. In many ways it can be treated like a dataframe (i.e when accessing/subsetting data), however it is important to recognize that there are differences for downstream steps like visualization. # Check what type of object is returned class ( res_tableOE ) Now let\u2019s take a look at what information is stored in the results: # What is stored in results? res_tableOE %>% data.frame () %>% head () We have six columns of information reported for each gene (row). We can use the mcols() function to extract information on what the values stored in each column represent: # Get information on each column in results mcols ( res_tableOE , use.names = T ) baseMean : mean of normalized counts for all samples log2FoldChange : log2 fold change lfcSE : standard error stat : Wald statistic pvalue : Wald test p-value padj : BH adjusted p-values","title":"The results table"},{"location":"07b_hypothesis_testing.html#p-values","text":"The p-value is a probability value used to determine whether there is evidence to reject the null hypothesis. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis . The padj column in the results table represents the p-value adjusted for multiple testing, and is the most important column of the results. Typically, a threshold such as padj \\< 0.05 is a good starting point for identifying significant genes. The default method for multiple test correction in DESeq2 is the FDR. Other methods can be used with the pAdjustMethod argument in the results() function.","title":"P-values"},{"location":"07b_hypothesis_testing.html#gene-level-filtering","text":"Let\u2019s take a closer look at our results table. As we scroll through it, you will notice that for selected genes there are NA values in the pvalue and padj columns . What does this mean? The missing values represent genes that have undergone filtering as part of the DESeq() function. Prior to differential expression analysis it is beneficial to omit genes that have little or no chance of being detected as differentially expressed. This will increase the power to detect differentially expressed genes. DESeq2 does not physically remove any genes from the original counts matrix, and so all genes will be present in your results table. The genes omitted by DESeq2 meet one of the three filtering criteria outlined below : 1. Genes with zero counts in all samples If within a row, all samples have zero counts there is no expression information and therefore these genes are not tested. # Filter genes by zero expression res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% dplyr :: filter ( baseMean == 0 ) %>% head () The baseMean column for these genes will be zero, and the log2 fold change estimates, p-value and adjusted p-value will all be set to NA. 2. Genes with an extreme count outlier The DESeq() function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook\u2019s distance. Cook\u2019s distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook\u2019s distance is intended to indicate an outlier count. Genes which contain a Cook\u2019s distance above a threshold are flagged, however at least 3 replicates are required for flagging, as it is difficult to judge which sample might be an outlier with only 2 replicates. We can turn off this filtering by using the cooksCutoff argument in the results() function. # Filter genes that have an extreme outlier res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% dplyr :: filter ( is.na ( pvalue ) & is.na ( padj ) & baseMean > 0 ) %>% head () If a gene contains a sample with an extreme count outlier then the p-value and adjusted p-value will be set to NA. 3. Genes with a low mean normalized counts DESeq2 defines a low mean threshold, that is empirically determined from your data, in which the fraction of significant genes can be increased by reducing the number of genes that are considered for multiple testing. This is based on the notion that genes with very low counts are not likely to see significant differences typically due to high dispersion. Image courtesy of slideshare presentation from Joachim Jacob, 2014. At a user-specified value ( alpha = 0.05 ), DESeq2 evaluates the change in the number of significant genes as it filters out increasingly bigger portions of genes based on their mean counts, as shown in the figure above. The point at which the number of significant genes reaches a peak is the low mean threshold that is used to filter genes that undergo multiple testing. There is also an argument to turn off the filtering off by setting independentFiltering = F . # Filter genes below the low mean threshold res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% dplyr :: filter ( ! is.na ( pvalue ) & is.na ( padj ) & baseMean > 0 ) %>% head () If a gene is filtered by independent filtering, then only the adjusted p-value will be set to NA. NOTE: DESeq2 will perform the filtering outlined above by default; however other DE tools, such as EdgeR will not. Filtering is a necessary step, even if you are using limma-voom and/or edgeR\u2019s quasi-likelihood methods. Be sure to follow pre-filtering steps when using other tools, as outlined in their user guides found on Bioconductor as they generally perform much better.","title":"Gene-level filtering"},{"location":"07b_hypothesis_testing.html#fold-changes","text":"Another important column in the results table, is the log2FoldChange . With large significant gene lists it can be hard to extract meaningful biological relevance. To help increase stringency, one can also add a fold change threshold . Keep in mind when setting that value that we are working with log2 fold changes, so a cutoff of log2FoldChange \\< 1 would translate to an actual fold change of 2.","title":"Fold changes"},{"location":"07b_hypothesis_testing.html#an-alternative-approach-to-add-the-fold-change-threshold","text":"The results() function has an option to add a fold change threshold using the lfcThrehsold argument. This method is more statistically motivated, and is recommended when you want a more confident set of genes based on a certain fold-change. It actually performs a statistical test against the desired threshold, by performing a two-tailed test for log2 fold changes greater than the absolute value specified. The user can change the alternative hypothesis using altHypothesis and perform two one-tailed tests as well. This is a more conservative approach, so expect to retrieve a much smaller set of genes! The fold changes reported in the results table are calculated by: log2 ( normalized_counts_group1 / normalized_counts_group2 )","title":"An alternative approach to add the fold change threshold:"},{"location":"07b_hypothesis_testing.html#summarizing-results","text":"To summarize the results table, a handy function in DESeq2 is summary() . Confusingly it has the same name as the function used to inspect data frames. This function, when called with a DESeq results table as input, will summarize the results using a default threshold of padj \\< 0.1. However, since we had set the alpha argument to 0.05 when creating our results table threshold: FDR \\< 0.05 (padj/FDR is used even though the output says p-value < 0.05 ). Let\u2019s start with the OE vs control results: ## Summarize results summary ( res_tableOE , alpha = 0.05 ) In addition to the number of genes up- and down-regulated at the default threshold, the function also reports the number of genes that were tested (genes with non-zero total read count), and the number of genes not included in multiple test correction due to a low mean count .","title":"Summarizing results"},{"location":"07b_hypothesis_testing.html#extracting-significant-differentially-expressed-genes","text":"Let\u2019s first create variables that contain our threshold criteria. We will only be using the adjusted p-values in our criteria: ### Set thresholds padj.cutoff <- 0.05 We can easily subset the results table to only include those that are significant using the dplyr::filter() function, but first we will convert the results table into a tibble: # Create a tibble of results and add gene symbols to new object res_tableOE_tb <- res_tableOE %>% as_tibble ( rownames = \"gene\" ) %>% relocate ( gene , .before = baseMean ) Now we can subset that table to only keep the significant genes using our pre-defined thresholds: # Subset the tibble to keep only significant genes sigOE <- res_tableOE_tb %>% dplyr :: filter ( padj < padj.cutoff ) # Take a quick look at this tibble sigOE Now that we have extracted the significant results, we are ready for visualization! Excercise MOV10 Differential Expression Analysis: Control versus Knockdown Now that we have results for the overexpression results, do the same for the Control vs. Knockdown samples . Create a contrast vector called contrast_kd . Use contrast vector in the results() to extract a results table and store that to a variable called res_tableKD . Using a p-adjusted threshold of 0.05 ( padj.cutoff < 0.05 ), subset res_tableKD to report the number of genes that are up- and down-regulated in Mov10_knockdown compared to control. How many genes are differentially expressed in the Knockdown compared to Control? How does this compare to the overexpression significant gene list (in terms of numbers)? This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . Some materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website","title":"Extracting significant differentially expressed genes"},{"location":"07c_DEA_visualization.html","text":"Approximate time: 75 minutes Learning Objectives \u00b6 Explain log fold change shrinkage Setup results data for application of visualization techniques Describe different data visualization useful for exploring results from a DGE analysis Create a volcano plot and MA plot to evaluate relationship among DGE statistics Create a heatmap to illustrate expression changes of differentially expressed genes More accurate LFC estimates \u00b6 In the previous lessons, we learned about how to generate a table with Differentially Expressed genes ## DO NOT RUN res_tableOE <- results ( dds , contrast = contrast_oe , alpha = 0.05 ) head ( res_tableOE ) The problem with these fold change estimates is that they are not entirely accurate as they do not account for the large dispersion we observe with low read counts. To address this, the log2 fold changes need to be adjusted . To generate more accurate log2 fold change (LFC) estimates, DESeq2 allows for the shrinkage of the LFC estimates toward zero when the information for a gene is low, which could include: Low counts High dispersion values LFC shrinkage uses information from all genes to generate more accurate estimates. Specifically, the distribution of LFC estimates for all genes is used (as a prior) to shrink the LFC estimates of genes with little information or high dispersion toward more likely (lower) LFC estimates. Illustration taken from the DESeq2 paper . In the figure above, we have an example using two genes: green gene and purple gene. For each gene the expression values are plotted for each sample in the two different mouse strains (C57BL/6J and DBA/2J). Both genes have the same mean values for the two sample groups, but the green gene has little within-group variation while the purple gene has high levels of variation. For the green gene with low within-group variation, the unshrunken LFC estimate (vertex of the green solid line ) is very similar to the shrunken LFC estimate (vertex of the green dotted line). However, LFC estimates for the purple gene are quite different due to the high dispersion. So even though two genes can have similar normalized count values, they can have differing degrees of LFC shrinkage. Notice the LFC estimates are shrunken toward the prior (black solid line) . Shrinking the log2 fold changes will not change the total number of genes that are identified as significantly differentially expressed. The shrinkage of fold change is to help with downstream assessment of results. For example, if you wanted to subset your significant genes based on fold change for further evaluation, you may want to use shruken values. Additionally, for functional analysis tools such as GSEA which require fold change values as input you would want to provide shrunken values. To generate the shrunken log2 fold change estimates, you have to run an additional step on your results object (that we will create below) with the function lfcShrink() . ## Save the unshrunken results to compare res_tableOE_unshrunken <- res_tableOE # Apply fold change shrinkage res_tableOE <- lfcShrink ( dds , coef = \"sampletype_MOV10_overexpression_vs_control\" , type = \"apeglm\" ) Depending on the version of DESeq2 you are using the default method for shrinkage estimation will differ. The defaults can be changed by adding the argument type in the lfcShrink() function as we have above. For most recent versions of DESeq2, type=\"normal\" is the default and was the only method in earlier versions. It has been shown that in most situations there are alternative methods that have less bias than the \u2019normal` method , and therefore we chose to use apeglm . For more information on shrinkage, the DESeq2 vignette has an Extended section on shrinkage estimators that is quite useful. contrast vs coef \u00b6 When using the shrinkage method, rather than using the contrast argument you will be required to specify coef . Using contrast forms an expanded model matrix, treating all factor levels equally, and averages over all distances between all pairs of factor levels to estimate the prior. Using coef, means looking only at that column of the model matrix (so usually that would be one level against the reference level) and estimates the prior for that coefficient from the distribution of those MLE of coefficients. When using coef, the shrinkage depends on which level is chosen as reference. How do I know what to value to provide to the coef argument? \u00b6 The value you provide here needs to match identically to what is stored in the column header of the coefficients table. To see what values you have to work with you can use resultsNames(dds) . Visualizing the results \u00b6 MA plot \u00b6 A plot that can be useful to exploring our results is the MA plot. The MA plot shows the mean of the normalized counts versus the log2 fold changes for all genes tested . The genes that are significantly DE are colored to be easily identified (adjusted p-value \\< 0.01 by default). This is also a great way to illustrate the effect of LFC shrinkage. The DESeq2 package offers a simple function to generate an MA plot. Let\u2019s start with the unshrunken results: # MA plot using unshrunken fold changes plotMA ( res_tableOE_unshrunken , ylim = c ( -2 , 2 )) And now the shrunken results: # MA plot using shrunken fold changes plotMA ( res_tableOE , ylim = c ( -2 , 2 )) On the left you have the unshrunken fold change values plotted and you can see the abundance of scatter for the lowly expressed genes. That is, many of these genes exhibit very high fold changes. After shrinkage, we see the fold changes are much smaller estimates. In addition to the comparison described above, this plot allows us to evaluate the magnitude of fold changes and how they are distributed relative to mean expression. Generally, we would expect to see significant genes across the full range of expression levels. Advanced visualizations \u00b6 When we are working with large amounts of data it can be useful to display that information graphically. During this lesson, we will get you started with some basic and more advanced plots commonly used when exploring differential gene expression data, however, many of these plots can be helpful in visualizing other types of data as well. We will be working with three different data objects we have already created in earlier lessons: Metadata for our samples (a dataframe): meta Normalized expression data for every gene in each of our samples (a matrix): normalized_counts Tibble versions of the DESeq2 results we generated in the last lesson: res_tableOE_tb and res_tableKD_tb First, we already have a metadata tibble. meta %>% head () Next, let\u2019s bring in the normalized_counts object with our gene names. # DESeq2 creates a matrix when you use the counts() function ## First convert normalized_counts to a data frame and transfer the row names to a new column called \"gene\" normalized_counts <- counts ( dds , normalized = T ) %>% data.frame () %>% rownames_to_column ( var = \"gene\" ) Plotting significant DE genes \u00b6 One way to visualize results would be to simply plot the expression data for a handful of genes. We could do that by picking out specific genes of interest or selecting a range of genes. Using DESeq2 plotCounts() to plot expression of a single gene To pick out a specific gene of interest to plot, for example MOV10, we can use the plotCounts() from DESeq2. plotCounts() requires that the gene specified matches the original input to DESeq2. # Plot expression for single gene plotCounts ( dds , gene = \"MOV10\" , intgroup = \"sampletype\" ) This DESeq2 function only allows for plotting the counts of a single gene at a time, and is not flexible regarding the appearance. Using ggplot2 to plot expression of a single gene If you wish to change the appearance of this plot, we can save the output of plotCounts() to a variable specifying the returnData=TRUE argument, then use ggplot() : # Save plotcounts to a data frame object d <- plotCounts ( dds , gene = \"MOV10\" , intgroup = \"sampletype\" , returnData = TRUE ) # What is the data output of plotCounts()? d %>% head () # Plot the MOV10 normalized counts, using the samplenames (rownames(d) as labels) ggplot ( d , aes ( x = sampletype , y = count , color = sampletype )) + geom_point ( position = position_jitter ( w = 0.1 , h = 0 )) + geom_text_repel ( aes ( label = rownames ( d ))) + theme_bw () + ggtitle ( \"MOV10\" ) + theme ( plot.title = element_text ( hjust = 0.5 )) Note that in the plot below (code above), we are using geom_text_repel() from the ggrepel package to label our individual points on the plot. Heatmap \u00b6 In addition to plotting subsets, we could also extract the normalized values of all the significant genes and plot a heatmap of their expression using pheatmap() . ### Extract normalized expression for significant genes from the OE and control samples (2:4 and 7:9) norm_OEsig <- normalized_counts [, c ( 1 : 4 , 7 : 9 )] %>% dplyr : filter ( gene %in% sigOE $ gene ) Now let\u2019s draw the heatmap using pheatmap : ### Run pheatmap using the metadata data frame for the annotation pheatmap ( norm_OEsig [ 2 : 7 ], cluster_rows = T , show_rownames = F , annotation = meta %>% column_to_rownames ( var = \"samplename\" ), border_color = NA , fontsize = 10 , scale = \"row\" , fontsize_row = 10 , height = 20 ) NOTE: There are several additional arguments we have included in the function for aesthetics. One important one is scale=\"row\" , in which Z-scores are plotted, rather than the actual normalized count value. Z-scores are computed on a gene-by-gene basis by subtracting the mean and then dividing by the standard deviation. The Z-scores are computed after the clustering , so that it only affects the graphical aesthetics and the color visualization is improved. Volcano plot \u00b6 The above plot would be great to look at the expression levels of a good number of genes, but for more of a global view there are other plots we can draw. A commonly used one is a volcano plot; in which you have the log transformed adjusted p-values plotted on the y-axis and log2 fold change values on the x-axis. To generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here. ## Obtain logical vector where TRUE values denote padj values < 0.05 and fold change > 1.5 in either direction res_tableOE_tb <- res_tableOE_tb %>% mutate ( threshold_OE = padj < 0.05 & abs ( log2FoldChange ) >= 0.58 ) Now we can start plotting. The geom_point object is most applicable, as this is essentially a scatter plot: ## Volcano plot ggplot ( res_tableOE_tb ) + geom_point ( aes ( x = log2FoldChange , y = - log10 ( padj ), colour = threshold_OE )) + ggtitle ( \"Mov10 overexpression\" ) + xlab ( \"log2 fold change\" ) + ylab ( \"-log10 adjusted p-value\" ) + #scale_y_continuous(limits = c(0,50)) + theme ( legend.position = \"none\" , plot.title = element_text ( size = rel ( 1.5 ), hjust = 0.5 ), axis.title = element_text ( size = rel ( 1.25 ))) This is a great way to get an overall picture of what is going on, but what if we also wanted to know where the top 10 genes (lowest padj) in our DE list are located on this plot? We could label those dots with the gene name on the Volcano plot using geom_text_repel() . First, we need to order the res_tableOE tibble by padj , and add an additional column to it, to include on those gene names we want to use to label the plot. ## Create an empty column to indicate which genes to label res_tableOE_tb <- res_tableOE_tb %>% mutate ( genelabels = \"\" ) ## Sort by padj values res_tableOE_tb <- res_tableOE_tb %>% arrange ( padj ) ## Populate the genelabels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes res_tableOE_tb $ genelabels [ 1 : 10 ] <- as.character ( res_tableOE_tb $ gene [ 1 : 10 ]) View ( res_tableOE_tb ) Next, we plot it as before with an additional layer for geom_text_repel() wherein we can specify the column of gene labels we just created. ggplot ( res_tableOE_tb , aes ( x = log2FoldChange , y = - log10 ( padj ))) + geom_point ( aes ( colour = threshold_OE )) + geom_text_repel ( aes ( label = genelabels )) + ggtitle ( \"Mov10 overexpression\" ) + xlab ( \"log2 fold change\" ) + ylab ( \"-log10 adjusted p-value\" ) + theme ( legend.position = \"none\" , plot.title = element_text ( size = rel ( 1.5 ), hjust = 0.5 ), axis.title = element_text ( size = rel ( 1.25 ))) NOTE: If using the DESeq2 tool for differential expression analysis, the package \u2018DEGreport\u2019 can use the DESeq2 results output to make the top20 genes and the volcano plots generated above by writing a few lines of simple code. While you can customize the plots above, you may be interested in using the easier code. Below are examples of the code to create these plots: DEGreport :: degPlot ( dds = dds , res = res , n = 20 , xs = \"type\" , group = \"condition\" ) # dds object is output from DESeq2 DEGreport :: degVolcano ( data.frame ( res [, c ( \"log2fold change\" , \"padj\" )]), # table - 2 columns plot_text = data.frame ( res [ 1 : 10 , c ( \"log2fold change\" , \"padj\" , \"id\" )])) # table to add names # Available in the newer version for R 3.4 DEGreport :: degPlotWide ( dds = dds , genes = row.names ( res )[ 1 : 5 ], group = \"condition\" ) This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . Materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website","title":"Visualization"},{"location":"07c_DEA_visualization.html#learning-objectives","text":"Explain log fold change shrinkage Setup results data for application of visualization techniques Describe different data visualization useful for exploring results from a DGE analysis Create a volcano plot and MA plot to evaluate relationship among DGE statistics Create a heatmap to illustrate expression changes of differentially expressed genes","title":"Learning Objectives"},{"location":"07c_DEA_visualization.html#more-accurate-lfc-estimates","text":"In the previous lessons, we learned about how to generate a table with Differentially Expressed genes ## DO NOT RUN res_tableOE <- results ( dds , contrast = contrast_oe , alpha = 0.05 ) head ( res_tableOE ) The problem with these fold change estimates is that they are not entirely accurate as they do not account for the large dispersion we observe with low read counts. To address this, the log2 fold changes need to be adjusted . To generate more accurate log2 fold change (LFC) estimates, DESeq2 allows for the shrinkage of the LFC estimates toward zero when the information for a gene is low, which could include: Low counts High dispersion values LFC shrinkage uses information from all genes to generate more accurate estimates. Specifically, the distribution of LFC estimates for all genes is used (as a prior) to shrink the LFC estimates of genes with little information or high dispersion toward more likely (lower) LFC estimates. Illustration taken from the DESeq2 paper . In the figure above, we have an example using two genes: green gene and purple gene. For each gene the expression values are plotted for each sample in the two different mouse strains (C57BL/6J and DBA/2J). Both genes have the same mean values for the two sample groups, but the green gene has little within-group variation while the purple gene has high levels of variation. For the green gene with low within-group variation, the unshrunken LFC estimate (vertex of the green solid line ) is very similar to the shrunken LFC estimate (vertex of the green dotted line). However, LFC estimates for the purple gene are quite different due to the high dispersion. So even though two genes can have similar normalized count values, they can have differing degrees of LFC shrinkage. Notice the LFC estimates are shrunken toward the prior (black solid line) . Shrinking the log2 fold changes will not change the total number of genes that are identified as significantly differentially expressed. The shrinkage of fold change is to help with downstream assessment of results. For example, if you wanted to subset your significant genes based on fold change for further evaluation, you may want to use shruken values. Additionally, for functional analysis tools such as GSEA which require fold change values as input you would want to provide shrunken values. To generate the shrunken log2 fold change estimates, you have to run an additional step on your results object (that we will create below) with the function lfcShrink() . ## Save the unshrunken results to compare res_tableOE_unshrunken <- res_tableOE # Apply fold change shrinkage res_tableOE <- lfcShrink ( dds , coef = \"sampletype_MOV10_overexpression_vs_control\" , type = \"apeglm\" ) Depending on the version of DESeq2 you are using the default method for shrinkage estimation will differ. The defaults can be changed by adding the argument type in the lfcShrink() function as we have above. For most recent versions of DESeq2, type=\"normal\" is the default and was the only method in earlier versions. It has been shown that in most situations there are alternative methods that have less bias than the \u2019normal` method , and therefore we chose to use apeglm . For more information on shrinkage, the DESeq2 vignette has an Extended section on shrinkage estimators that is quite useful.","title":"More accurate LFC estimates"},{"location":"07c_DEA_visualization.html#contrast-vs-coef","text":"When using the shrinkage method, rather than using the contrast argument you will be required to specify coef . Using contrast forms an expanded model matrix, treating all factor levels equally, and averages over all distances between all pairs of factor levels to estimate the prior. Using coef, means looking only at that column of the model matrix (so usually that would be one level against the reference level) and estimates the prior for that coefficient from the distribution of those MLE of coefficients. When using coef, the shrinkage depends on which level is chosen as reference.","title":"contrast vs coef"},{"location":"07c_DEA_visualization.html#how-do-i-know-what-to-value-to-provide-to-the-coef-argument","text":"The value you provide here needs to match identically to what is stored in the column header of the coefficients table. To see what values you have to work with you can use resultsNames(dds) .","title":"How do I know what to value to provide to the coef argument?"},{"location":"07c_DEA_visualization.html#visualizing-the-results","text":"","title":"Visualizing the results"},{"location":"07c_DEA_visualization.html#ma-plot","text":"A plot that can be useful to exploring our results is the MA plot. The MA plot shows the mean of the normalized counts versus the log2 fold changes for all genes tested . The genes that are significantly DE are colored to be easily identified (adjusted p-value \\< 0.01 by default). This is also a great way to illustrate the effect of LFC shrinkage. The DESeq2 package offers a simple function to generate an MA plot. Let\u2019s start with the unshrunken results: # MA plot using unshrunken fold changes plotMA ( res_tableOE_unshrunken , ylim = c ( -2 , 2 )) And now the shrunken results: # MA plot using shrunken fold changes plotMA ( res_tableOE , ylim = c ( -2 , 2 )) On the left you have the unshrunken fold change values plotted and you can see the abundance of scatter for the lowly expressed genes. That is, many of these genes exhibit very high fold changes. After shrinkage, we see the fold changes are much smaller estimates. In addition to the comparison described above, this plot allows us to evaluate the magnitude of fold changes and how they are distributed relative to mean expression. Generally, we would expect to see significant genes across the full range of expression levels.","title":"MA plot"},{"location":"07c_DEA_visualization.html#advanced-visualizations","text":"When we are working with large amounts of data it can be useful to display that information graphically. During this lesson, we will get you started with some basic and more advanced plots commonly used when exploring differential gene expression data, however, many of these plots can be helpful in visualizing other types of data as well. We will be working with three different data objects we have already created in earlier lessons: Metadata for our samples (a dataframe): meta Normalized expression data for every gene in each of our samples (a matrix): normalized_counts Tibble versions of the DESeq2 results we generated in the last lesson: res_tableOE_tb and res_tableKD_tb First, we already have a metadata tibble. meta %>% head () Next, let\u2019s bring in the normalized_counts object with our gene names. # DESeq2 creates a matrix when you use the counts() function ## First convert normalized_counts to a data frame and transfer the row names to a new column called \"gene\" normalized_counts <- counts ( dds , normalized = T ) %>% data.frame () %>% rownames_to_column ( var = \"gene\" )","title":"Advanced visualizations"},{"location":"07c_DEA_visualization.html#plotting-significant-de-genes","text":"One way to visualize results would be to simply plot the expression data for a handful of genes. We could do that by picking out specific genes of interest or selecting a range of genes. Using DESeq2 plotCounts() to plot expression of a single gene To pick out a specific gene of interest to plot, for example MOV10, we can use the plotCounts() from DESeq2. plotCounts() requires that the gene specified matches the original input to DESeq2. # Plot expression for single gene plotCounts ( dds , gene = \"MOV10\" , intgroup = \"sampletype\" ) This DESeq2 function only allows for plotting the counts of a single gene at a time, and is not flexible regarding the appearance. Using ggplot2 to plot expression of a single gene If you wish to change the appearance of this plot, we can save the output of plotCounts() to a variable specifying the returnData=TRUE argument, then use ggplot() : # Save plotcounts to a data frame object d <- plotCounts ( dds , gene = \"MOV10\" , intgroup = \"sampletype\" , returnData = TRUE ) # What is the data output of plotCounts()? d %>% head () # Plot the MOV10 normalized counts, using the samplenames (rownames(d) as labels) ggplot ( d , aes ( x = sampletype , y = count , color = sampletype )) + geom_point ( position = position_jitter ( w = 0.1 , h = 0 )) + geom_text_repel ( aes ( label = rownames ( d ))) + theme_bw () + ggtitle ( \"MOV10\" ) + theme ( plot.title = element_text ( hjust = 0.5 )) Note that in the plot below (code above), we are using geom_text_repel() from the ggrepel package to label our individual points on the plot.","title":"Plotting significant DE genes"},{"location":"07c_DEA_visualization.html#heatmap","text":"In addition to plotting subsets, we could also extract the normalized values of all the significant genes and plot a heatmap of their expression using pheatmap() . ### Extract normalized expression for significant genes from the OE and control samples (2:4 and 7:9) norm_OEsig <- normalized_counts [, c ( 1 : 4 , 7 : 9 )] %>% dplyr : filter ( gene %in% sigOE $ gene ) Now let\u2019s draw the heatmap using pheatmap : ### Run pheatmap using the metadata data frame for the annotation pheatmap ( norm_OEsig [ 2 : 7 ], cluster_rows = T , show_rownames = F , annotation = meta %>% column_to_rownames ( var = \"samplename\" ), border_color = NA , fontsize = 10 , scale = \"row\" , fontsize_row = 10 , height = 20 ) NOTE: There are several additional arguments we have included in the function for aesthetics. One important one is scale=\"row\" , in which Z-scores are plotted, rather than the actual normalized count value. Z-scores are computed on a gene-by-gene basis by subtracting the mean and then dividing by the standard deviation. The Z-scores are computed after the clustering , so that it only affects the graphical aesthetics and the color visualization is improved.","title":"Heatmap"},{"location":"07c_DEA_visualization.html#volcano-plot","text":"The above plot would be great to look at the expression levels of a good number of genes, but for more of a global view there are other plots we can draw. A commonly used one is a volcano plot; in which you have the log transformed adjusted p-values plotted on the y-axis and log2 fold change values on the x-axis. To generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here. ## Obtain logical vector where TRUE values denote padj values < 0.05 and fold change > 1.5 in either direction res_tableOE_tb <- res_tableOE_tb %>% mutate ( threshold_OE = padj < 0.05 & abs ( log2FoldChange ) >= 0.58 ) Now we can start plotting. The geom_point object is most applicable, as this is essentially a scatter plot: ## Volcano plot ggplot ( res_tableOE_tb ) + geom_point ( aes ( x = log2FoldChange , y = - log10 ( padj ), colour = threshold_OE )) + ggtitle ( \"Mov10 overexpression\" ) + xlab ( \"log2 fold change\" ) + ylab ( \"-log10 adjusted p-value\" ) + #scale_y_continuous(limits = c(0,50)) + theme ( legend.position = \"none\" , plot.title = element_text ( size = rel ( 1.5 ), hjust = 0.5 ), axis.title = element_text ( size = rel ( 1.25 ))) This is a great way to get an overall picture of what is going on, but what if we also wanted to know where the top 10 genes (lowest padj) in our DE list are located on this plot? We could label those dots with the gene name on the Volcano plot using geom_text_repel() . First, we need to order the res_tableOE tibble by padj , and add an additional column to it, to include on those gene names we want to use to label the plot. ## Create an empty column to indicate which genes to label res_tableOE_tb <- res_tableOE_tb %>% mutate ( genelabels = \"\" ) ## Sort by padj values res_tableOE_tb <- res_tableOE_tb %>% arrange ( padj ) ## Populate the genelabels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes res_tableOE_tb $ genelabels [ 1 : 10 ] <- as.character ( res_tableOE_tb $ gene [ 1 : 10 ]) View ( res_tableOE_tb ) Next, we plot it as before with an additional layer for geom_text_repel() wherein we can specify the column of gene labels we just created. ggplot ( res_tableOE_tb , aes ( x = log2FoldChange , y = - log10 ( padj ))) + geom_point ( aes ( colour = threshold_OE )) + geom_text_repel ( aes ( label = genelabels )) + ggtitle ( \"Mov10 overexpression\" ) + xlab ( \"log2 fold change\" ) + ylab ( \"-log10 adjusted p-value\" ) + theme ( legend.position = \"none\" , plot.title = element_text ( size = rel ( 1.5 ), hjust = 0.5 ), axis.title = element_text ( size = rel ( 1.25 ))) NOTE: If using the DESeq2 tool for differential expression analysis, the package \u2018DEGreport\u2019 can use the DESeq2 results output to make the top20 genes and the volcano plots generated above by writing a few lines of simple code. While you can customize the plots above, you may be interested in using the easier code. Below are examples of the code to create these plots: DEGreport :: degPlot ( dds = dds , res = res , n = 20 , xs = \"type\" , group = \"condition\" ) # dds object is output from DESeq2 DEGreport :: degVolcano ( data.frame ( res [, c ( \"log2fold change\" , \"padj\" )]), # table - 2 columns plot_text = data.frame ( res [ 1 : 10 , c ( \"log2fold change\" , \"padj\" , \"id\" )])) # table to add names # Available in the newer version for R 3.4 DEGreport :: degPlotWide ( dds = dds , genes = row.names ( res )[ 1 : 5 ], group = \"condition\" ) This lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) . Materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website","title":"Volcano plot"},{"location":"08a_FA_genomic_annotation.html","text":"Approximate time: 30 minutes Learning Objectives \u00b6 Discuss the available genomic annotation databases and the different types if information stored Compare and contrast the tools available for accessing genomic annotation databases Apply various R packages for retrieval of genomic annotations Genomic annotations \u00b6 The analysis of next-generation sequencing results requires associating genes, transcripts, proteins, etc. with functional or regulatory information. To perform functional analysis on gene lists, we often need to obtain gene identifiers that are compatible with the tools we wish to use and this is not always trivial. Here, we discuss ways in which you can obtain gene annotation information and some of the advantages and disadvantages of each method . Databases \u00b6 We retrieve information on the processes, pathways, etc. (for which a gene is involved in) from the necessary database where the information is stored. The database you choose will be dependent on what type of information you are trying to obtain. Examples of databases that are often queried, include: General databases Offer comprehensive information on genome features, feature coordinates, homology, variant information, phenotypes, protein domain/family information, associated biological processes/pathways, associated microRNAs, etc.: Ensembl (use Ensembl gene IDs) NCBI (use Entrez gene IDs) UCSC EMBL-EBI Annotation-specific databases Provide annotations related to a specific topic: Gene Ontology (GO): database of gene ontology biological processes, cellular components and molecular functions - based on Ensembl or Entrez gene IDs or official gene symbols KEGG: database of biological pathways - based on Entrez gene IDs MSigDB: database of gene sets Reactome: database of biological pathways Human Phenotype Ontology: database of genes associated with human disease CORUM: database of protein complexes for human, mouse, rat \u2026 This is by no means an exhaustive list, there are many other databases available that are not listed here. Genome builds \u00b6 Before you begin your search through any of these databases, you should know which build of the genome was used to generate your gene list and make sure you use the same build for the annotations during functional analysis. When a new genome build is acquired, the names and/or coordinate location of genomic features (gene, transcript, exon, etc.) may change. Therefore, the annotations regarding genome features (gene, transcript, exon, etc.) is genome-build specific and we need to make sure that our annotations are obtained from the appropriate resource. For example, if we used the GRCh38 build of the human genome to quantify gene expression used for differential expression analysis, then we should use the same GRCh38 build of the genome to convert between gene IDs and to identify annotations for each of the genes. Tools for accessing databases \u00b6 Within R, there are many popular packages used for gene/transcript-level annotation. These packages provide tools that take the list of genes you provide and retrieve information for each gene using one or more of the databases listed above. Annotation tools: for accessing/querying annotations from a specific databases \u00b6 Tool Description Pros Cons org.Xx.eg.db Query gene feature information for the organism of interest gene ID conversion, biotype and coordinate information only latest genome build available EnsDb.Xx.vxx Transcript and gene-level information directly fetched from Ensembl API (similar to TxDb, but with filtering ability and versioned by Ensembl release) easy functions to extract features, direct filtering Not the most up-to-date annotations, more difficult to use than some packages TxDb.Xx.UCSC.hgxx.knownGene UCSC database for transcript and gene-level information or can create own TxDb from an SQLite database file using the GenomicFeatures package feature information, easy functions to extract features only available current and recent genome builds - can create your own, less up-to-date with annotations than Ensembl annotables Gene-level feature information immediately available for the human and model organisms super quick and easy gene ID conversion, biotype and coordinate information static resource, not updated regularly biomaRt An R package version of the Ensembl BioMart online tool all Ensembl database information available, all organisms on Ensembl, wealth of information Interface tools: for accessing/querying annotations from multiple different annotation sources \u00b6 AnnotationDbi: queries the OrgDb , TxDb , Go.db , EnsDb , and BioMart annotations. AnnotationHub: queries large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc. NOTE: These are both packages that can be used to create the tx2gene files we had you download at the beginning of this workshop. AnnotationDbi \u00b6 AnnotationDbi is an R package that provides an interface for connecting and querying various annotation databases using SQLite data storage. The AnnotationDbi packages can query the OrgDb , TxDb , EnsDb , Go.db , and BioMart annotations. There is helpful documentation available to reference when extracting data from any of these databases. AnnotationHub \u00b6 AnnotationHub is a wonderful resource for accessing genomic data or querying large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc. All of this information is stored and easily accessible by directly connecting to the database. To get started with AnnotationHub, we first load the library and connect to the database: # Load libraries library ( AnnotationHub ) library ( ensembldb ) # Connect to AnnotationHub ah <- AnnotationHub () What is a cache? \u00b6 A cache is used in R to store data or a copy of the data so that future requests can be served faster without having to re-run a lengthy computation. The AnnotationHub() command creates a client that manages a local cache of the database, helping with quick and reproducible access. When encountering question AnnotationHub does not exist, create directory? , you can anwser either yes (create a permanent location to store cache) or no (create a temporary location to store cache). hubCache(ah) gets the file system location of the local AnnotationHub cache. hubUrl(ah) gets the URL for the online hub. To see the types of information stored inside our database, we can just type the name of the object: # Explore the AnnotationHub object ah Using the output, you can get an idea of the information that you can query within the AnnotationHub object: AnnotationHub with 47240 records # snapshotDate(): 2019-10-29 # $dataprovider: BroadInstitute, Ensembl, UCSC, ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/, H... # $species: Homo sapiens, Mus musculus, Drosophila melanogaster, Bos taurus, Pan troglod... # $rdataclass: GRanges, BigWigFile, TwoBitFile, Rle, OrgDb, EnsDb, ChainFile, TxDb, Inpa... # additional mcols(): taxonomyid, genome, description, coordinate_1_based, # maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl, # sourcetype # retrieve records with, e.g., 'object[[\"AH5012\"]]' title AH5012 | Chromosome Band AH5013 | STS Markers AH5014 | FISH Clones AH5015 | Recomb Rate AH5016 | ENCODE Pilot ... ... AH78364 | Xiphophorus_maculatus.X_maculatus-5.0-male.ncrna.2bit AH78365 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.cdna.all.2bit AH78366 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_rm.toplevel.2bit AH78367 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_sm.toplevel.2bit AH78368 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.ncrna.2bit Notice the note on retrieving records with object[[AH2]] - this will be how we can extract a single record from the AnnotationHub object. If you would like to see more information about any of the classes of data you can extract that information as well. For example, if you wanted to determine all species information available , you could explore that within the AnnotationHub object: # Explore all species information available unique ( ah $ species ) %>% View () In addition to species information, there is also additional information about the type of Data Objects and the Data Providers: # Explore the types of Data Objects available unique ( ah $ rdataclass ) %>% View () # Explore the Data Providers unique ( ah $ dataprovider ) %>% View () Now that we know the types of information available from AnnotationHub we can query it for the information we want using the query() function. Let\u2019s say we would like to return the Ensembl EnsDb information for Human . To return the records available, we need to use the terms as they are output from the ah object to extract the desired data. # Query AnnotationHub human_ens <- query ( ah , c ( \"Homo sapiens\" , \"EnsDb\" )) The query retrieves all hits for the EnsDb objects , and you will see that they are listed by the release number. The most current release for GRCh38 is Ensembl98 and AnnotationHub offers that as an option to use. However, if you look at options for older releases, for Homo sapiens it only go back as far as Ensembl 87. This is fine if you are using GRCh38, however if you were using an older genome build like hg19/GRCh37, you would need to load the EnsDb package if available for that release or you might need to build your own with ensembldb . human_ens AnnotationHub with 13 records # snapshotDate(): 2019-10-29 # $dataprovider: Ensembl # $species: Homo sapiens # $rdataclass: EnsDb # additional mcols(): taxonomyid, genome, description, coordinate_1_based, # maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl, # sourcetype # retrieve records with, e.g., 'object[[\"AH53211\"]]' title AH53211 | Ensembl 87 EnsDb for Homo Sapiens AH53715 | Ensembl 88 EnsDb for Homo Sapiens AH56681 | Ensembl 89 EnsDb for Homo Sapiens AH57757 | Ensembl 90 EnsDb for Homo Sapiens AH60773 | Ensembl 91 EnsDb for Homo Sapiens ... ... AH67950 | Ensembl 95 EnsDb for Homo sapiens AH69187 | Ensembl 96 EnsDb for Homo sapiens AH73881 | Ensembl 97 EnsDb for Homo sapiens AH73986 | Ensembl 79 EnsDb for Homo sapiens AH75011 | Ensembl 98 EnsDb for Homo sapiens In our case, we are looking for the latest Ensembl release so that the annotations are the most up-to-date. To extract this information from AnnotationHub, we can use the AnnotationHub ID to subset the object : # Extract annotations of interest human_ens <- human_ens [[ \"AH75011\" ]] Now we can use ensembldb functions to extract the information at the gene, transcript, or exon levels. We are interested in the gene-level annotations, so we can extract that information as follows: # Extract gene-level information genes ( human_ens , return.type = \"data.frame\" ) %>% View () But note that it is just as easy to get the transcript- or exon-level information: # Extract transcript-level information transcripts ( human_ens , return.type = \"data.frame\" ) %>% View () # Extract exon-level information exons ( human_ens , return.type = \"data.frame\" ) %>% View () To obtain an annotation data frame using AnnotationHub, we\u2019ll use the genes() function, but only keep selected columns and filter out rows to keep those corresponding to our gene identifiers in our results file: # Create a gene-level dataframe annotations_ahb <- genes ( human_ens , return.type = \"data.frame\" ) %>% dplyr :: select ( gene_id , gene_name , entrezid , gene_biotype ) %>% dplyr :: filter ( gene_name %in% res_tableOE_tb $ gene ) This dataframe looks like it should be fine as it is, but we look a little closer we will notice that the column containing Entrez identifiers is a list, and in fact there are many Ensembl identifiers that map to more than one Entrez identifier! # Wait a second, we don't have one-to-one mappings! class ( annotations_ahb $ entrezid ) which ( map ( annotations_ahb $ entrezid , length ) > 1 ) So what do we do here? And why do we have this problem? An answer from the Ensembl Help Desk is that this occurs when we cannot choose a perfect match; ie when we have two good matches, but one does not appear to match with a better percentage than the other. In that case, we assign both matches. What we will do is choose to keep the first identifier for these multiple mapping cases . annotations_ahb $ entrezid <- map ( annotations_ahb $ entrezid , 1 ) %>% unlist () NOTE: Not all databases handle multiple mappings in the same way. For example, if we used the OrgDb instead of the EnsDb: human_orgdb <- query(ah, c(\"Homo sapiens\", \"OrgDb\")) human_orgdb <- human_ens[[\"AH75742\"]] annotations_orgdb <- select(human_orgdb, res_tableOE_tb$gene, c(\"SYMBOL\", \"GENENAME\", \"ENTREZID\"), \"ENSEMBL\") We would find that multiple mapping entries would be automatically reduced to one-to-one. We would also find that more than half of the input genes do not return any annotations. This is because the OrgDb family of database are primarily based on mapping using Entrez Gene identifiers. Since our data is based on Ensembl mappings, using the OrgDb would result in a loss of information. Let\u2019s take a look and see how many of our Ensembl identifiers have an associated gene symbol, and how many of them are unique: which ( is.na ( annotations_ahb $ gene_name )) %>% length () which ( duplicated ( annotations_ahb $ gene_name )) %>% length () Let\u2019s identify the non-duplicated genes and only keep the ones that are not duplicated: # Determine the indices for the non-duplicated genes non_duplicates_idx <- which ( duplicated ( annotations_ahb $ gene_name ) == FALSE ) # How many rows does annotations_ahb have? annotations_ahb %>% nrow () # Return only the non-duplicated genes using indices annotations_ahb <- annotations_ahb [ non_duplicates_idx , ] # How many rows are we left with after removing? annotations_ahb %>% nrow () Finally, it would be good to know what proportion of the Ensembl identifiers map to an Entrez identifier : # Determine how many of the Entrez column entries are NA which ( is.na ( annotations_ahb $ entrezid )) %>% length () That\u2019s more than half of our genes! If we plan on using Entrez ID results for downstream analysis, we should definitely keep this in mind. If you look at some of the Ensembl IDs from our query that returned NA, these map to pseudogenes (i.e ENSG00000265439 ) or non-coding RNAs (i.e. ENSG00000265425 ). The discrepancy (which we can expect to observe) between databases is due to the fact that each implements its own different computational approaches for generating the gene builds. Using AnnotationHub to create our tx2gene file \u00b6 To create our tx2gene file, we would need to use a combination of the methods above and merge two dataframes together. For example: ## DO NOT RUN THIS CODE # Create a transcript dataframe txdb <- transcripts ( human_ens , return.type = \"data.frame\" ) %>% dplyr :: select ( tx_id , gene_id ) txdb <- txdb [ grep ( \"ENST\" , txdb $ tx_id ),] # Create a gene-level dataframe genedb <- genes ( human_ens , return.type = \"data.frame\" ) %>% dplyr :: select ( gene_id , gene_name ) # Merge the two dataframes together annotations <- inner_join ( txdb , genedb ) In this lesson our focus has been using annotation packages to extract information mainly just for gene ID conversion for the different tools that we use downstream. Many of the annotation packages we have presented have much more information than what we need for functional analysis and we have only just scratched the surface here. It\u2019s good to know the capabilities of the tools we use, so we encourage you to spend some time exploring these packages to become more familiar with them. NOTE: The annotables package is a super easy annotation package to use. It is not updated frequently, so it\u2019s not great for getting the most up-to-date information for the current builds and does not have information for other organisms than human and mouse, but is a quick way to get annotation information. # Install package BiocManager :: install ( \"annotables\" ) # Load library library ( annotables ) # Access previous build of annotations grch38 This lesson was originally developed by members of the teaching team (Mary Piper) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Gene annotation"},{"location":"08a_FA_genomic_annotation.html#learning-objectives","text":"Discuss the available genomic annotation databases and the different types if information stored Compare and contrast the tools available for accessing genomic annotation databases Apply various R packages for retrieval of genomic annotations","title":"Learning Objectives"},{"location":"08a_FA_genomic_annotation.html#genomic-annotations","text":"The analysis of next-generation sequencing results requires associating genes, transcripts, proteins, etc. with functional or regulatory information. To perform functional analysis on gene lists, we often need to obtain gene identifiers that are compatible with the tools we wish to use and this is not always trivial. Here, we discuss ways in which you can obtain gene annotation information and some of the advantages and disadvantages of each method .","title":"Genomic annotations"},{"location":"08a_FA_genomic_annotation.html#databases","text":"We retrieve information on the processes, pathways, etc. (for which a gene is involved in) from the necessary database where the information is stored. The database you choose will be dependent on what type of information you are trying to obtain. Examples of databases that are often queried, include: General databases Offer comprehensive information on genome features, feature coordinates, homology, variant information, phenotypes, protein domain/family information, associated biological processes/pathways, associated microRNAs, etc.: Ensembl (use Ensembl gene IDs) NCBI (use Entrez gene IDs) UCSC EMBL-EBI Annotation-specific databases Provide annotations related to a specific topic: Gene Ontology (GO): database of gene ontology biological processes, cellular components and molecular functions - based on Ensembl or Entrez gene IDs or official gene symbols KEGG: database of biological pathways - based on Entrez gene IDs MSigDB: database of gene sets Reactome: database of biological pathways Human Phenotype Ontology: database of genes associated with human disease CORUM: database of protein complexes for human, mouse, rat \u2026 This is by no means an exhaustive list, there are many other databases available that are not listed here.","title":"Databases"},{"location":"08a_FA_genomic_annotation.html#genome-builds","text":"Before you begin your search through any of these databases, you should know which build of the genome was used to generate your gene list and make sure you use the same build for the annotations during functional analysis. When a new genome build is acquired, the names and/or coordinate location of genomic features (gene, transcript, exon, etc.) may change. Therefore, the annotations regarding genome features (gene, transcript, exon, etc.) is genome-build specific and we need to make sure that our annotations are obtained from the appropriate resource. For example, if we used the GRCh38 build of the human genome to quantify gene expression used for differential expression analysis, then we should use the same GRCh38 build of the genome to convert between gene IDs and to identify annotations for each of the genes.","title":"Genome builds"},{"location":"08a_FA_genomic_annotation.html#tools-for-accessing-databases","text":"Within R, there are many popular packages used for gene/transcript-level annotation. These packages provide tools that take the list of genes you provide and retrieve information for each gene using one or more of the databases listed above.","title":"Tools for accessing databases"},{"location":"08a_FA_genomic_annotation.html#annotation-tools-for-accessingquerying-annotations-from-a-specific-databases","text":"Tool Description Pros Cons org.Xx.eg.db Query gene feature information for the organism of interest gene ID conversion, biotype and coordinate information only latest genome build available EnsDb.Xx.vxx Transcript and gene-level information directly fetched from Ensembl API (similar to TxDb, but with filtering ability and versioned by Ensembl release) easy functions to extract features, direct filtering Not the most up-to-date annotations, more difficult to use than some packages TxDb.Xx.UCSC.hgxx.knownGene UCSC database for transcript and gene-level information or can create own TxDb from an SQLite database file using the GenomicFeatures package feature information, easy functions to extract features only available current and recent genome builds - can create your own, less up-to-date with annotations than Ensembl annotables Gene-level feature information immediately available for the human and model organisms super quick and easy gene ID conversion, biotype and coordinate information static resource, not updated regularly biomaRt An R package version of the Ensembl BioMart online tool all Ensembl database information available, all organisms on Ensembl, wealth of information","title":"Annotation tools: for accessing/querying annotations from a specific databases"},{"location":"08a_FA_genomic_annotation.html#interface-tools-for-accessingquerying-annotations-from-multiple-different-annotation-sources","text":"AnnotationDbi: queries the OrgDb , TxDb , Go.db , EnsDb , and BioMart annotations. AnnotationHub: queries large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc. NOTE: These are both packages that can be used to create the tx2gene files we had you download at the beginning of this workshop.","title":"Interface tools: for accessing/querying annotations from multiple different annotation sources"},{"location":"08a_FA_genomic_annotation.html#annotationdbi","text":"AnnotationDbi is an R package that provides an interface for connecting and querying various annotation databases using SQLite data storage. The AnnotationDbi packages can query the OrgDb , TxDb , EnsDb , Go.db , and BioMart annotations. There is helpful documentation available to reference when extracting data from any of these databases.","title":"AnnotationDbi"},{"location":"08a_FA_genomic_annotation.html#annotationhub","text":"AnnotationHub is a wonderful resource for accessing genomic data or querying large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc. All of this information is stored and easily accessible by directly connecting to the database. To get started with AnnotationHub, we first load the library and connect to the database: # Load libraries library ( AnnotationHub ) library ( ensembldb ) # Connect to AnnotationHub ah <- AnnotationHub ()","title":"AnnotationHub"},{"location":"08a_FA_genomic_annotation.html#what-is-a-cache","text":"A cache is used in R to store data or a copy of the data so that future requests can be served faster without having to re-run a lengthy computation. The AnnotationHub() command creates a client that manages a local cache of the database, helping with quick and reproducible access. When encountering question AnnotationHub does not exist, create directory? , you can anwser either yes (create a permanent location to store cache) or no (create a temporary location to store cache). hubCache(ah) gets the file system location of the local AnnotationHub cache. hubUrl(ah) gets the URL for the online hub. To see the types of information stored inside our database, we can just type the name of the object: # Explore the AnnotationHub object ah Using the output, you can get an idea of the information that you can query within the AnnotationHub object: AnnotationHub with 47240 records # snapshotDate(): 2019-10-29 # $dataprovider: BroadInstitute, Ensembl, UCSC, ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/, H... # $species: Homo sapiens, Mus musculus, Drosophila melanogaster, Bos taurus, Pan troglod... # $rdataclass: GRanges, BigWigFile, TwoBitFile, Rle, OrgDb, EnsDb, ChainFile, TxDb, Inpa... # additional mcols(): taxonomyid, genome, description, coordinate_1_based, # maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl, # sourcetype # retrieve records with, e.g., 'object[[\"AH5012\"]]' title AH5012 | Chromosome Band AH5013 | STS Markers AH5014 | FISH Clones AH5015 | Recomb Rate AH5016 | ENCODE Pilot ... ... AH78364 | Xiphophorus_maculatus.X_maculatus-5.0-male.ncrna.2bit AH78365 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.cdna.all.2bit AH78366 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_rm.toplevel.2bit AH78367 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_sm.toplevel.2bit AH78368 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.ncrna.2bit Notice the note on retrieving records with object[[AH2]] - this will be how we can extract a single record from the AnnotationHub object. If you would like to see more information about any of the classes of data you can extract that information as well. For example, if you wanted to determine all species information available , you could explore that within the AnnotationHub object: # Explore all species information available unique ( ah $ species ) %>% View () In addition to species information, there is also additional information about the type of Data Objects and the Data Providers: # Explore the types of Data Objects available unique ( ah $ rdataclass ) %>% View () # Explore the Data Providers unique ( ah $ dataprovider ) %>% View () Now that we know the types of information available from AnnotationHub we can query it for the information we want using the query() function. Let\u2019s say we would like to return the Ensembl EnsDb information for Human . To return the records available, we need to use the terms as they are output from the ah object to extract the desired data. # Query AnnotationHub human_ens <- query ( ah , c ( \"Homo sapiens\" , \"EnsDb\" )) The query retrieves all hits for the EnsDb objects , and you will see that they are listed by the release number. The most current release for GRCh38 is Ensembl98 and AnnotationHub offers that as an option to use. However, if you look at options for older releases, for Homo sapiens it only go back as far as Ensembl 87. This is fine if you are using GRCh38, however if you were using an older genome build like hg19/GRCh37, you would need to load the EnsDb package if available for that release or you might need to build your own with ensembldb . human_ens AnnotationHub with 13 records # snapshotDate(): 2019-10-29 # $dataprovider: Ensembl # $species: Homo sapiens # $rdataclass: EnsDb # additional mcols(): taxonomyid, genome, description, coordinate_1_based, # maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl, # sourcetype # retrieve records with, e.g., 'object[[\"AH53211\"]]' title AH53211 | Ensembl 87 EnsDb for Homo Sapiens AH53715 | Ensembl 88 EnsDb for Homo Sapiens AH56681 | Ensembl 89 EnsDb for Homo Sapiens AH57757 | Ensembl 90 EnsDb for Homo Sapiens AH60773 | Ensembl 91 EnsDb for Homo Sapiens ... ... AH67950 | Ensembl 95 EnsDb for Homo sapiens AH69187 | Ensembl 96 EnsDb for Homo sapiens AH73881 | Ensembl 97 EnsDb for Homo sapiens AH73986 | Ensembl 79 EnsDb for Homo sapiens AH75011 | Ensembl 98 EnsDb for Homo sapiens In our case, we are looking for the latest Ensembl release so that the annotations are the most up-to-date. To extract this information from AnnotationHub, we can use the AnnotationHub ID to subset the object : # Extract annotations of interest human_ens <- human_ens [[ \"AH75011\" ]] Now we can use ensembldb functions to extract the information at the gene, transcript, or exon levels. We are interested in the gene-level annotations, so we can extract that information as follows: # Extract gene-level information genes ( human_ens , return.type = \"data.frame\" ) %>% View () But note that it is just as easy to get the transcript- or exon-level information: # Extract transcript-level information transcripts ( human_ens , return.type = \"data.frame\" ) %>% View () # Extract exon-level information exons ( human_ens , return.type = \"data.frame\" ) %>% View () To obtain an annotation data frame using AnnotationHub, we\u2019ll use the genes() function, but only keep selected columns and filter out rows to keep those corresponding to our gene identifiers in our results file: # Create a gene-level dataframe annotations_ahb <- genes ( human_ens , return.type = \"data.frame\" ) %>% dplyr :: select ( gene_id , gene_name , entrezid , gene_biotype ) %>% dplyr :: filter ( gene_name %in% res_tableOE_tb $ gene ) This dataframe looks like it should be fine as it is, but we look a little closer we will notice that the column containing Entrez identifiers is a list, and in fact there are many Ensembl identifiers that map to more than one Entrez identifier! # Wait a second, we don't have one-to-one mappings! class ( annotations_ahb $ entrezid ) which ( map ( annotations_ahb $ entrezid , length ) > 1 ) So what do we do here? And why do we have this problem? An answer from the Ensembl Help Desk is that this occurs when we cannot choose a perfect match; ie when we have two good matches, but one does not appear to match with a better percentage than the other. In that case, we assign both matches. What we will do is choose to keep the first identifier for these multiple mapping cases . annotations_ahb $ entrezid <- map ( annotations_ahb $ entrezid , 1 ) %>% unlist () NOTE: Not all databases handle multiple mappings in the same way. For example, if we used the OrgDb instead of the EnsDb: human_orgdb <- query(ah, c(\"Homo sapiens\", \"OrgDb\")) human_orgdb <- human_ens[[\"AH75742\"]] annotations_orgdb <- select(human_orgdb, res_tableOE_tb$gene, c(\"SYMBOL\", \"GENENAME\", \"ENTREZID\"), \"ENSEMBL\") We would find that multiple mapping entries would be automatically reduced to one-to-one. We would also find that more than half of the input genes do not return any annotations. This is because the OrgDb family of database are primarily based on mapping using Entrez Gene identifiers. Since our data is based on Ensembl mappings, using the OrgDb would result in a loss of information. Let\u2019s take a look and see how many of our Ensembl identifiers have an associated gene symbol, and how many of them are unique: which ( is.na ( annotations_ahb $ gene_name )) %>% length () which ( duplicated ( annotations_ahb $ gene_name )) %>% length () Let\u2019s identify the non-duplicated genes and only keep the ones that are not duplicated: # Determine the indices for the non-duplicated genes non_duplicates_idx <- which ( duplicated ( annotations_ahb $ gene_name ) == FALSE ) # How many rows does annotations_ahb have? annotations_ahb %>% nrow () # Return only the non-duplicated genes using indices annotations_ahb <- annotations_ahb [ non_duplicates_idx , ] # How many rows are we left with after removing? annotations_ahb %>% nrow () Finally, it would be good to know what proportion of the Ensembl identifiers map to an Entrez identifier : # Determine how many of the Entrez column entries are NA which ( is.na ( annotations_ahb $ entrezid )) %>% length () That\u2019s more than half of our genes! If we plan on using Entrez ID results for downstream analysis, we should definitely keep this in mind. If you look at some of the Ensembl IDs from our query that returned NA, these map to pseudogenes (i.e ENSG00000265439 ) or non-coding RNAs (i.e. ENSG00000265425 ). The discrepancy (which we can expect to observe) between databases is due to the fact that each implements its own different computational approaches for generating the gene builds.","title":"What is a cache?"},{"location":"08a_FA_genomic_annotation.html#using-annotationhub-to-create-our-tx2gene-file","text":"To create our tx2gene file, we would need to use a combination of the methods above and merge two dataframes together. For example: ## DO NOT RUN THIS CODE # Create a transcript dataframe txdb <- transcripts ( human_ens , return.type = \"data.frame\" ) %>% dplyr :: select ( tx_id , gene_id ) txdb <- txdb [ grep ( \"ENST\" , txdb $ tx_id ),] # Create a gene-level dataframe genedb <- genes ( human_ens , return.type = \"data.frame\" ) %>% dplyr :: select ( gene_id , gene_name ) # Merge the two dataframes together annotations <- inner_join ( txdb , genedb ) In this lesson our focus has been using annotation packages to extract information mainly just for gene ID conversion for the different tools that we use downstream. Many of the annotation packages we have presented have much more information than what we need for functional analysis and we have only just scratched the surface here. It\u2019s good to know the capabilities of the tools we use, so we encourage you to spend some time exploring these packages to become more familiar with them. NOTE: The annotables package is a super easy annotation package to use. It is not updated frequently, so it\u2019s not great for getting the most up-to-date information for the current builds and does not have information for other organisms than human and mouse, but is a quick way to get annotation information. # Install package BiocManager :: install ( \"annotables\" ) # Load library library ( annotables ) # Access previous build of annotations grch38 This lesson was originally developed by members of the teaching team (Mary Piper) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Using AnnotationHub to create our tx2gene file"},{"location":"08b_FA_overrepresentation.html","text":"Approximate time: 120 minutes Learning Objectives: \u00b6 Determine how functions are attributed to genes using Gene Ontology terms Describe the theory of how functional enrichment tools yield statistically enriched functions or interactions Discuss functional analysis using over-representation analysis, functional class scoring, and pathway topology methods Identify popular functional analysis tools for over-representation analysis Functional analysis \u00b6 The output of RNA-seq differential expression analysis is a list of significant differentially expressed genes (DEGs). To gain greater biological insight on the differentially expressed genes there are various analyses that can be done: determine whether there is enrichment of known biological functions, interactions, or pathways identify genes\u2019 involvement in novel pathways or networks by grouping genes together based on similar trends use global changes in gene expression by visualizing all genes being significantly up- or down-regulated in the context of external interaction data Generally for any differential expression analysis, it is useful to interpret the resulting gene lists using freely available web- and R-based tools. While tools for functional analysis span a wide variety of techniques, they can loosely be categorized into three main types: over-representation analysis, functional class scoring, and pathway topology [ 1 ]. The goal of functional analysis is to provide biological insight, so it\u2019s necessary to analyze our results in the context of our experimental hypothesis: FMRP and MOV10 associate and regulate the translation of a subset of RNAs . Therefore, based on the authors\u2019 hypothesis, we may expect the enrichment of processes/pathways related to translation, splicing, and the regulation of mRNAs , which we would need to validate experimentally. Note that all tools described below are great tools to validate experimental results and to make hypotheses. These tools suggest genes/pathways that may be involved with your condition of interest; however, you should NOT use these tools to make conclusions about the pathways involved in your experimental process. You will need to perform experimental validation of any suggested pathways. Over-representation analysis \u00b6 There are a plethora of functional enrichment tools that perform some type of \u201cover-representation\u201d analysis by querying databases containing information about gene function and interactions. These databases typically categorize genes into groups (gene sets) based on shared function, or involvement in a pathway, or presence in a specific cellular location, or other categorizations, e.g. functional pathways, etc. Essentially, known genes are binned into categories that have been consistently named (controlled vocabulary) based on how the gene has been annotated functionally. These categories are independent of any organism, however each organism has distinct categorizations available. To determine whether any categories are over-represented, you can determine the probability of having the observed proportion of genes associated with a specific category in your gene list based on the proportion of genes associated with the same category in the background set (gene categorizations for the appropriate organism). The statistical test that will determine whether something is actually over-represented is the Hypergeometric test . Hypergeometric testing \u00b6 Using the example of the first functional category above, hypergeometric distribution is a probability distribution that describes the probability of 25 genes (k) being associated with \u201cFunctional category 1\u201d, for all genes in our gene list (n=1000), from a population of all of the genes in entire genome (N=13,000) which contains 35 genes (K) associated with \u201cFunctional category 1\u201d [ 4 ]. The calculation of probability of k successes follows the formula: This test will result in an adjusted p-value (after multiple test correction) for each category tested. Gene Ontology project \u00b6 One of the most widely-used categorizations is the Gene Ontology (GO) established by the Gene Ontology project. \u201cThe Gene Ontology project is a collaborative effort to address the need for consistent descriptions of gene products across databases\u201d [ 2 ]. The Gene Ontology Consortium maintains the GO terms, and these GO terms are incorporated into gene annotations in many of the popular repositories for animal, plant, and microbial genomes. Tools that investigate enrichment of biological functions or interactions often use the Gene Ontology (GO) categorizations, i.e. the GO terms to determine whether any have significantly modified representation in a given list of genes. Therefore, to best use and interpret the results from these functional analysis tools, it is helpful to have a good understanding of the GO terms themselves and their organization. GO Ontologies \u00b6 To describe the roles of genes and gene products, GO terms are organized into three independent controlled vocabularies (ontologies) in a species-independent manner: Biological process: refers to the biological role involving the gene or gene product, and could include \u201ctranscription\u201d, \u201csignal transduction\u201d, and \u201capoptosis\u201d. A biological process generally involves a chemical or physical change of the starting material or input. Molecular function: represents the biochemical activity of the gene product, such activities could include \u201cligand\u201d, \u201cGTPase\u201d, and \u201ctransporter\u201d. Cellular component: refers to the location in the cell of the gene product. Cellular components could include \u201cnucleus\u201d, \u201clysosome\u201d, and \u201cplasma membrane\u201d. Each GO term has a term name (e.g. DNA repair ) and a unique term accession number ( ), and a single gene product can be associated with many GO terms, since a single gene product \u201cmay function in several processes, contain domains that carry out diverse molecular functions, and participate in multiple alternative interactions with other proteins, organelles or locations in the cell\u201d [ 3 ]. GO term hierarchy \u00b6 Some gene products are well-researched, with vast quantities of data available regarding their biological processes and functions. However, other gene products have very little data available about their roles in the cell. For example, the protein, \u201cp53\u201d, would contain a wealth of information on it\u2019s roles in the cell, whereas another protein might only be known as a \u201cmembrane-bound protein\u201d with no other information available. The GO ontologies were developed to describe and query biological knowledge with differing levels of information available. To do this, GO ontologies are loosely hierarchical, ranging from general, \u2018parent\u2019, terms to more specific, \u2018child\u2019 terms. The GO ontologies are \u201cloosely\u201d hierarchical since \u2018child\u2019 terms can have multiple \u2018parent\u2019 terms. Some genes with less information may only be associated with general \u2018parent\u2019 terms or no terms at all, while other genes with a lot of information be associated with many terms. Tips for working with GO terms clusterProfiler \u00b6 We will be using clusterProfiler to perform over-representation analysis on GO terms associated with our list of significant genes. The tool takes as input a significant gene list and a background gene list and performs statistical enrichment analysis using hypergeometric testing. The basic arguments allow the user to select the appropriate organism and GO ontology (BP, CC, MF) to test. Running clusterProfiler \u00b6 To run clusterProfiler GO over-representation analysis, we will change our gene names into Ensembl IDs, since the tool works a bit easier with the Ensembl IDs. Then load the following libraries: # Load libraries library ( DOSE ) library ( pathview ) library ( clusterProfiler ) library ( org.Hs.eg.db ) For the different steps in the functional analysis, we require Ensembl and Entrez IDs. We will use the gene annotations that we generated previously to merge with our differential expression results. ## Merge the AnnotationHub dataframe with the results res_ids <- left_join ( res_tableOE_tb , annotations_ahb , by = c ( \"gene\" = \"gene_name\" )) NOTE: If you were unable to generate the annotations_ahb object, you can download the annotations to your data folder by right-clicking here and selecting \u201cSave link as\u2026\u201d To read in the object, you can run the following code: annotations_ahb <- read.csv(\"annotations_ahb.csv\") To perform the over-representation analysis, we need a list of background genes and a list of significant genes. For our background dataset we will use all genes tested for differential expression (all genes in our results table). For our significant gene list we will use genes with p-adjusted values less than 0.05 (we could include a fold change threshold too if we have many DE genes). ## Create background dataset for hypergeometric testing using all genes tested for significance in the results allOE_genes <- dplyr :: filter ( res_ids , ! is.na ( gene_id )) %>% pull ( gene_id ) %>% as.character () ## Extract significant results sigOE <- dplyr :: filter ( res_ids , padj < 0.05 & ! is.na ( gene_id )) sigOE_genes <- sigOE %>% pull ( gene_id ) %>% as.character () Now we can perform the GO enrichment analysis and save the results: ## Run GO enrichment analysis ego <- enrichGO ( gene = sigOE_genes , universe = allOE_genes , keyType = \"ENSEMBL\" , OrgDb = org.Hs.eg.db , ont = \"BP\" , pAdjustMethod = \"BH\" , qvalueCutoff = 0.05 , readable = TRUE ) NOTE: The different organisms with annotation databases available to use with for the OrgDb argument can be found here . Also, the keyType argument may be coded as keytype in different versions of clusterProfiler. Finally, the ont argument can accept either \u201cBP\u201d (Biological Process), \u201cMF\u201d (Molecular Function), and \u201cCC\u201d (Cellular Component) subontologies, or \u201cALL\u201d for all three. ## Output results from GO analysis to a table cluster_summary <- data.frame ( ego ) write.csv ( cluster_summary , \"Results/clusterProfiler_Mov10oe.csv\" ) NOTE: Instead of saving just the results summary from the ego object, it might also be beneficial to save the object itself. The save() function enables you to save it as a .rda file, e.g. save(ego, file=\"results/ego.rda\") . The complementary function to save() is the function load() , e.g. ego <- load(file=\"results/ego.rda\") . This is a useful set of functions to know, since it enables one to preserve analyses at specific stages and reload them when needed. More information about these functions can be found here & here . NOTE: You can also perform GO enrichment analysis with only the up or down regulated genes in addition to performing it for the full list of significant genes. This can be useful to identify GO terms impacted in one direction and not the other. If very few genes are in any of these lists (\\< 50, roughly) it may not be possible to get any significant GO terms. ## Extract upregulated genes sigOE_up <- dplyr::filter(res_ids, padj < 0.05 & log2foldchange > 0) sigOE_up_genes <- as.character(sigOE_up$gene) ## Extract downregulated genes sigOE_down <- dplyr::filter(res_ids, padj < 0.05 & log2foldchange < 0) sigOE_down_genes <- as.character(sigOE_down$gene) You can then create ego_up & ego_down objects by running the enrichGO() function for gene = sigOE_up_genes or gene = sigOE_down_genes . Visualizing clusterProfiler results \u00b6 clusterProfiler has a variety of options for viewing the over-represented GO terms. We will explore the dotplot, enrichment plot, and the category netplot. The dotplot shows the number of genes associated with the first 50 terms (size) and the p-adjusted values for these terms (color). This plot displays the top 50 GO terms by gene ratio (# genes related to GO term / total number of sig genes), not p-adjusted value. ## Dotplot dotplot ( ego , showCategory = 50 ) The next plot is the enrichment GO plot , which shows the relationship between the top 50 most significantly enriched GO terms (padj.), by grouping similar terms together. Before creating the plot, we will need to obtain the similarity between terms using the pairwise_termsim() function ( instructions for emapplot ). In the enrichment plot, the color represents the p-values relative to the other displayed terms (brighter red is more significant), and the size of the terms represents the number of genes that are significant from our list. ## Add similarity matrix to the termsim slot of enrichment result ego <- enrichplot :: pairwise_termsim ( ego ) ## Enrichmap clusters the 50 most significant (by padj) GO terms to visualize relationships between terms emapplot ( ego , showCategory = 50 ) Finally, the category netplot shows the relationships between the genes associated with the top five most significant GO terms and the fold changes of the significant genes associated with these terms (color). The size of the GO terms reflects the pvalues of the terms, with the more significant terms being larger. This plot is particularly useful for hypothesis generation in identifying genes that may be important to several of the most affected processes. Note - You may need to install the ggnewscale package using install.packages(\"ggnewscale\") for the cnetplot() function to work. ## To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector OE_foldchanges <- sigOE $ log2FoldChange names ( OE_foldchanges ) <- sigOE $ gene ## Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj) cnetplot ( ego , categorySize = \"pvalue\" , showCategory = 5 , foldChange = OE_foldchanges , vertex.label.font = 6 ) ## If some of the high fold changes are getting drowned out due to a large range, you could set a maximum fold change value OE_foldchanges <- ifelse ( OE_foldchanges > 2 , 2 , OE_foldchanges ) OE_foldchanges <- ifelse ( OE_foldchanges < -2 , -2 , OE_foldchanges ) cnetplot ( ego , categorySize = \"pvalue\" , showCategory = 5 , foldChange = OE_foldchanges , vertex.label.font = 6 ) If you are interested in significant processes that are not among the top five, you can subset your ego dataset to only display these processes: ## Subsetting the ego results without overwriting original `ego` variable ego2 <- ego ego2 @ result <- ego @ result [ c ( 1 , 3 , 4 , 8 , 9 ),] ## Plotting terms of interest cnetplot ( ego2 , categorySize = \"pvalue\" , foldChange = OE_foldchanges , showCategory = 5 , vertex.label.font = 6 ) This lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Over representation analysis"},{"location":"08b_FA_overrepresentation.html#learning-objectives","text":"Determine how functions are attributed to genes using Gene Ontology terms Describe the theory of how functional enrichment tools yield statistically enriched functions or interactions Discuss functional analysis using over-representation analysis, functional class scoring, and pathway topology methods Identify popular functional analysis tools for over-representation analysis","title":"Learning Objectives:"},{"location":"08b_FA_overrepresentation.html#functional-analysis","text":"The output of RNA-seq differential expression analysis is a list of significant differentially expressed genes (DEGs). To gain greater biological insight on the differentially expressed genes there are various analyses that can be done: determine whether there is enrichment of known biological functions, interactions, or pathways identify genes\u2019 involvement in novel pathways or networks by grouping genes together based on similar trends use global changes in gene expression by visualizing all genes being significantly up- or down-regulated in the context of external interaction data Generally for any differential expression analysis, it is useful to interpret the resulting gene lists using freely available web- and R-based tools. While tools for functional analysis span a wide variety of techniques, they can loosely be categorized into three main types: over-representation analysis, functional class scoring, and pathway topology [ 1 ]. The goal of functional analysis is to provide biological insight, so it\u2019s necessary to analyze our results in the context of our experimental hypothesis: FMRP and MOV10 associate and regulate the translation of a subset of RNAs . Therefore, based on the authors\u2019 hypothesis, we may expect the enrichment of processes/pathways related to translation, splicing, and the regulation of mRNAs , which we would need to validate experimentally. Note that all tools described below are great tools to validate experimental results and to make hypotheses. These tools suggest genes/pathways that may be involved with your condition of interest; however, you should NOT use these tools to make conclusions about the pathways involved in your experimental process. You will need to perform experimental validation of any suggested pathways.","title":"Functional analysis"},{"location":"08b_FA_overrepresentation.html#over-representation-analysis","text":"There are a plethora of functional enrichment tools that perform some type of \u201cover-representation\u201d analysis by querying databases containing information about gene function and interactions. These databases typically categorize genes into groups (gene sets) based on shared function, or involvement in a pathway, or presence in a specific cellular location, or other categorizations, e.g. functional pathways, etc. Essentially, known genes are binned into categories that have been consistently named (controlled vocabulary) based on how the gene has been annotated functionally. These categories are independent of any organism, however each organism has distinct categorizations available. To determine whether any categories are over-represented, you can determine the probability of having the observed proportion of genes associated with a specific category in your gene list based on the proportion of genes associated with the same category in the background set (gene categorizations for the appropriate organism). The statistical test that will determine whether something is actually over-represented is the Hypergeometric test .","title":"Over-representation analysis"},{"location":"08b_FA_overrepresentation.html#hypergeometric-testing","text":"Using the example of the first functional category above, hypergeometric distribution is a probability distribution that describes the probability of 25 genes (k) being associated with \u201cFunctional category 1\u201d, for all genes in our gene list (n=1000), from a population of all of the genes in entire genome (N=13,000) which contains 35 genes (K) associated with \u201cFunctional category 1\u201d [ 4 ]. The calculation of probability of k successes follows the formula: This test will result in an adjusted p-value (after multiple test correction) for each category tested.","title":"Hypergeometric testing"},{"location":"08b_FA_overrepresentation.html#gene-ontology-project","text":"One of the most widely-used categorizations is the Gene Ontology (GO) established by the Gene Ontology project. \u201cThe Gene Ontology project is a collaborative effort to address the need for consistent descriptions of gene products across databases\u201d [ 2 ]. The Gene Ontology Consortium maintains the GO terms, and these GO terms are incorporated into gene annotations in many of the popular repositories for animal, plant, and microbial genomes. Tools that investigate enrichment of biological functions or interactions often use the Gene Ontology (GO) categorizations, i.e. the GO terms to determine whether any have significantly modified representation in a given list of genes. Therefore, to best use and interpret the results from these functional analysis tools, it is helpful to have a good understanding of the GO terms themselves and their organization.","title":"Gene Ontology project"},{"location":"08b_FA_overrepresentation.html#go-ontologies","text":"To describe the roles of genes and gene products, GO terms are organized into three independent controlled vocabularies (ontologies) in a species-independent manner: Biological process: refers to the biological role involving the gene or gene product, and could include \u201ctranscription\u201d, \u201csignal transduction\u201d, and \u201capoptosis\u201d. A biological process generally involves a chemical or physical change of the starting material or input. Molecular function: represents the biochemical activity of the gene product, such activities could include \u201cligand\u201d, \u201cGTPase\u201d, and \u201ctransporter\u201d. Cellular component: refers to the location in the cell of the gene product. Cellular components could include \u201cnucleus\u201d, \u201clysosome\u201d, and \u201cplasma membrane\u201d. Each GO term has a term name (e.g. DNA repair ) and a unique term accession number ( ), and a single gene product can be associated with many GO terms, since a single gene product \u201cmay function in several processes, contain domains that carry out diverse molecular functions, and participate in multiple alternative interactions with other proteins, organelles or locations in the cell\u201d [ 3 ].","title":"GO Ontologies"},{"location":"08b_FA_overrepresentation.html#go-term-hierarchy","text":"Some gene products are well-researched, with vast quantities of data available regarding their biological processes and functions. However, other gene products have very little data available about their roles in the cell. For example, the protein, \u201cp53\u201d, would contain a wealth of information on it\u2019s roles in the cell, whereas another protein might only be known as a \u201cmembrane-bound protein\u201d with no other information available. The GO ontologies were developed to describe and query biological knowledge with differing levels of information available. To do this, GO ontologies are loosely hierarchical, ranging from general, \u2018parent\u2019, terms to more specific, \u2018child\u2019 terms. The GO ontologies are \u201cloosely\u201d hierarchical since \u2018child\u2019 terms can have multiple \u2018parent\u2019 terms. Some genes with less information may only be associated with general \u2018parent\u2019 terms or no terms at all, while other genes with a lot of information be associated with many terms. Tips for working with GO terms","title":"GO term hierarchy"},{"location":"08b_FA_overrepresentation.html#clusterprofiler","text":"We will be using clusterProfiler to perform over-representation analysis on GO terms associated with our list of significant genes. The tool takes as input a significant gene list and a background gene list and performs statistical enrichment analysis using hypergeometric testing. The basic arguments allow the user to select the appropriate organism and GO ontology (BP, CC, MF) to test.","title":"clusterProfiler"},{"location":"08b_FA_overrepresentation.html#running-clusterprofiler","text":"To run clusterProfiler GO over-representation analysis, we will change our gene names into Ensembl IDs, since the tool works a bit easier with the Ensembl IDs. Then load the following libraries: # Load libraries library ( DOSE ) library ( pathview ) library ( clusterProfiler ) library ( org.Hs.eg.db ) For the different steps in the functional analysis, we require Ensembl and Entrez IDs. We will use the gene annotations that we generated previously to merge with our differential expression results. ## Merge the AnnotationHub dataframe with the results res_ids <- left_join ( res_tableOE_tb , annotations_ahb , by = c ( \"gene\" = \"gene_name\" )) NOTE: If you were unable to generate the annotations_ahb object, you can download the annotations to your data folder by right-clicking here and selecting \u201cSave link as\u2026\u201d To read in the object, you can run the following code: annotations_ahb <- read.csv(\"annotations_ahb.csv\") To perform the over-representation analysis, we need a list of background genes and a list of significant genes. For our background dataset we will use all genes tested for differential expression (all genes in our results table). For our significant gene list we will use genes with p-adjusted values less than 0.05 (we could include a fold change threshold too if we have many DE genes). ## Create background dataset for hypergeometric testing using all genes tested for significance in the results allOE_genes <- dplyr :: filter ( res_ids , ! is.na ( gene_id )) %>% pull ( gene_id ) %>% as.character () ## Extract significant results sigOE <- dplyr :: filter ( res_ids , padj < 0.05 & ! is.na ( gene_id )) sigOE_genes <- sigOE %>% pull ( gene_id ) %>% as.character () Now we can perform the GO enrichment analysis and save the results: ## Run GO enrichment analysis ego <- enrichGO ( gene = sigOE_genes , universe = allOE_genes , keyType = \"ENSEMBL\" , OrgDb = org.Hs.eg.db , ont = \"BP\" , pAdjustMethod = \"BH\" , qvalueCutoff = 0.05 , readable = TRUE ) NOTE: The different organisms with annotation databases available to use with for the OrgDb argument can be found here . Also, the keyType argument may be coded as keytype in different versions of clusterProfiler. Finally, the ont argument can accept either \u201cBP\u201d (Biological Process), \u201cMF\u201d (Molecular Function), and \u201cCC\u201d (Cellular Component) subontologies, or \u201cALL\u201d for all three. ## Output results from GO analysis to a table cluster_summary <- data.frame ( ego ) write.csv ( cluster_summary , \"Results/clusterProfiler_Mov10oe.csv\" ) NOTE: Instead of saving just the results summary from the ego object, it might also be beneficial to save the object itself. The save() function enables you to save it as a .rda file, e.g. save(ego, file=\"results/ego.rda\") . The complementary function to save() is the function load() , e.g. ego <- load(file=\"results/ego.rda\") . This is a useful set of functions to know, since it enables one to preserve analyses at specific stages and reload them when needed. More information about these functions can be found here & here . NOTE: You can also perform GO enrichment analysis with only the up or down regulated genes in addition to performing it for the full list of significant genes. This can be useful to identify GO terms impacted in one direction and not the other. If very few genes are in any of these lists (\\< 50, roughly) it may not be possible to get any significant GO terms. ## Extract upregulated genes sigOE_up <- dplyr::filter(res_ids, padj < 0.05 & log2foldchange > 0) sigOE_up_genes <- as.character(sigOE_up$gene) ## Extract downregulated genes sigOE_down <- dplyr::filter(res_ids, padj < 0.05 & log2foldchange < 0) sigOE_down_genes <- as.character(sigOE_down$gene) You can then create ego_up & ego_down objects by running the enrichGO() function for gene = sigOE_up_genes or gene = sigOE_down_genes .","title":"Running clusterProfiler"},{"location":"08b_FA_overrepresentation.html#visualizing-clusterprofiler-results","text":"clusterProfiler has a variety of options for viewing the over-represented GO terms. We will explore the dotplot, enrichment plot, and the category netplot. The dotplot shows the number of genes associated with the first 50 terms (size) and the p-adjusted values for these terms (color). This plot displays the top 50 GO terms by gene ratio (# genes related to GO term / total number of sig genes), not p-adjusted value. ## Dotplot dotplot ( ego , showCategory = 50 ) The next plot is the enrichment GO plot , which shows the relationship between the top 50 most significantly enriched GO terms (padj.), by grouping similar terms together. Before creating the plot, we will need to obtain the similarity between terms using the pairwise_termsim() function ( instructions for emapplot ). In the enrichment plot, the color represents the p-values relative to the other displayed terms (brighter red is more significant), and the size of the terms represents the number of genes that are significant from our list. ## Add similarity matrix to the termsim slot of enrichment result ego <- enrichplot :: pairwise_termsim ( ego ) ## Enrichmap clusters the 50 most significant (by padj) GO terms to visualize relationships between terms emapplot ( ego , showCategory = 50 ) Finally, the category netplot shows the relationships between the genes associated with the top five most significant GO terms and the fold changes of the significant genes associated with these terms (color). The size of the GO terms reflects the pvalues of the terms, with the more significant terms being larger. This plot is particularly useful for hypothesis generation in identifying genes that may be important to several of the most affected processes. Note - You may need to install the ggnewscale package using install.packages(\"ggnewscale\") for the cnetplot() function to work. ## To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector OE_foldchanges <- sigOE $ log2FoldChange names ( OE_foldchanges ) <- sigOE $ gene ## Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj) cnetplot ( ego , categorySize = \"pvalue\" , showCategory = 5 , foldChange = OE_foldchanges , vertex.label.font = 6 ) ## If some of the high fold changes are getting drowned out due to a large range, you could set a maximum fold change value OE_foldchanges <- ifelse ( OE_foldchanges > 2 , 2 , OE_foldchanges ) OE_foldchanges <- ifelse ( OE_foldchanges < -2 , -2 , OE_foldchanges ) cnetplot ( ego , categorySize = \"pvalue\" , showCategory = 5 , foldChange = OE_foldchanges , vertex.label.font = 6 ) If you are interested in significant processes that are not among the top five, you can subset your ego dataset to only display these processes: ## Subsetting the ego results without overwriting original `ego` variable ego2 <- ego ego2 @ result <- ego @ result [ c ( 1 , 3 , 4 , 8 , 9 ),] ## Plotting terms of interest cnetplot ( ego2 , categorySize = \"pvalue\" , foldChange = OE_foldchanges , showCategory = 5 , vertex.label.font = 6 ) This lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Visualizing clusterProfiler results"},{"location":"08c_FA_GSEA.html","text":"Approximate time: 40 minutes Learning Objectives: \u00b6 Discuss functional class scoring, and pathway topology methods Construct a GSEA analysis using GO and KEGG gene sets Examine results of a GSEA using pathview package List other tools and resources for identifying genes of novel pathways or networks Functional analysis \u00b6 Over-representation analysis is only a single type of functional analysis method that is available for teasing apart the biological processes important to your condition of interest. Other types of analyses can be equally important or informative, including functional class scoring methods. Functional class scoring \u00b6 Functional class scoring (FCS) tools, such as GSEA , most often use the gene-level statistics or log2 fold changes for all genes from the differential expression results, then look to see whether gene sets for particular biological pathways are enriched among the large positive or negative fold changes. The hypothesis of FCS methods is that although large changes in individual genes can have significant effects on pathways (and will be detected via ORA methods), weaker but coordinated changes in sets of functionally related genes (i.e., pathways) can also have significant effects. Thus, rather than setting an arbitrary threshold to identify \u2018significant genes\u2019, all genes are considered in the analysis. The gene-level statistics from the dataset are aggregated to generate a single pathway-level statistic and statistical significance of each pathway is reported. This type of analysis can be particularly helpful if the differential expression analysis only outputs a small list of significant DE genes. Gene set enrichment analysis using clusterProfiler and Pathview \u00b6 Using the log2 fold changes obtained from the differential expression analysis for every gene, gene set enrichment analysis and pathway analysis can be performed using clusterProfiler and Pathview tools. For a gene set or pathway analysis using clusterProfiler, coordinated differential expression over gene sets is tested instead of changes of individual genes. \u201cGene sets are pre-defined groups of genes, which are functionally related. Commonly used gene sets include those derived from KEGG pathways, Gene Ontology terms, MSigDB, Reactome, or gene groups that share some other functional annotations, etc. Consistent perturbations over such gene sets frequently suggest mechanistic changes\u201d. Preparation for GSEA \u00b6 clusterProfiler offers several functions to perform GSEA using different genes sets, including but not limited to GO, KEGG, and MSigDb. We will use the KEGG gene sets, which identify genes using their Entrez IDs. Therefore, to perform the analysis, we will need to acquire the Entrez IDs. We will also need to remove the Entrez ID NA values and duplicates (due to gene ID conversion) prior to the analysis: ## Remove any NA values (reduces the data by quite a bit) and duplicates res_entrez <- dplyr :: filter ( res_ids , entrezid != \"NA\" & duplicated ( entrezid ) == F ) Finally, extract and name the fold changes: ## Extract the foldchanges foldchanges <- res_entrez $ log2FoldChange ## Name each fold change with the corresponding Entrez ID names ( foldchanges ) <- res_entrez $ entrezid Next we need to order the fold changes in decreasing order. To do this we\u2019ll use the sort() function, which takes a vector as input. This is in contrast to Tidyverse\u2019s arrange() , which requires a data frame. ## Sort fold changes in decreasing order foldchanges <- sort ( foldchanges , decreasing = TRUE ) head ( foldchanges ) Theory of GSEA \u00b6 Now we are ready to perform GSEA. The details regarding GSEA can be found in the PNAS paper by Subramanian et al. We will describe briefly the steps outlined in the paper below: Image credit: Subramanian et al. Proceedings of the National Academy of Sciences Oct 2005, 102 (43) 15545-15550; DOI: 10.1073/pnas.0506580102 This image describes the theory of GSEA, with the \u2018gene set S\u2019 showing the metric used (in our case, ranked log2 fold changes) to determine enrichment of genes in the gene set. The left-most image is representing this metric used for the GSEA analysis. The log2 fold changes for each gene in the \u2018gene set S\u2019 is shown as a line in the middle image. The large positive log2 fold changes are at the top of the gene set image, while the largest negative log2 fold changes are at the bottom of the gene set image. In the right-most image, the gene set is turned horizontally, underneath which is an image depicting the calculations involved in determining enrichment, as described below. Step 1: Calculation of enrichment score: An enrichment score for a particular gene set is calculated by walking down the list of log2 fold changes and increasing the running-sum statistic every time a gene in the gene set is encountered and decreasing it when genes are not part of the gene set. The size of the increase/decrease is determined by magnitude of the log2 fold change. Larger (positive or negative) log2 fold changes will result in larger increases or decreases. The final enrichment score is where the running-sum statistic is the largest deviation from zero. Step 2: Estimation of significance: The significance of the enrichment score is determined using permutation testing, which performs rearrangements of the data points to determine the likelihood of generating an enrichment score as large as the enrichment score calculated from the observed data. Essentially, for this step, the first permutation would reorder the log2 fold changes and randomly assign them to different genes, reorder the gene ranks based on these new log2 fold changes, and recalculate the enrichment score. The second permutation would reorder the log2 fold changes again and recalculate the enrichment score again, and this would continue for the total number of permutations run. Therefore, the number of permutations run will increase the confidence in the signficance estimates. Step 3: Adjust for multiple test correction After all gene sets are tested, the enrichment scores are normalized for the size of the gene set, then the p-values are corrected for multiple testing. The GSEA output will yield the core genes in the gene sets that most highly contribute to the enrichment score. The genes output are generally the genes at or before the running sum reaches its maximum value (eg. the most influential genes driving the differences between conditions for that gene set). Performing GSEA \u00b6 To perform the GSEA using KEGG gene sets with clusterProfiler, we can use the gseKEGG() function: ## GSEA using gene sets from KEGG pathways gseaKEGG <- gseKEGG ( geneList = foldchanges , # ordered named vector of fold changes (Entrez IDs are the associated names) organism = \"hsa\" , # supported organisms listed below pvalueCutoff = 0.05 , # padj cutoff value verbose = FALSE ) ## Extract the GSEA results gseaKEGG_results <- gseaKEGG @ result NOTE: The organisms with KEGG pathway information are listed here . How many pathways are enriched? View the enriched pathways: ## Write GSEA results to file View ( gseaKEGG_results ) write.csv ( gseaKEGG_results , \"Results/gseaOE_kegg.csv\" , quote = F ) NOTE: We will all get different results for the GSEA because the permutations performed use random reordering. If we would like to use the same permutations every time we run a function (i.e. we would like the same results every time we run the function), then we could use the set.seed(123456) function prior to running. The input to set.seed() could be any number, but if you would want the same results, then you would need to use the same number as input. Explore the GSEA plot of enrichment of one of the pathways in the ranked list: ## Plot the GSEA plot for a single enriched pathway, `hsa03040` gseaplot ( gseaKEGG , geneSetID = 'hsa03040' ) In this plot, the lines in plot represent the genes in the gene set \u2018hsa03040\u2019, and where they occur among the log2 fold changes. The largest positive log2 fold changes are on the left-hand side of the plot, while the largest negative log2 fold changes are on the right. The top plot shows the magnitude of the log2 fold changes for each gene, while the bottom plot shows the running sum, with the enrichment score peaking at the red dotted line (which is among the negative log2 fold changes). Use the Pathview R package to integrate the KEGG pathway data from clusterProfiler into pathway images: #detach(\"package:dplyr\", unload=TRUE) # first unload dplyr to avoid conflicts ## Output images for a single significant KEGG pathway pathview ( gene.data = foldchanges , pathway.id = \"hsa03040\" , species = \"hsa\" , limit = list ( gene = 2 , # value gives the max/min limit for foldchanges cpd = 1 )) NOTE: If the below error message occurs: Error in detach(\"package:dplyr\", unload = T) : invalid 'name' argument , that means the dplyr package is not currently loaded. Ignore the message and continue to run pathview command. NOTE: Printing out Pathview images for all significant pathways can be easily performed as follows: ## Output images for all significant KEGG pathways get_kegg_plots <- function ( x ) { pathview ( gene.data = foldchanges , pathway.id = gseaKEGG_results $ ID [ x ], species = \"hsa\" , limit = list ( gene = 2 , cpd = 1 )) } purrr :: map ( 1 : length ( gseaKEGG_results $ ID ), get_kegg_plots ) Instead of exploring enrichment of KEGG gene sets, we can also explore the enrichment of BP Gene Ontology terms using gene set enrichment analysis: # GSEA using gene sets associated with BP Gene Ontology terms gseaGO <- gseGO ( geneList = foldchanges , OrgDb = org.Hs.eg.db , ont = 'BP' , minGSSize = 20 , pvalueCutoff = 0.05 , verbose = FALSE ) gseaGO_results <- gseaGO @ result gseaplot ( gseaGO , geneSetID = 'GO:0007423' ) There are other gene sets available for GSEA analysis in clusterProfiler (Disease Ontology, Reactome pathways, etc.). In addition, it is possible to supply your own gene set GMT file, such as a GMT for MSigDB using special clusterProfiler functions as shown below: # DO NOT RUN BiocManager :: install ( \"GSEABase\" ) library ( GSEABase ) # Load in GMT file of gene sets (we downloaded from the Broad Institute for MSigDB) c2 <- read.gmt ( \"/data/c2.cp.v6.0.entrez.gmt.txt\" ) msig <- GSEA ( foldchanges , TERM2GENE = c2 , verbose = FALSE ) msig_df <- data.frame ( msig ) Pathway topology tools \u00b6 The last main type of functional analysis technique is pathway topology analysis. Pathway topology analysis often takes into account gene interaction information along with the fold changes and adjusted p-values from differential expression analysis to identify dysregulated pathways. Depending on the tool, pathway topology tools explore how genes interact with each other (e.g. activation, inhibition, phosphorylation, ubiquitination, etc.) to determine the pathway-level statistics. Pathway topology-based methods utilize the number and type of interactions between gene product (our DE genes) and other gene products to infer gene function or pathway association. For instance, the SPIA (Signaling Pathway Impact Analysis) tool can be used to integrate the lists of differentially expressed genes, their fold changes, and pathway topology to identify affected pathways. Other Tools \u00b6 Co-expression clustering \u00b6 Co-expression clustering is often used to identify genes of novel pathways or networks by grouping genes together based on similar trends in expression. These tools are useful in identifying genes in a pathway, when their participation in a pathway and/or the pathway itself is unknown. These tools cluster genes with similar expression patterns to create \u2018modules\u2019 of co-expressed genes which often reflect functionally similar groups of genes. These \u2018modules\u2019 can then be compared across conditions or in a time-course experiment to identify any biologically relevant pathway or network information. You can visualize co-expression clustering using heatmaps, which should be viewed as suggestive only; serious classification of genes needs better methods. The way the tools perform clustering is by taking the entire expression matrix and computing pair-wise co-expression values. A network is then generated from which we explore the topology to make inferences on gene co-regulation. The WGCNA package (in R) is one example of a more sophisticated method for co-expression clustering. Resources for functional analysis \u00b6 g:Profiler - http://biit.cs.ut.ee/gprofiler/index.cgi DAVID - http://david.abcc.ncifcrf.gov/tools.jsp clusterProfiler - http://bioconductor.org/packages/release/bioc/html/clusterProfiler.html GeneMANIA - http://www.genemania.org/ GenePattern - http://www.broadinstitute.org/cancer/software/genepattern/ (need to register) WebGestalt - http://bioinfo.vanderbilt.edu/webgestalt/ (need to register) AmiGO - http://amigo.geneontology.org/amigo ReviGO (visualizing GO analysis, input is GO terms) - http://revigo.irb.hr/ WGCNA - https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/ GSEA - http://software.broadinstitute.org/gsea/index.jsp SPIA - https://www.bioconductor.org/packages/release/bioc/html/SPIA.html GAGE/Pathview - http://www.bioconductor.org/packages/release/bioc/html/gage.html This lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Class scoring"},{"location":"08c_FA_GSEA.html#learning-objectives","text":"Discuss functional class scoring, and pathway topology methods Construct a GSEA analysis using GO and KEGG gene sets Examine results of a GSEA using pathview package List other tools and resources for identifying genes of novel pathways or networks","title":"Learning Objectives:"},{"location":"08c_FA_GSEA.html#functional-analysis","text":"Over-representation analysis is only a single type of functional analysis method that is available for teasing apart the biological processes important to your condition of interest. Other types of analyses can be equally important or informative, including functional class scoring methods.","title":"Functional analysis"},{"location":"08c_FA_GSEA.html#functional-class-scoring","text":"Functional class scoring (FCS) tools, such as GSEA , most often use the gene-level statistics or log2 fold changes for all genes from the differential expression results, then look to see whether gene sets for particular biological pathways are enriched among the large positive or negative fold changes. The hypothesis of FCS methods is that although large changes in individual genes can have significant effects on pathways (and will be detected via ORA methods), weaker but coordinated changes in sets of functionally related genes (i.e., pathways) can also have significant effects. Thus, rather than setting an arbitrary threshold to identify \u2018significant genes\u2019, all genes are considered in the analysis. The gene-level statistics from the dataset are aggregated to generate a single pathway-level statistic and statistical significance of each pathway is reported. This type of analysis can be particularly helpful if the differential expression analysis only outputs a small list of significant DE genes.","title":"Functional class scoring"},{"location":"08c_FA_GSEA.html#gene-set-enrichment-analysis-using-clusterprofiler-and-pathview","text":"Using the log2 fold changes obtained from the differential expression analysis for every gene, gene set enrichment analysis and pathway analysis can be performed using clusterProfiler and Pathview tools. For a gene set or pathway analysis using clusterProfiler, coordinated differential expression over gene sets is tested instead of changes of individual genes. \u201cGene sets are pre-defined groups of genes, which are functionally related. Commonly used gene sets include those derived from KEGG pathways, Gene Ontology terms, MSigDB, Reactome, or gene groups that share some other functional annotations, etc. Consistent perturbations over such gene sets frequently suggest mechanistic changes\u201d.","title":"Gene set enrichment analysis using clusterProfiler and Pathview"},{"location":"08c_FA_GSEA.html#preparation-for-gsea","text":"clusterProfiler offers several functions to perform GSEA using different genes sets, including but not limited to GO, KEGG, and MSigDb. We will use the KEGG gene sets, which identify genes using their Entrez IDs. Therefore, to perform the analysis, we will need to acquire the Entrez IDs. We will also need to remove the Entrez ID NA values and duplicates (due to gene ID conversion) prior to the analysis: ## Remove any NA values (reduces the data by quite a bit) and duplicates res_entrez <- dplyr :: filter ( res_ids , entrezid != \"NA\" & duplicated ( entrezid ) == F ) Finally, extract and name the fold changes: ## Extract the foldchanges foldchanges <- res_entrez $ log2FoldChange ## Name each fold change with the corresponding Entrez ID names ( foldchanges ) <- res_entrez $ entrezid Next we need to order the fold changes in decreasing order. To do this we\u2019ll use the sort() function, which takes a vector as input. This is in contrast to Tidyverse\u2019s arrange() , which requires a data frame. ## Sort fold changes in decreasing order foldchanges <- sort ( foldchanges , decreasing = TRUE ) head ( foldchanges )","title":"Preparation for GSEA"},{"location":"08c_FA_GSEA.html#theory-of-gsea","text":"Now we are ready to perform GSEA. The details regarding GSEA can be found in the PNAS paper by Subramanian et al. We will describe briefly the steps outlined in the paper below: Image credit: Subramanian et al. Proceedings of the National Academy of Sciences Oct 2005, 102 (43) 15545-15550; DOI: 10.1073/pnas.0506580102 This image describes the theory of GSEA, with the \u2018gene set S\u2019 showing the metric used (in our case, ranked log2 fold changes) to determine enrichment of genes in the gene set. The left-most image is representing this metric used for the GSEA analysis. The log2 fold changes for each gene in the \u2018gene set S\u2019 is shown as a line in the middle image. The large positive log2 fold changes are at the top of the gene set image, while the largest negative log2 fold changes are at the bottom of the gene set image. In the right-most image, the gene set is turned horizontally, underneath which is an image depicting the calculations involved in determining enrichment, as described below. Step 1: Calculation of enrichment score: An enrichment score for a particular gene set is calculated by walking down the list of log2 fold changes and increasing the running-sum statistic every time a gene in the gene set is encountered and decreasing it when genes are not part of the gene set. The size of the increase/decrease is determined by magnitude of the log2 fold change. Larger (positive or negative) log2 fold changes will result in larger increases or decreases. The final enrichment score is where the running-sum statistic is the largest deviation from zero. Step 2: Estimation of significance: The significance of the enrichment score is determined using permutation testing, which performs rearrangements of the data points to determine the likelihood of generating an enrichment score as large as the enrichment score calculated from the observed data. Essentially, for this step, the first permutation would reorder the log2 fold changes and randomly assign them to different genes, reorder the gene ranks based on these new log2 fold changes, and recalculate the enrichment score. The second permutation would reorder the log2 fold changes again and recalculate the enrichment score again, and this would continue for the total number of permutations run. Therefore, the number of permutations run will increase the confidence in the signficance estimates. Step 3: Adjust for multiple test correction After all gene sets are tested, the enrichment scores are normalized for the size of the gene set, then the p-values are corrected for multiple testing. The GSEA output will yield the core genes in the gene sets that most highly contribute to the enrichment score. The genes output are generally the genes at or before the running sum reaches its maximum value (eg. the most influential genes driving the differences between conditions for that gene set).","title":"Theory of GSEA"},{"location":"08c_FA_GSEA.html#performing-gsea","text":"To perform the GSEA using KEGG gene sets with clusterProfiler, we can use the gseKEGG() function: ## GSEA using gene sets from KEGG pathways gseaKEGG <- gseKEGG ( geneList = foldchanges , # ordered named vector of fold changes (Entrez IDs are the associated names) organism = \"hsa\" , # supported organisms listed below pvalueCutoff = 0.05 , # padj cutoff value verbose = FALSE ) ## Extract the GSEA results gseaKEGG_results <- gseaKEGG @ result NOTE: The organisms with KEGG pathway information are listed here . How many pathways are enriched? View the enriched pathways: ## Write GSEA results to file View ( gseaKEGG_results ) write.csv ( gseaKEGG_results , \"Results/gseaOE_kegg.csv\" , quote = F ) NOTE: We will all get different results for the GSEA because the permutations performed use random reordering. If we would like to use the same permutations every time we run a function (i.e. we would like the same results every time we run the function), then we could use the set.seed(123456) function prior to running. The input to set.seed() could be any number, but if you would want the same results, then you would need to use the same number as input. Explore the GSEA plot of enrichment of one of the pathways in the ranked list: ## Plot the GSEA plot for a single enriched pathway, `hsa03040` gseaplot ( gseaKEGG , geneSetID = 'hsa03040' ) In this plot, the lines in plot represent the genes in the gene set \u2018hsa03040\u2019, and where they occur among the log2 fold changes. The largest positive log2 fold changes are on the left-hand side of the plot, while the largest negative log2 fold changes are on the right. The top plot shows the magnitude of the log2 fold changes for each gene, while the bottom plot shows the running sum, with the enrichment score peaking at the red dotted line (which is among the negative log2 fold changes). Use the Pathview R package to integrate the KEGG pathway data from clusterProfiler into pathway images: #detach(\"package:dplyr\", unload=TRUE) # first unload dplyr to avoid conflicts ## Output images for a single significant KEGG pathway pathview ( gene.data = foldchanges , pathway.id = \"hsa03040\" , species = \"hsa\" , limit = list ( gene = 2 , # value gives the max/min limit for foldchanges cpd = 1 )) NOTE: If the below error message occurs: Error in detach(\"package:dplyr\", unload = T) : invalid 'name' argument , that means the dplyr package is not currently loaded. Ignore the message and continue to run pathview command. NOTE: Printing out Pathview images for all significant pathways can be easily performed as follows: ## Output images for all significant KEGG pathways get_kegg_plots <- function ( x ) { pathview ( gene.data = foldchanges , pathway.id = gseaKEGG_results $ ID [ x ], species = \"hsa\" , limit = list ( gene = 2 , cpd = 1 )) } purrr :: map ( 1 : length ( gseaKEGG_results $ ID ), get_kegg_plots ) Instead of exploring enrichment of KEGG gene sets, we can also explore the enrichment of BP Gene Ontology terms using gene set enrichment analysis: # GSEA using gene sets associated with BP Gene Ontology terms gseaGO <- gseGO ( geneList = foldchanges , OrgDb = org.Hs.eg.db , ont = 'BP' , minGSSize = 20 , pvalueCutoff = 0.05 , verbose = FALSE ) gseaGO_results <- gseaGO @ result gseaplot ( gseaGO , geneSetID = 'GO:0007423' ) There are other gene sets available for GSEA analysis in clusterProfiler (Disease Ontology, Reactome pathways, etc.). In addition, it is possible to supply your own gene set GMT file, such as a GMT for MSigDB using special clusterProfiler functions as shown below: # DO NOT RUN BiocManager :: install ( \"GSEABase\" ) library ( GSEABase ) # Load in GMT file of gene sets (we downloaded from the Broad Institute for MSigDB) c2 <- read.gmt ( \"/data/c2.cp.v6.0.entrez.gmt.txt\" ) msig <- GSEA ( foldchanges , TERM2GENE = c2 , verbose = FALSE ) msig_df <- data.frame ( msig )","title":"Performing GSEA"},{"location":"08c_FA_GSEA.html#pathway-topology-tools","text":"The last main type of functional analysis technique is pathway topology analysis. Pathway topology analysis often takes into account gene interaction information along with the fold changes and adjusted p-values from differential expression analysis to identify dysregulated pathways. Depending on the tool, pathway topology tools explore how genes interact with each other (e.g. activation, inhibition, phosphorylation, ubiquitination, etc.) to determine the pathway-level statistics. Pathway topology-based methods utilize the number and type of interactions between gene product (our DE genes) and other gene products to infer gene function or pathway association. For instance, the SPIA (Signaling Pathway Impact Analysis) tool can be used to integrate the lists of differentially expressed genes, their fold changes, and pathway topology to identify affected pathways.","title":"Pathway topology tools"},{"location":"08c_FA_GSEA.html#other-tools","text":"","title":"Other Tools"},{"location":"08c_FA_GSEA.html#co-expression-clustering","text":"Co-expression clustering is often used to identify genes of novel pathways or networks by grouping genes together based on similar trends in expression. These tools are useful in identifying genes in a pathway, when their participation in a pathway and/or the pathway itself is unknown. These tools cluster genes with similar expression patterns to create \u2018modules\u2019 of co-expressed genes which often reflect functionally similar groups of genes. These \u2018modules\u2019 can then be compared across conditions or in a time-course experiment to identify any biologically relevant pathway or network information. You can visualize co-expression clustering using heatmaps, which should be viewed as suggestive only; serious classification of genes needs better methods. The way the tools perform clustering is by taking the entire expression matrix and computing pair-wise co-expression values. A network is then generated from which we explore the topology to make inferences on gene co-regulation. The WGCNA package (in R) is one example of a more sophisticated method for co-expression clustering.","title":"Co-expression clustering"},{"location":"08c_FA_GSEA.html#resources-for-functional-analysis","text":"g:Profiler - http://biit.cs.ut.ee/gprofiler/index.cgi DAVID - http://david.abcc.ncifcrf.gov/tools.jsp clusterProfiler - http://bioconductor.org/packages/release/bioc/html/clusterProfiler.html GeneMANIA - http://www.genemania.org/ GenePattern - http://www.broadinstitute.org/cancer/software/genepattern/ (need to register) WebGestalt - http://bioinfo.vanderbilt.edu/webgestalt/ (need to register) AmiGO - http://amigo.geneontology.org/amigo ReviGO (visualizing GO analysis, input is GO terms) - http://revigo.irb.hr/ WGCNA - https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/ GSEA - http://software.broadinstitute.org/gsea/index.jsp SPIA - https://www.bioconductor.org/packages/release/bioc/html/SPIA.html GAGE/Pathview - http://www.bioconductor.org/packages/release/bioc/html/gage.html This lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Resources for functional analysis"},{"location":"09_summarized_workflow.html","text":"Approximate time: 15 minutes Learning Objectives \u00b6 Identify the R commands needed to run a complete differential expression analysis using DESeq2 Summary of differential expression analysis workflow \u00b6 We have detailed the various steps in a differential expression analysis workflow, providing theory with example code. To provide a more succinct reference for the code needed to run a DGE analysis, we have summarized the steps in an analysis below: Obtaining gene-level counts from your preprocessing data <- read_table ( \"data/Mov10_full_counts.txt\" ) meta <- read_table ( \"meta/Mov10_full_meta.txt\" ) Creating the dds object: # Check that the row names of the metadata equal the column names of the **raw counts** data all ( colnames ( datacounts )[ -1 ] == meta $ samplename ) # Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( countData = data.frame ( data [, -1 ], row.names = data $ GeneSymbol ), colData = meta , design = ~ sampletype ) Exploratory data analysis (PCA & hierarchical clustering) - identifying outliers and sources of variation in the data: # Transform counts for data visualization rld <- rlog ( dds , blind = TRUE ) # Plot PCA plotPCA ( rld , intgroup = \"sampletype\" ) # Extract the rlog matrix from the object and compute pairwise correlation values rld_mat <- assay ( rld ) rld_cor <- cor ( rld_mat ) # Plot heatmap pheatmap ( rld_cor , annotation = meta %>% column_to_rownames ( \"samplename\" )) Run DESeq2: # **Optional step** - Re-create DESeq2 dataset if the design formula has changed after QC analysis in include other sources of variation using \"dds <- DESeqDataSetFromMatrix(data, colData = metadata, design = ~ covariate + condition)\" # Run DESeq2 differential expression analysis dds <- DESeq ( dds ) # **Optional step** - Output normalized counts to save as a file to access outside RStudio using \"normalized_counts <- counts(dds, normalized=TRUE)\" Check the fit of the dispersion estimates: # Plot dispersion estimates plotDispEsts ( dds ) Create contrasts to perform Wald testing on the shrunken log2 foldchanges between specific conditions: # Specify contrast for comparison of interest contrast <- c ( \"condition\" , \"level_to_compare\" , \"base_level\" ) # Output results of Wald test for contrast res <- results ( dds , contrast = contrast , alpha = 0.05 ) # Shrink the log2 fold changes to be more accurate res <- lfcShrink ( dds , contrast = contrast , type = \"normal\" ) Output significant results: # Set thresholds padj.cutoff < - 0.05 # Turn the results object into a tibble for use with tidyverse functions res_tbl <- res %>% data.frame () %>% rownames_to_column ( var = \"gene\" ) %>% as_tibble () # Subset the significant results sig_res <- filter ( res_tbl , padj < padj.cutoff ) Visualize results: volcano plots, heatmaps, normalized counts plots of top genes, etc. Perform analysis to extract functional significance of results: GO or KEGG enrichment, GSEA, etc. Make sure to output the versions of all tools used in the DE analysis: sessionInfo () For better reproducibility, it can help to create RMarkdown reports , which save all code, results, and visualizations as nicely formatted html reports. We have available an example html report for perusal. To create these reports we have additional materials available. This lesson was originally developed by members of the teaching team (Mary Piper) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Summarized workflow"},{"location":"09_summarized_workflow.html#learning-objectives","text":"Identify the R commands needed to run a complete differential expression analysis using DESeq2","title":"Learning Objectives"},{"location":"09_summarized_workflow.html#summary-of-differential-expression-analysis-workflow","text":"We have detailed the various steps in a differential expression analysis workflow, providing theory with example code. To provide a more succinct reference for the code needed to run a DGE analysis, we have summarized the steps in an analysis below: Obtaining gene-level counts from your preprocessing data <- read_table ( \"data/Mov10_full_counts.txt\" ) meta <- read_table ( \"meta/Mov10_full_meta.txt\" ) Creating the dds object: # Check that the row names of the metadata equal the column names of the **raw counts** data all ( colnames ( datacounts )[ -1 ] == meta $ samplename ) # Create DESeq2Dataset object dds <- DESeqDataSetFromMatrix ( countData = data.frame ( data [, -1 ], row.names = data $ GeneSymbol ), colData = meta , design = ~ sampletype ) Exploratory data analysis (PCA & hierarchical clustering) - identifying outliers and sources of variation in the data: # Transform counts for data visualization rld <- rlog ( dds , blind = TRUE ) # Plot PCA plotPCA ( rld , intgroup = \"sampletype\" ) # Extract the rlog matrix from the object and compute pairwise correlation values rld_mat <- assay ( rld ) rld_cor <- cor ( rld_mat ) # Plot heatmap pheatmap ( rld_cor , annotation = meta %>% column_to_rownames ( \"samplename\" )) Run DESeq2: # **Optional step** - Re-create DESeq2 dataset if the design formula has changed after QC analysis in include other sources of variation using \"dds <- DESeqDataSetFromMatrix(data, colData = metadata, design = ~ covariate + condition)\" # Run DESeq2 differential expression analysis dds <- DESeq ( dds ) # **Optional step** - Output normalized counts to save as a file to access outside RStudio using \"normalized_counts <- counts(dds, normalized=TRUE)\" Check the fit of the dispersion estimates: # Plot dispersion estimates plotDispEsts ( dds ) Create contrasts to perform Wald testing on the shrunken log2 foldchanges between specific conditions: # Specify contrast for comparison of interest contrast <- c ( \"condition\" , \"level_to_compare\" , \"base_level\" ) # Output results of Wald test for contrast res <- results ( dds , contrast = contrast , alpha = 0.05 ) # Shrink the log2 fold changes to be more accurate res <- lfcShrink ( dds , contrast = contrast , type = \"normal\" ) Output significant results: # Set thresholds padj.cutoff < - 0.05 # Turn the results object into a tibble for use with tidyverse functions res_tbl <- res %>% data.frame () %>% rownames_to_column ( var = \"gene\" ) %>% as_tibble () # Subset the significant results sig_res <- filter ( res_tbl , padj < padj.cutoff ) Visualize results: volcano plots, heatmaps, normalized counts plots of top genes, etc. Perform analysis to extract functional significance of results: GO or KEGG enrichment, GSEA, etc. Make sure to output the versions of all tools used in the DE analysis: sessionInfo () For better reproducibility, it can help to create RMarkdown reports , which save all code, results, and visualizations as nicely formatted html reports. We have available an example html report for perusal. To create these reports we have additional materials available. This lesson was originally developed by members of the teaching team (Mary Piper) at the Harvard Chan Bioinformatics Core (HBC) .","title":"Summary of differential expression analysis workflow"},{"location":"empty.html","text":"General Page \u00b6 Section Overview \ud83d\udd70 Time Estimation: X minutes \ud83d\udcac Learning Objectives: 1. First item 2. Second item Write your introduction to the page here. Useful functions \u00b6 You should continue to write your markdown document as normal. But here are some useful functions. You can find more in the Reference guide listed on the tab above. Be sure to delete the guide and the tips below when finished with them. Code \u00b6 Text will be highlighted appropriately when you include language abbreviation: import tensorflow as tf Admonitions \u00b6 The admonitions used for course/section overview and requirements should be consistent, though you can use any other admonitions freely. Examples (both drop-down and not): Quote Here is a quote What is the smallest country in the world? A: Vatican City Footnotes \u00b6 We can include footnotes like this one 1 . LaTeX \u00b6 You write an equation as normal: \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] Images \u00b6 Format is similar to links, but include an exclamation mark before: You can link to a URL or to somewhere locally. Remember to eat your vegetables. \u21a9","title":"General page"},{"location":"empty.html#general-page","text":"Section Overview \ud83d\udd70 Time Estimation: X minutes \ud83d\udcac Learning Objectives: 1. First item 2. Second item Write your introduction to the page here.","title":"General Page"},{"location":"empty.html#useful-functions","text":"You should continue to write your markdown document as normal. But here are some useful functions. You can find more in the Reference guide listed on the tab above. Be sure to delete the guide and the tips below when finished with them.","title":"Useful functions"},{"location":"empty.html#code","text":"Text will be highlighted appropriately when you include language abbreviation: import tensorflow as tf","title":"Code"},{"location":"empty.html#admonitions","text":"The admonitions used for course/section overview and requirements should be consistent, though you can use any other admonitions freely. Examples (both drop-down and not): Quote Here is a quote What is the smallest country in the world? A: Vatican City","title":"Admonitions"},{"location":"empty.html#footnotes","text":"We can include footnotes like this one 1 .","title":"Footnotes"},{"location":"empty.html#latex","text":"You write an equation as normal: \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\]","title":"LaTeX"},{"location":"empty.html#images","text":"Format is similar to links, but include an exclamation mark before: You can link to a URL or to somewhere locally. Remember to eat your vegetables. \u21a9","title":"Images"}]}