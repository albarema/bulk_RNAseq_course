[
  {
    "objectID": "develop/03_data_explanation.html",
    "href": "develop/03_data_explanation.html",
    "title": "Dataset explanation",
    "section": "",
    "text": "Dataset explanation\n\n\n\n\n\n\nSection Overview\n\n\n\n‚è∞ Time Estimation: 5 minutes\nüí¨ Learning Objectives:\n\nExplain the experiment and its objectives.\n\n\n\nWe will be using the sequencing reads from the RNA-Seq dataset that is part of a larger study described in Kenny PJ et al, Cell Rep 2014.\nNonetheless, we have decided to make the dataset and the study a bit more spicy. We have modified our gene annotation file so that we would get some interesting conditions and results! The story goes like this:\nA mysterious condition named ‚ÄúVampirium‚Äù is causing individuals to exhibit aggressive behavior and a strange desire for blood. Local individuals treat the symptoms using a concoction named ‚ÄúGarlicum‚Äù, which seem to return the individuals to a normal status.\n\nScientists suspect that the condition might be related to changes in expression of genes related to blood production and impulse control. Thus, they have taken samples of individuals affected by Vampirum, individuals cured by the Garlicum concoction and healthy individuals used as control samples. With these samples, they have performed bulk RNAseq.\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nAs you go through the upcoming sections, please keep the following questions in mind:\n\nWhat patterns of expression can we identify between the Vampirium individuals and controls?\nWhat happens to the Vampirium individuals when treated with Garlicum?\nAre there any genes shared between the two comparisons?\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Experiment design",
      "Dataset explanation"
    ]
  },
  {
    "objectID": "develop/07a_DEA.html",
    "href": "develop/07a_DEA.html",
    "title": "DESeq2",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 40 minutes\nüí¨ Learning Objectives:\n\nExplain the different steps involved in running DESeq()\nExamine size factors and undertand the source of differences\nInspect gene-level dispersion estimates\nRecognize the importance of dispersion during differential expression analysis\nGene-level differential expression analysis with DESeq2\nPreviously, we created the DESeq2 object using the appropriate design formula.\nThen, to run the actual differential expression analysis, we use a single call to the function DESeq().\nAnd with that we completed the entire workflow for the differential gene expression analysis with DESeq2! The DESeq() function performs a default analysis through the following steps:\nWe will be taking a detailed look at each of these steps to better understand how DESeq2 is performing the statistical analysis and what metrics we should examine to explore the quality of our analysis.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "DESeq2"
    ]
  },
  {
    "objectID": "develop/07a_DEA.html#step-1-estimate-size-factors",
    "href": "develop/07a_DEA.html#step-1-estimate-size-factors",
    "title": "DESeq2",
    "section": "Step 1: Estimate size factors",
    "text": "Step 1: Estimate size factors\nThe first step in the differential expression analysis is to estimate the size factors, which is exactly what we already did to normalize the raw counts. \nDESeq2 will automatically estimate the size factors when performing the differential expression analysis. However, if you have already generated the size factors using estimateSizeFactors(), as we did earlier, then DESeq2 will use these values.\nTo normalize the count data, DESeq2 calculates size factors for each sample using the median of ratios method discussed previously in the count normalization lesson.\n\nVampirium DE analysis: examining the size factors\nLet‚Äôs take a quick look at size factor values we have for each sample:\n## Check the size factors\nsizeFactors(dds)\ncontrol_1 control_2 control_3 garlicum_2 garlicum_3 vampirium_1 vampirium_2 \n 1.1149694  0.9606733  0.7492240  1.5633640  0.9359695  1.2262649  1.1405026 \nvampirium_3 \n 0.6542030 \nThese numbers should be identical to those we generated initially when we had run the function estimateSizeFactors(dds). Take a look at the total number of reads for each sample:\n## Total number of raw counts per sample\ncolSums(counts(dds))\n\n\n\n\n\n\nHow do the numbers correlate with the size factor?‚Äù\n\n\n\nWe see that the larger size factors correspond to the samples with higher sequencing depth, which makes sense, because to generate our normalized counts we need to divide the counts by the size factors. This accounts for the differences in sequencing depth between samples.\nNow take a look at the total depth after normalization using:\n## Total number of normalized counts per sample\ncolSums(counts(dds, normalized=T))\n\n\n\n\n\n\n\n\nHow do the values across samples compare with the total counts taken for each sample?‚Äù\n\n\n\nYou might have expected the counts to be the exact same across the samples after normalization. However, DESeq2 also accounts for RNA composition during the normalization procedure. By using the median ratio value for the size factor, DESeq2 should not be biased to a large number of counts sucked up by a few DE genes; however, this may lead to the size factors being quite different than what would be anticipated just based on sequencing depth.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "DESeq2"
    ]
  },
  {
    "objectID": "develop/07a_DEA.html#step-2-estimate-gene-wise-dispersion",
    "href": "develop/07a_DEA.html#step-2-estimate-gene-wise-dispersion",
    "title": "DESeq2",
    "section": "Step 2: Estimate gene-wise dispersion",
    "text": "Step 2: Estimate gene-wise dispersion\nThe next step in the differential expression analysis is the estimation of gene-wise dispersions. Before we get into the details, we should have a good idea about what dispersion is referring to in DESeq2. \nIn RNA-seq count data, we know:\n\nTo determine differentially expressed genes, we need to identify genes that have significantly different mean expression between groups given the variation within the groups (between replicates).\nThe variation within group (between replicates) needs to account for the fact that variance increases with the mean expression, as shown in the plot below (each black dot is a gene).\n\n\nTo accurately identify DE genes, DESeq2 needs to account for the relationship between the variance and mean. We don‚Äôt want all of our DE genes to be genes with low counts because the variance is lower for lowly expressed genes.\nInstead of using variance as the measure of variation in the data (since variance correlates with gene expression level), DESeq2 uses a measure of variation called dispersion, which accounts for a gene‚Äôs variance and mean expression level. Dispersion is calculated by:\n\\(Var=\\mu+\\alpha*\\mu^2\\), where:\n\n\\(\\alpha\\) = dispersion\n\\(Var\\) = variance\n\\(\\mu\\) = mean\n\nWhich results in the following relationship:\n\n\n\n\n\nEffect on dispersion\n\n\n\n\nVariance increases\nDispersion increases\n\n\nMean expression increases\nDispersion decreases\n\n\n\n\nFor genes with moderate to high count values, the square root of dispersion will be equal to the coefficient of variation. So 0.01 dispersion means 10% variation around the mean expected across biological replicates. The dispersion estimates for genes with the same mean will differ only based on their variance. Therefore, the dispersion estimates reflect the variance in gene expression for a given mean value. In the plot below, each black dot is a gene, and the dispersion is plotted against the mean expression (across within-group replicates) for each gene.\n\n\n\n\n\n\n\n\n\n\n\nHow does the dispersion relate to our model?\n\n\n\nTo accurately model sequencing counts, we need to generate accurate estimates of within-group variation (variation between replicates of the same sample group) for each gene. With only a few (3-6) replicates per group, the estimates of variation for each gene are often unreliable.\nTo address this problem, DESeq2 shares information across genes to generate more accurate estimates of variation based on the mean expression level of the gene using a method called ‚Äòshrinkage‚Äô. DESeq2 assumes that genes with similar expression levels should have similar dispersion.\n\n\n\n\n\n\n\n\nNote on estimating gene dispersion\n\n\n\nDESeq2 estimates the dispersion for each gene separately, based on the gene‚Äôs expression level (mean counts of within-group replicates) and variance.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "DESeq2"
    ]
  },
  {
    "objectID": "develop/07a_DEA.html#step-3-fit-and-shrink-gene-wise-dispersion-estimates",
    "href": "develop/07a_DEA.html#step-3-fit-and-shrink-gene-wise-dispersion-estimates",
    "title": "DESeq2",
    "section": "Step 3: Fit and shrink gene-wise dispersion estimates",
    "text": "Step 3: Fit and shrink gene-wise dispersion estimates\nThe next step in the workflow is to fit a curve to the gene-wise dispersion estimates and then shrink those estimates towards the curve. The idea behind fitting a curve to the data is that different genes will have different scales of biological variability, but, across all genes, there will be a distribution of reasonable estimates of dispersion.\n\nThis curve is displayed as a red line in the figure below, which plots the estimate for the expected dispersion value for genes of a given expression strength. Each black dot is a gene with an associated mean expression level and maximum likelihood estimation (MLE) of the dispersion (Step 1).\n\n\n\n\n\nAfter we fit the curve to the estimates, we can shrink the gene-wise dispersion estimates toward the expected dispersion values. Thanks to the curve we can identify more accurately differentially expressed genes when sample sizes are small, and the strength of the shrinkage for each gene depends on:\n\nhow close gene dispersions are from the curve\nsample size (more samples = less shrinkage)\n\nThis shrinkage method is particularly important to reduce false positives in the differential expression analysis. Genes with low dispersion estimates are shrunken towards the curve, and the more accurate, higher shrunken values are output for fitting of the model and differential expression testing. These shrunken estimates represent the within-group variation that is needed to determine whether the gene expression across groups is significantly different.\nDispersion estimates that are slightly above the curve are also shrunk toward the curve for better dispersion estimation; however, genes with extremely high dispersion values are not. This is due to the likelihood that the gene does not follow the modeling assumptions and has higher variability than others for biological or technical reasons [1]. Shrinking the values toward the curve could result in false positives, so these values are not shrunken. These genes are shown surrounded by blue circles below.\n\n\n\n\n\nThis is a good plot to evaluate whether your data is a good fit for the DESeq2 model. You expect your data to generally scatter around the curve, with the dispersion decreasing with increasing mean expression levels. If you see a cloud or different shapes, then you might want to explore your data more to see if you have contamination or outlier samples. Note how much shrinkage you get across the whole range of means in the plotDispEsts() plot for any experiment with low degrees of freedom.\nExamples of worrisome dispersion plots are shown below:\nThe plot below shows a cloud of dispersion values, which do not generally follow the curve. This would suggest a bad fit of the data to the model.\n\nThe next plot shows the dispersion values initially decreasing, then increasing with larger expression values. The larger mean expression values should not have larger dispersions based on our expectations - we expect decreasing dispersions with increasing mean. This indicates that there is less variation for more highly expressed genes than expected. This also indicates that there could be an outlier sample or contamination present in our analysis.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "DESeq2"
    ]
  },
  {
    "objectID": "develop/07a_DEA.html#vampirium-de-analysis-exploring-the-dispersion-estimates-and-assessing-model-fit",
    "href": "develop/07a_DEA.html#vampirium-de-analysis-exploring-the-dispersion-estimates-and-assessing-model-fit",
    "title": "DESeq2",
    "section": "Vampirium DE analysis: exploring the dispersion estimates and assessing model fit",
    "text": "Vampirium DE analysis: exploring the dispersion estimates and assessing model fit\nLet‚Äôs take a look at the dispersion estimates for our Vampirium data:\n## Plot dispersion estimates\nplotDispEsts(dds)\n\n\n\n\n\n\n\n\n\n\n\nSince we have a small sample size, for many genes we see quite a bit of shrinkage. Do you think our data are a good fit for the model?\n\n\n\n\n\nWe see a nice decrease in dispersion with increasing mean expression, which is good. We also see the dispersion estimates generally surround the curve, which is also expected. Overall, this plot looks good. We do see strong shrinkage, which is likely due to the fact that we have only two replicates for one of our sample groups. The more replicates we have, the less shrinkage is applied to the dispersion estimates, and the more DE genes are able to be identified. We would generally recommend having at least 4 biological replicates per condition for better estimation of variation.\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\nGiven the dispersion plot below, would you have any concerns regarding the fit of your data to the model?\n\nIf not, what aspects of the plot makes you feel confident about your data?\nIf so, what are your concerns? What would you do to address them?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe plot looks really bad. The fitted line (red) is above the final dispersions, and there is a ‚Äúrainfall‚Äù coming from the cloud of dispersions. This could mean that our original data has some sort of contamination or outlier, or that the count matrix is not a ‚Äúraw‚Äù count matrix.\n\n\n\n\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nSome materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "DESeq2"
    ]
  },
  {
    "objectID": "develop/06_exploratory_analysis.html",
    "href": "develop/06_exploratory_analysis.html",
    "title": "Exploratory analysis",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 80 minutes\nüí¨ Learning Objectives:\n\nRecognize the importance of methods for count data transformation\nDescribe the PCA (principal component analysis) technique\nInterpret different examples of PCA plots\nEvaluate sample quality using PCA and hierarchical clustering\nExploratory Analysis and Quality Control of bulk RNAseq\nThe next step in the DESeq2 workflow is QC, which includes sample-level and gene-level steps to perform QC checks on the count data to help us ensure that the samples/replicates look good.",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "develop/06_exploratory_analysis.html#sample-level-qc",
    "href": "develop/06_exploratory_analysis.html#sample-level-qc",
    "title": "Exploratory analysis",
    "section": "Sample-level QC",
    "text": "Sample-level QC\nA useful initial step in an RNA-seq analysis is often to assess overall similarity between samples:\n\nWhich samples are similar to each other, which are different?\nDoes this fit to the expectation from the experiment‚Äôs design?\nWhat are the major sources of variation in the dataset?\n\nTo explore the similarity of our samples, we will be performing sample-level QC using Principal Component Analysis (PCA) and hierarchical clustering methods. These methods/tools allow us to check how similar the replicates are to each other (clustering) and to make sure that the experimental condition is the major source of variation in the data. Sample-level QC can also help identify any samples behaving like outliers; we can further explore any potential outliers to determine whether they need to be removed prior to DE analysis.\n\nThese unsupervised clustering methods are run using log2 transformed normalized counts. The log2 transformation improves the sample distances for clustering visualization, i.e., it reduces the impact of large outlier counts. Instead of using a classical log2 transform, we will be using the regularized log transform (rlog). This type of transformation helps to avoid any bias from the abundance of low-count genes.\n\n\n\nImage adapted from ‚ÄúBeginner‚Äôs guide to using the DESeq2 package‚Äù by Love, Anders and Huber, 2014\n\n\n\n\n\n\n\n\nNote\n\n\n\nMany common statistical methods for exploratory analysis of multidimensional data, especially methods for clustering and ordination (e. g., principal-component analysis and the like), work best for (at least approximately) homoskedastic data; this means that the variance of an observable quantity (i.e., here, the expression strength of a gene) does not depend on the mean. In RNA-Seq data, however, variance grows with the mean. For example, if one performs PCA directly on a matrix of normalized read counts, the result typically depends only on the few most strongly expressed genes because they show the largest absolute differences between samples. A simple and often used strategy to avoid this is to take the logarithm of the normalized count values plus a small pseudocount; however, now the genes with low counts tend to dominate the results because, due to the strong Poisson noise inherent to small count values, they show the strongest relative differences between samples.\nAs a solution, DESeq2 offers the regularized-logarithm transformation, or rlog for short. For genes with high counts, the rlog transformation differs not much from an ordinary log2 transformation. For genes with lower counts, however, the values are shrunken towards the genes‚Äô averages across all samples. Using an empirical Bayesian prior in the form of a ridge penality, this is done such that the rlog-transformed data are approximately homoskedastic.‚Äù - From the ‚ÄúBeginner‚Äôs guide to using the DESeq2 package‚Äù by Love, Anders and Huber, 2014 (the DESeq2 vignette is the updated version of this doc).\nThe DESeq2 vignette suggests large datasets (100s of samples) to use the variance-stabilizing transformation (vst) instead of rlog for transformation of the counts, since the rlog function might take too long to run and the vst() function is faster with similar properties to rlog.",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "develop/06_exploratory_analysis.html#principal-component-analysis-pca",
    "href": "develop/06_exploratory_analysis.html#principal-component-analysis-pca",
    "title": "Exploratory analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPrincipal Component Analysis (PCA) is a technique used to represent and visualize the variation in a dataset of high dimensionality. The number of dimensions, d, in a dataset may be thought of as the number of variables it has, e.g., for an RNA-seq dataset with 20.000 different transcripts, d is 20.000. Principally, this means we would need a dimensional space of size d to fully represent that dataset. However, as we are only able to view and comprehend things in 1,2 or 3 dimensions, we would like to project this dataset into a lower dimensional space, a process called dimensionality reduction. This makes PCA is a very important technique used in the QC and analysis of both bulk and single-cell RNAseq data, specially because many their dimensions (transcripts) do not contain any information.\nTo better understand how it works, please go through this YouTube video from StatQuest that explains PCA). After you have gone through the video, please proceed with the interpretation section below.\n\nInterpreting PCA plots\nEssentially, if two samples have similar levels of expression for the genes that contribute significantly to the variation represented by a given PC (Principal Component), they will be plotted close together on the axis that represents that PC. Therefore, we would expect that biological replicates to have similar scores (because our expectation is that the same genes are changing) and cluster together. This is easiest to understand by visualizing some example PCA plots.\nWe have an example dataset and a few associated PCA plots below to get a feel for how to interpret them. The metadata for the experiment is displayed below. The main condition of interest is treatment.\n\nWhen visualizing on PC1 and PC2, we don‚Äôt see the samples separate by treatment, so we decide to explore other sources of variation present in the data. We hope that we have included all possible known sources of variation in our metadata table, and we can use these factors to color the PCA plot.\n\n\n\n\n\nWe start with the factor cage, but the cage factor does not seem to explain the variation on PC1 or PC2.\n\n\n\n\n\nThen, we color by the sex factor, which appears to separate samples on PC2. This is good information to take note of, as we can use it downstream to account for the variation due to sex in the model and regress it out.\n\n\n\n\n\nNext we explore the strain factor and find that it explains the variation on PC1.\n\n\n\n\n\nIt‚Äôs great that we have been able to identify the sources of variation for both PC1 and PC2. By accounting for it in our model, we should be able to detect more genes differentially expressed due to treatment.\nSomething worrisome about this plot is that we see two samples that do not cluster with the correct strain. This would indicate a likely sample swap and should be investigated to determine whether these samples are indeed the labeled strains. If we found there was a switch, we could swap the samples in the metadata. However, if we think they are labeled correctly or are unsure, we could just remove the samples from the dataset.\nStill we haven‚Äôt found if treatment is a major source of variation after strain and sex. So, we explore PC3 and PC4 to see if treatment is driving the variation represented by either of these PCs.\n\n\n\n\n\nWe find that the samples separate by treatment on PC3, and are optimistic about our DE analysis since our condition of interest, treatment, is separating on PC3 and we can regress out the variation driving PC1 and PC2.\nDepending on how much variation is explained by the first few principal components, you may want to explore more (i.e consider more components and plot pairwise combinations). Even if your samples do not separate clearly by the experimental variable, you may still get biologically relevant results from the DE analysis. If you are expecting very small effect sizes, then it‚Äôs possible the signal is drowned out by extraneous sources of variation. In situations where you can identify those sources, it is important to account for these in your model, as it provides more power to the tool for detecting DE genes.",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "develop/06_exploratory_analysis.html#hierarchical-clustering-heatmap",
    "href": "develop/06_exploratory_analysis.html#hierarchical-clustering-heatmap",
    "title": "Exploratory analysis",
    "section": "Hierarchical Clustering Heatmap",
    "text": "Hierarchical Clustering Heatmap\nHierarchical clustering is another method for identifying correlation patterns in a dataset and potential sample outliers. A heatmap displays the correlation of gene expression for all pairwise combinations of samples in the dataset. The hierarchical tree along the axes indicates which samples are more similar to each other, i.e.¬†cluster together. The color blocks at the top indicate substructure in the data, and you would expect to see your replicates cluster together as a block for each sample group. Our expectation would be that the samples cluster together similar to the groupings we‚Äôve observed in the PCA plot.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the example above, we see a clustering of wild-type (Wt) and knock-down (KD) cell line samples and we would be quite concerned that the ‚ÄòWt_3‚Äô and ‚ÄòKD_3‚Äô samples are not clustering with the other replicates. Furthermore, since the majority of genes are not differentially expressed, we observe that the samples generally have high correlations with each other (values higher than 0.80). In this case, samples with correlations below 0.80 may indicate an outlier in your data and/or sample contamination. N.B It is important to stress that these is no universal cut-off for what is a good/bad correlation/distance score, it depends on the particular dataset.",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "develop/06_exploratory_analysis.html#vampirium-quality-assessment-and-exploratory-analysis-using-deseq2",
    "href": "develop/06_exploratory_analysis.html#vampirium-quality-assessment-and-exploratory-analysis-using-deseq2",
    "title": "Exploratory analysis",
    "section": "Vampirium quality assessment and exploratory analysis using DESeq2",
    "text": "Vampirium quality assessment and exploratory analysis using DESeq2\nNow that we have a good understanding of the QC steps normally employed for RNA-seq, let‚Äôs implement them for the Vampirium dataset we are going to be working with.\n\nTransform normalized counts for the Vampirium dataset\nTo improve the distances/clustering for the PCA and hierarchical clustering visualization methods, we need to moderate the variance across the mean by applying the rlog transformation to the normalized counts.\n\n\n\n\n\n\nNote on transformed normalized counts\n\n\n\nThe rlog transformation of the normalized counts is only necessary for these visualization methods during this quality assessment. We will not be using these transformed counts for determining differential expression.\n\n\n### Transform counts for data visualization\nrld &lt;- rlog(dds, blind=TRUE)\nThe blind=TRUE argument is to make sure that the rlog() function does not take our sample groups into account - i.e.¬†does the transformation in an unbiased manner. When performing quality assessment, it is important to include this option. The DESeq2 vignette has more details about this.\nThe rlog() function returns a DESeqTransform object, another type of DESeq-specific object. The reason you don‚Äôt just get a matrix of transformed values is because all of the parameters (i.e.¬†size factors) that went into computing the rlog transform are stored in that object. We use this object to plot the PCA and hierarchical clustering figures for quality assessment.\n\n\n\n\n\n\nPerformance of rlog vs vst\n\n\n\nThe rlog() function can be a bit slow when you have e.g.¬†&gt; 20 samples. In these situations the vst() function is much faster and performs a similar transformation appropriate for use with plotPCA(). It‚Äôs typically just a few seconds with vst() due to optimizations and the nature of the transformation.\n### Transform counts for data visualization\nvsd &lt;- vst(dds, blind = TRUE)\n\n\n\n\nPrincipal component analysis (PCA) for the Vampirium dataset\nWe are now ready for the QC steps, let‚Äôs start with PCA!\nDESeq2 has a built-in function for generating PCA plots using ggplot2 under the hood. This is great because it saves us having to type out lines of code and having to fiddle with the different ggplot2 layers. In addition, it takes the rlog object as an input directly, hence saving us the trouble of extracting the relevant information from it.\nThe function plotPCA() requires two arguments as input: a DESeqTransform object and the ‚Äúintgroup‚Äù (interesting group), i.e.¬†the name of the column in our metadata that has information about the experimental sample groups.\n### Plot PCA \nplotPCA(rld, intgroup=\"condition\")\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\nBy default plotPCA() uses the top 500 most variable genes. You can change this by adding the ntop= argument and specifying how many of the genes you want the function to consider. For example, try 1000 genes. Did the plot change a lot?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nUsing the 1000 most variable genes, the plot does not change a lot:\nplotPCA(rld, intgroup=\"condition\", ntop = 1000)\nWhat about 2000 genes? It seems that the PCs have a bit different % variance explained:\nplotPCA(rld, intgroup=\"condition\", ntop = 2000)\nWhat about all genes? Not much change either!\nplotPCA(rld, intgroup=\"condition\", ntop = nrow(rld)) #nrow is number of rows and it equals to all genes!\nAs you can see, most of the info comes from the top most variable genes. Since PCs are capturing the variation of our data, adding genes that are hardly variable makes any difference to the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\n\n\nWhat does the above plot tell you about the similarity of samples?\nDoes it fit the expectation from the experimental design?\nWhat do you think the %variance information (in the axes titles) tell you about the data in the context of the PCA?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nIt shows that our replicates are very close to each other, and each group far from each other!\nYes, which is great!\nIt tells us how much is captured by each PC. In our case, PC1 already captures the differences between our conditions!\n\n\n\n\n\n\n\n\n\n\n\n\nCustom PCA plot\nThe plotPCA() function will only return the values for PC1 and PC2. If you would like to explore the additional PCs in your data or if you would like to identify genes that contribute most to the PCs, you can use the prcomp() function. For example, to plot any of the PCs we could run the following code:\n# Input is a matrix of log transformed values\nrld_mat &lt;- assay(rld) # extract rlog count matrix\npca &lt;- prcomp(t(rld_mat)) # perform PCA on the transposed (t) matrix of data \nTo see what the PCA object contains we can use again the attributes() function.\nattributes(pca)\nYou can check the ?prcomp() for more information. The most important variables are: - sdev: standard deviation explained by each PC. - rotation: contribution of each gene to each PC. - x: PC values for each sample (we use this values for our plots).\nWe can create a new object that contains all our metadata information and the PC values.\ndf &lt;- cbind(meta, pca$x) # Create data frame with metadata and PC3 and PC4 values for input to ggplot\n# ggplot with info for all PCAs\nggplot(df) + geom_point(aes(x=PC3, y=PC4, color = condition))\nIf you want to add PC variation information to the plot we can fetch it using the summary() function and take the second row:\nsummary(pca)\npca_var &lt;- summary(pca)$importance[2,] # second row is stored in the object \"importance\"\npca_var &lt;- round(pca_var * 100, digits = 2) # make it percentage and round to 2 digits\nFinally, we can add it to our plot\nggplot(df) + geom_point(aes(x=PC3, y=PC4, color = condition)) + \n  xlab(paste0(\"PC3: \",pca_var[\"PC3\"], \"% variance\")) + \n  ylab(paste0(\"PC4: \",pca_var[\"PC4\"], \"% variance\")) \nknitr::include_graphics(\"./img/06_exploratory_analysis/custom_PCA.png\")\n\n\n\nHierarchical Clustering for the Vampirium dataset\nThere is no built-in function in DESeq2 for plotting the heatmap for displaying the pairwise correlation or distances between all the samples and the hierarchical clustering information; we will use the pheatmap() function from the pheatmap package. This function cannot use the DESeqTransform object as input, but requires a matrix or dataframe. So, the first thing to do is retrieve that information from the rld object using a function called assay().\n# Extract the rlog matrix from the object\nrld_mat &lt;- assay(rld)    \nNext, we need to compute the distances values for all the samples. We can do this using the dist function:\nsampleDists &lt;- dist(t(rld_mat)) # Distances are computed by rows, so we need to transpose (t) the matrix\nsampleDistMatrix &lt;- as.matrix(sampleDists)\nLet‚Äôs take a look at the column and row names of the correlation matrix.\n# Check the output of sampleDistMatrix, make note of the row names and column names\nhead(sampleDistMatrix)\n\nhead(meta)\nYou will notice that they match the names we have given our samples in the metadata data frame we started with. It is important that these match, so we can use the annotation argument below to plot a color block across the top. This block enables easy visualization of the hierarchical clustering.\nNow, let‚Äôs plot the heatmap!\n# Load pheatmap package\nlibrary(pheatmap)\n\npheatmap(sampleDistMatrix, annotation_col = meta %&gt;% column_to_rownames(\"sample\") %&gt;% \n           select(condition)) # we only want to use the condition column as an annotation\nWhen you plot using pheatmap() the hierarchical clustering information is used to place similar samples together and this information is represented by the tree structure along the axes. The annotation argument accepts a dataframe as input, in our case it is the meta data frame.\n\nOverall, we observe pretty high correlations across the board (&gt; 0.999) suggesting no outlying sample(s). Also, similar to the PCA plot you see the samples clustering together by sample group. Together, these plots suggest to us that the data are of good quality and we have the green light to proceed to differential expression analysis.\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\n\nInstead of using distances between expression patterns, check the Pearson correlation between samples using cor(). Use your rlog count matrix as an input.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nFirst, we get pearson correlations between our samples:\npearson &lt;- cor(rld_mat) \npearson\nAs you can see, the result of cor for a matrix like rld_mat is a square matrix (rows are the same as columns). Each value is the Pearson correlation between a row and a column. Then, we create the plot:\npheatmap(pearson, annotation_col = meta %&gt;% column_to_rownames(\"sample\") %&gt;% \n    dplyr::select(condition)) # we only want to use the condition column as an annotation\n\n\n\n\n\n\n\n\n\n\n\nCustom heatmap\nThere are many arguments and options for the pheatmap() function. You could, for example, change the color scale used, remove the dendograms, avoid clustering or even scale the values per row or per column.\nlibrary(RColorBrewer)\nheat.colors &lt;- brewer.pal(6, \"Blues\") # Colors from the RColorBrewer package (only 6)\nheat.colors &lt;- colorRampPalette(heat.colors)(100) # Interpolate 100 colors\npheatmap(sampleDistMatrix, annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"), \n         color = heat.colors, border_color=NA, fontsize = 10, \n         fontsize_row = 10, height=20)\nYou can check all the colors that RColorBrewer offers by using the following command: display.brewer.all()\ndisplay.brewer.all()\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "develop/05a_data_analysis_setup.html",
    "href": "develop/05a_data_analysis_setup.html",
    "title": "UCloud setup",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 20 minutes\nüí¨ Learning Objectives:\n\nStart a transcriptomics app job in UCloud for the next lessons in data analysis\nGet started on UCloud to perform data analysis",
    "crumbs": [
      "Data analyses",
      "UCloud setup"
    ]
  },
  {
    "objectID": "develop/05a_data_analysis_setup.html#submit-the-job-in-ucloud",
    "href": "develop/05a_data_analysis_setup.html#submit-the-job-in-ucloud",
    "title": "UCloud setup",
    "section": "Submit the job in UCloud",
    "text": "Submit the job in UCloud\nAccess UCloud with your account and choose the project Sandbox RNASeq Workshop where you have been invited.\n\nClick on Apps on the left-side menu, and search for the application Transcriptomics Sandbox and select it:\n\nOnce you are in the Transcriptomics Sandbox, choose appropriate parameters to run a job. Here, you will find a set of recommended values to choose. First, click on Import parameters, then, in the prompt window, choose Import file from UCloud and select the file jobParameters.json which you will find located in the Drive sandbox_bulkRNASeq -&gt; bulk_RNAseq_course -&gt; jobParameters.json\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that the hard-drive icon is labeled ‚Äúsequencing_data‚Äù and you are in the ‚Äúsandbox_bulkRNAseq‚Äù workspace.\nYou can select the drive by clicking on the down arrow (‚à®) icon to open a dropdown menu.\n\n\n\nLet‚Äôs take a look at the parameters we have chosen. We have given it a Job name, Hours, Machine type as well as a Mandatory Parameter Select a module. We have selected the module Introduction to bulk RNAseq analysis in R. This module will load the materials necessary to follow the next lessons. It will also contain a backup of the preprocessing results so that you may continue in case that your preprocessing did not work.\n\nOptional: If you have successfully run nf-core RNA jobs and want to use the files you generated from your own preprocessing results, go to Select folders to use and Add folder by selecting the one containing the pipeline results. They will be located in Member Files:YourName#numbers -&gt; Jobs -&gt; nf-core: rnaseq -&gt; job_name (RNAseq preprocessing ...) -&gt; preprocessing_results_salmon (unless you have moved them elsewhere).\nNow, you are ready to run the app by clicking on the button on the right column of the screen (submit).\nNext, wait until the screen looks like the figure below. This process usually takes a few minutes. You can always come back to this screen via the Runs button in the left menu on UCloud. From there, you can add extra time or stop the app if you no longer need it.\n\nClick on open interface on the top right-hand side of the screen. You will start Rstudio through your browser!\nOn the lower right side of Rstudio, in the file explorer, you should see a folder called Intro_to_bulkRNAseq. Here you will find the materials of the course. If you have added your own preprocessing results, they will also listed here.\n\nYou are ready to start analysing your data!",
    "crumbs": [
      "Data analyses",
      "UCloud setup"
    ]
  },
  {
    "objectID": "develop/05a_data_analysis_setup.html#stopping-the-app",
    "href": "develop/05a_data_analysis_setup.html#stopping-the-app",
    "title": "UCloud setup",
    "section": "Stopping the app",
    "text": "Stopping the app\nWhen you are finished, go to Runs &gt; Running jobs in uCloud and click on the Job name you want to stop if you have several running. It is important to stop the job to prevent it from using unnecessary resources. Here, you can also click on the job and add extra time!",
    "crumbs": [
      "Data analyses",
      "UCloud setup"
    ]
  },
  {
    "objectID": "develop/05a_data_analysis_setup.html#saved-work",
    "href": "develop/05a_data_analysis_setup.html#saved-work",
    "title": "UCloud setup",
    "section": "Saved work",
    "text": "Saved work\nAfter running a first work session, everything that you have created, including the scripts and results of your analysis, will be saved in your own personal ‚ÄúJobs‚Äù folder. Inside this folder there will be a subfolder called Transcriptomics Sandbox, which will contain all the jobs you have run with the Transcriptomics Sandbox app. Inside this folder, you will find your folder named after the job name you gave in the previous step.\n\nYour material will be saved in a volume with your username, that you should be able to see under the menu Files.\n\n\n\nGo to Jobs ‚Üí Transcriptomics Sandbox ‚Üí &lt;Job name&gt; ‚Üí Intro_to_bulkRNAseq",
    "crumbs": [
      "Data analyses",
      "UCloud setup"
    ]
  },
  {
    "objectID": "develop/05a_data_analysis_setup.html#restarting-the-rstudio-session",
    "href": "develop/05a_data_analysis_setup.html#restarting-the-rstudio-session",
    "title": "UCloud setup",
    "section": "Restarting the Rstudio session",
    "text": "Restarting the Rstudio session\nIf you want to keep working on your previous results, you can restart an Rstudio session following these steps:\nClick on Apps on the left-side menu, and look for the application Transcriptomics Sandbox and click on it (recheck the top figures).\nYou will be met again with a series of possible parameters to choose. You have to assign again the Import parameters file as before but now you can either select the JSON file provided by us or click on one of your previous runs and all the selected parameters will be automatically imported (previous runs are shown in the main prompt window).\nsandbox_bulkRNASeq -&gt; bulk_RNAseq_course -&gt; jobParameters.json\nIn ‚ÄúSelect folders to use‚Äù, add the folder with the results of your previous job. First, select:\nMember Files: your_username -&gt; Jobs -&gt; Transcriptomics Sandbox -&gt; &lt;job_name&gt; -&gt; Intro_to_bulkRNAseq\nThen, click Use.\nYou are ready to run the app again by clicking on the button on the right column of the screen (submit). After opening the Rstudio interface, you should be able to access the folder Intro_to_bulkRNAseq, where you will find your course notebooks and results from your previous work!",
    "crumbs": [
      "Data analyses",
      "UCloud setup"
    ]
  },
  {
    "objectID": "develop/05b_count_matrix.html",
    "href": "develop/05b_count_matrix.html",
    "title": "RNAseq count matrix",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 20 minutes\nüí¨ Learning Objectives:\n\nDescribe how to set up an RNA-seq project in R\nDescribe RNA-seq data and the differential gene expression analysis workflow\nLoad and create a count matrix from our preprocessing analysis using Salmon\nExplain why negative binomial distribution is used to model RNA-seq count data\nDifferential gene expression (DGE) analysis overview\nThe goal of RNA-seq is often to perform differential expression testing to determine which genes are expressed at different levels between conditions. These genes can offer biological insight into the processes affected by the condition(s) of interest.\nTo determine the expression levels of genes, our RNA-seq workflow followed the steps detailed in the image below.\nAll steps were performed using the nf-core RNAseq pipeline in our previous lesson. The differential expression analysis and any downstream functional analysis are generally performed in R using R packages specifically designed for the complex statistical analyses required to determine whether genes are differentially expressed.\nIn the next few lessons, we will walk you through an end-to-end gene-level RNA-seq differential expression workflow using various R packages. We will start with the count matrix, do some exploratory data analysis for quality assessment and explore the relationship between samples. Next, we will perform differential expression analysis, and visually explore the results prior to performing downstream functional analysis.",
    "crumbs": [
      "Data analyses",
      "RNAseq count matrix"
    ]
  },
  {
    "objectID": "develop/05b_count_matrix.html#setting-up",
    "href": "develop/05b_count_matrix.html#setting-up",
    "title": "RNAseq count matrix",
    "section": "Setting up",
    "text": "Setting up\nBefore we get into the details of the analysis, let‚Äùs get started by opening up RStudio and setting up a new project for this analysis.\n\nGo to the File menu and select New Project.\nIn the New Project window, choose Existing Directory. Then, choose Intro_to_bulkRNAseq as your project working directory.\n\n\n\nThe new project should automatically open in RStudio.\n\nTo check whether or not you are in the correct working directory, use getwd(). The path /work/Intro_to_bulkRNAseq should be returned to you in the console. When finished your working directory should now look similar to this (check the bottom-right corner):\n\n\nInside the folder Notebooks you will find the scripts (in Rmd format) that we will follow during the sessions.\nIn the folder Results you will save the results of your scripts, analysis and tests.\n\nTo avoid copying the original dataset for each student (very inefficient) a backup of the preprocessing results is inside this folder /work/Intro_to_bulkRNAseq/Data/. You are also very welcome to use your own preprocessing results! Now you can open the first practical session: 05b_count_matrix.Rmd (see the image below).\n\n\nLoading libraries\nFor this analysis we will be using several R packages, some which have been installed from CRAN and others from Bioconductor. To use these packages (and the functions contained within them), we need to load the libraries. Add the following to your script and don‚Äùt forget to comment liberally!\nlibrary(tidyverse)\nlibrary(DESeq2)\nlibrary(tximport)\n\n# And with this last line of code, we set our working directory to the folder with this notebook.\n# This way, the relative paths will work without issues\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\nThe directories of output from the mapping/quantification step of the workflow (Salmon) is the data that we will be using. These transcript abundance estimates, often referred to as ‚Äúpseudocounts‚Äù, will be the starting point for our differential gene expression analysis. The main output of Salmon is a quant.sf file, and we have one of these for each individual sample in our dataset.\nFor the sake of reproducibility, we will be using the backup results from our preprocessing pipeline. You are welcome to use your own results!\n# Tabulated separated files can be opened using the read_table() function.\nread_table(\"/work/Intro_to_bulkRNAseq/Data/salmon/control_1/quant.sf\") %&gt;% head()\nFor each transcript that was assayed in the reference, we have:\n\nThe transcript identifier\nThe transcript length (in bp)\nThe effective length (described in detail below)\nTPM (transcripts per million), which is computed using the effective length\nThe estimated read count (‚Äúpseudocount‚Äù)\n\n\n\n\n\n\n\nWhat exactly is the effective length?\n\n\n\nThe sequence composition of a transcript affects how many reads are sampled from it. While two transcripts might be of identical actual length, depending on the sequence composition we are more likely to generate fragments from one versus the other. The transcript that has a higher likelihood of being sampled, will end up with the larger effective length. The effective length is transcript length which has been ‚Äúcorrected‚Äù to include factors due to sequence-specific and GC biases.\n\n\nWe will be using the R Bioconductor package tximport to prepare the quant.sf files for DESeq2. The first thing we need to do is create a variable that contains the paths to each of our quant.sf files. Then we will add names to our quant files which will allow us to easily distinguish between samples in the final output matrix.\nWe will use the samplesheet.csv file that we use to process our raw reads, since it already contains all the information we need to run our analysis.\n# Load metadata\nmeta &lt;- read_csv(\"/work/Intro_to_bulkRNAseq/Data/samplesheet.csv\")\n\n# View metadata\nmeta\nUsing the samples column, we can create all the paths needed:\n# Directory where salmon files are. You can change this path to the results of your own analysis\ndir &lt;- \"/work/Intro_to_bulkRNAseq/Data\"\n\n# List all directories containing quant.sf files using the samplename column of metadata\nfiles &lt;- file.path(dir,\"salmon\", meta$sample, \"quant.sf\")\n\n# Name the file list with the samplenames\nnames(files) &lt;- meta$sample\nfiles\nOur Salmon files were generated with transcript sequences listed by Ensembl IDs, but tximport needs to know which genes these transcripts came from. We will use annotation table the that was created in our workflow, called tx2gene.txt.\ntx2gene &lt;- read_table(\"/work/Intro_to_bulkRNAseq/Data/salmon/salmon_tx2gene.tsv\", col_names = c(\"transcript_ID\",\"gene_ID\",\"gene_symbol\"))\n\ntx2gene %&gt;% head()\ntx2gene is a three-column data frame linking transcript ID (column 1) to gene ID (column 2) to gene symbol (column 3). We will take the first two columns as input to tximport. The column names are not relevant, but the column order is (i.e transcript ID must be first).\nNow we are ready to run tximport. The tximport() function imports transcript-level estimates from various external software (e.g.¬†Salmon, Kallisto) and summarizes to the gene-level (default) or outputs transcript-level matrices. There are optional arguments to use the abundance estimates as they appear in the quant.sf files or to calculate alternative values.\nFor our analysis we need non-normalized or ‚Äúraw‚Äù count estimates at the gene-level for performing DESeq2 analysis.\nSince the gene-level count matrix is a default (txOut=FALSE) there is only one additional argument for us to modify to specify how to obtain our ‚Äúraw‚Äù count values. The options for countsFromAbundance are as follows:\n\nno (default): This will take the values in TPM (as our scaled values) and NumReads (as our ‚Äúraw‚Äù counts) columns, and collapse it down to the gene-level.\nscaledTPM: This is taking the TPM scaled up to library size as our ‚Äúraw‚Äù counts\nlengthScaledTPM: This is used to generate the ‚Äúraw‚Äù count table from the TPM (rather than summarizing the NumReads column). ‚ÄúRaw‚Äù count values are generated by using the TPM value x featureLength x library size. These represent quantities that are on the same scale as original counts, except no longer correlated with transcript length across samples. We will use this option for DESeq2 downstream analysis.\n\nAn additional argument for tximport: When performing your own analysis you may find that the reference transcriptome file you obtain from Ensembl will have version numbers included on your identifiers (i.e ENSG00000265439.2). This will cause a discrepancy with the tx2gene file since the annotation databases don‚Äùt usually contain version numbers (i.e ENSG00000265439). To get around this issue you can use the argument ignoreTxVersion  = TRUE. The logical value indicates whether to split the tx id on the ‚Äú.‚Äù character to remove version information, for easier matching.\ntxi &lt;- tximport(files, type=\"salmon\", tx2gene=tx2gene, countsFromAbundance = \"lengthScaledTPM\", ignoreTxVersion = TRUE)\n\n\nViewing data\nThe txi object is a simple list containing matrices of the abundance, counts, length. Another list element ‚ÄúcountsFromAbundance‚Äù carries through the character argument used in the tximport call. The length matrix contains the average transcript length for each gene which can be used as an offset for gene-level analysis.\nattributes(txi)\nWe will be using the txi object as is for input into DESeq2, but will save it until the next lesson. For now let‚Äùs take a look at the count matrix. You will notice that there are decimal values, so let‚Äùs round to the nearest whole number and convert it into a dataframe. We will save it to a variable called data that we can play with.\n# Look at the counts\ntxi$counts %&gt;% head()\n# Write the counts to an object\ndata &lt;- txi$counts %&gt;% \n  round() %&gt;% \n  data.frame()\nThere are a lot of rows with no gene expression at all.\nsum(rowSums(data) == 0)\nLet‚Äôs take them out!\nkeep &lt;- rowSums(data) &gt; 0\ndata &lt;- data[keep,]",
    "crumbs": [
      "Data analyses",
      "RNAseq count matrix"
    ]
  },
  {
    "objectID": "develop/05b_count_matrix.html#differential-gene-expression-analysis-overview",
    "href": "develop/05b_count_matrix.html#differential-gene-expression-analysis-overview",
    "title": "RNAseq count matrix",
    "section": "Differential gene expression analysis overview",
    "text": "Differential gene expression analysis overview\nSo, what does this count data actually represent? The count data used for differential expression analysis represents the number of sequence reads that originated from a particular gene. The higher the number of counts, the more reads associated with that gene, and the assumption that there was a higher level of expression of that gene in the sample.\n\nWith differential expression analysis, we are looking for genes that change in expression between two or more groups (defined in the metadata) - case vs control - correlation of expression with some variable or clinical outcome\nWhy does it not work to identify differentially expressed gene by ranking the genes by how different they are between the two groups (based on fold change values)?\n\nGenes that vary in expression level between groups of samples may do so solely as a consequence of the biological variable(s) of interest. However, this difference is often also related to extraneous effects, in fact, sometimes these effects exclusively account for the observed variation. The goal of differential expression analysis to determine the relative role of these effects, hence separating the ‚Äúinteresting‚Äù variance from the ‚Äúuninteresting‚Äù variance.\n\nAlthough the mean expression levels between sample groups may appear to be quite different, it is possible that the difference is not actually significant. This is illustrated for ‚ÄúGeneA‚Äù expression between ‚Äúuntreated‚Äù and ‚Äútreated‚Äù groups in the figure below. The mean expression level of geneA for the ‚Äútreated‚Äù group is twice as large as for the ‚Äúuntreated‚Äù group, but the variation between replicates indicates that this may not be a significant difference. We need to take into account the variation in the data (and where it might be coming from) when determining whether genes are differentially expressed.\n\nDifferential expression analysis is used to determine, for each gene, whether the differences in expression (counts) between groups is significant given the amount of variation observed within groups (replicates). To test for significance, we need an appropriate statistical model that accurately performs normalization (to account for differences in sequencing depth, etc.) and variance modeling (to account for few numbers of replicates and large dynamic expression range).",
    "crumbs": [
      "Data analyses",
      "RNAseq count matrix"
    ]
  },
  {
    "objectID": "develop/05b_count_matrix.html#rna-seq-count-distribution",
    "href": "develop/05b_count_matrix.html#rna-seq-count-distribution",
    "title": "RNAseq count matrix",
    "section": "RNA-seq count distribution",
    "text": "RNA-seq count distribution\nTo determine the appropriate statistical model, we need information about the distribution of counts. To get an idea about how RNA-seq counts are distributed, let‚Äùs plot the counts of all the samples:\n# Here we format the data into long format instead of wide format\npdata &lt;- data %&gt;% \n  gather(key = Sample, value = Count)\n\npdata\nAnd we plot our count distribution using all our samples:\nggplot(pdata) +\n  geom_density(aes(x = Count, color = Sample)) +\n  xlab(\"Raw expression counts\") +\n  ylab(\"Number of genes\")\nIf we zoom in close to zero, we can see a large number of genes with counts close to zero:\nggplot(pdata) +\n  geom_density(aes(x = Count, color = Sample)) +\n  xlim(-5, 500)  +\n  xlab(\"Raw expression counts\") +\n  ylab(\"Number of genes\")\nThese images illustrate some common features of RNA-seq count data, including a low number of counts associated with a large proportion of genes, and a long right tail due to the lack of any upper limit for expression. Unlike microarray data, which has a dynamic range maximum limited due to when the probes max out, there is no limit of maximum expression for RNA-seq data. Due to the differences in these technologies, the statistical models used to fit the data are different between the two methods.\n\n\n\n\n\n\nNote on microarray data distribution\n\n\n\n\n\nThe log intensities of the microarray data approximate a normal distribution. However, due to the different properties of the of RNA-seq count data, such as integer counts instead of continuous measurements and non-normally distributed data, the normal distribution does not accurately model RNA-seq counts. More info here.",
    "crumbs": [
      "Data analyses",
      "RNAseq count matrix"
    ]
  },
  {
    "objectID": "develop/05b_count_matrix.html#modeling-count-data",
    "href": "develop/05b_count_matrix.html#modeling-count-data",
    "title": "RNAseq count matrix",
    "section": "Modeling count data",
    "text": "Modeling count data\nRNAseq count data can be modeled using a Poisson distribution. this particular distribution is fitting for data where the number of cases is very large but the probability of an event occurring is very small. To give you an example, think of the lottery: many people buy lottery tickets (high number of cases), but only very few win (the probability of the event is small).\nWith RNA-Seq data, a very large number of RNAs are represented and the probability of pulling out a particular transcript is very small. Thus, it would be an appropriate situation to use the Poisson distribution. However, a unique property of this distribution is that the mean == variance. Realistically, with RNA-Seq data there is always some biological variation present across the replicates (within a sample class). Genes with larger average expression levels will tend to have larger observed variances across replicates.\nThe model that fits best, given this type of variability observed for replicates, is the Negative Binomial (NB) model. Essentially, the NB model is a good approximation for data where the mean &lt; variance, as is the case with RNA-Seq count data.\n\n\n\n\n\n\n\nNote on technical replicates\n\n\n\n\nBiological replicates represent multiple samples (i.e.¬†RNA from different mice) representing the same sample class\nTechnical replicates represent the same sample (i.e.¬†RNA from the same mouse) but with technical steps replicated\nUsually biological variance is much greater than technical variance, so we do not need to account for technical variance to identify biological differences in expression\nDon‚Äôt spend money on technical replicates - biological replicates are much more useful\n\n\n\n\n\n\n\n\n\nNote on cell lines\n\n\n\nIf you are using cell lines and are unsure whether or not you have prepared biological or technical replicates, take a look at this link. This is a useful resource in helping you determine how best to set up your in-vitro experiment.\n\n\n\n\n\n\n\n\nHow do I know if my data should be modeled using the Poisson distribution or Negative Binomial distribution?\n\n\n\n\n\nIf it‚Äôs count data, it should fit the negative binomial, as discussed previously. However, it can be helpful to plot the mean versus the variance of your data. Remember for the Poisson model, mean = variance, but for NB, mean &lt; variance.\nHere we calculate the mean and the variance per gene for all columns and genes:\ndf &lt;- data %&gt;% \n  rowwise() %&gt;% \n  summarise(mean_counts = mean(c_across(everything())), \n                            variance_counts = var(c_across(everything())))\nRun the following code to plot the mean versus variance of each gene for our data:\nggplot(df) +\n  geom_point(aes(x=mean_counts, y=variance_counts)) + \n  geom_abline(intercept = 0, slope = 1, color=\"red\") +\n  scale_y_log10() +\n  scale_x_log10()\n\nNote that in the above figure, the variance across replicates tends to be greater than the mean (red line), especially for genes with large mean expression levels. This is a good indication that our data do not fit the Poisson distribution and we need to account for this increase in variance using the Negative Binomial model (i.e.¬†Poisson will underestimate variability leading to an increase in false positive DE genes).",
    "crumbs": [
      "Data analyses",
      "RNAseq count matrix"
    ]
  },
  {
    "objectID": "develop/05b_count_matrix.html#improving-mean-estimates-i.e.-reducing-variance-with-biological-replicates",
    "href": "develop/05b_count_matrix.html#improving-mean-estimates-i.e.-reducing-variance-with-biological-replicates",
    "title": "RNAseq count matrix",
    "section": "Improving mean estimates (i.e.¬†reducing variance) with biological replicates",
    "text": "Improving mean estimates (i.e.¬†reducing variance) with biological replicates\nThe variance or scatter tends to reduce as we increase the number of biological replicates (the distribution will approach the Poisson distribution with increasing numbers of replicates), since standard deviations of averages are smaller than standard deviations of individual observations. The value of additional replicates is that as you add more data (replicates), you get increasingly precise estimates of group means, and ultimately greater confidence in the ability to distinguish differences between sample classes (i.e.¬†more DE genes).\nThe figure below illustrates the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified (from Liu et al.¬†(2013)):\n\nNote that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth. Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. Generally, the minimum sequencing depth recommended is 20-30 million reads per sample, but we have seen good RNA-seq experiments with 10 million reads if there are a good number of replicates.",
    "crumbs": [
      "Data analyses",
      "RNAseq count matrix"
    ]
  },
  {
    "objectID": "develop/05b_count_matrix.html#differential-expression-analysis-workflow",
    "href": "develop/05b_count_matrix.html#differential-expression-analysis-workflow",
    "title": "RNAseq count matrix",
    "section": "Differential expression analysis workflow",
    "text": "Differential expression analysis workflow\nTo model counts appropriately when performing a differential expression analysis, there are a number of software packages that have been developed for differential expression analysis of RNA-seq data. Even as new methods are continuously being developed a few tools are generally recommended as best practice, like DESeq2, EdgeR and Limma-Voom.\nMany studies describing comparisons between these methods show that while there is some agreement, there is also much variability between tools. Additionally, there is no one method that performs optimally under all conditions (Soneson and Dleorenzi, 2013, Corchete et al, 2020).\n\nWe will be using DESeq2 for the DE analysis, and the analysis steps with DESeq2 are shown in the flowchart below in green. DESeq2 first normalizes the count data to account for differences in library sizes and RNA composition between samples. Then, we will use the normalized counts to make some plots for QC at the gene and sample level. The final step is to use the appropriate functions from the DESeq2 package to perform the differential expression analysis.\n\nWe will go in-depth into each of these steps in the following lessons, but additional details and helpful suggestions regarding DESeq2 can be found in the DESeq2 vignette. As you go through this workflow and questions arise, you can reference the vignette from within RStudio:\nvignette(\"DESeq2\")\nThis is very convenient, as it provides a wealth of information at your fingertips! Be sure to use this as you need during the workshop.\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Data analyses",
      "RNAseq count matrix"
    ]
  },
  {
    "objectID": "develop/08b_FA_overrepresentation.html",
    "href": "develop/08b_FA_overrepresentation.html",
    "title": "Functional Analysis",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 120 minutes\nüí¨ Learning Objectives:\n\nDetermine how functions are attributed to genes using Gene Ontology terms\nDescribe the theory of how functional enrichment tools yield statistically enriched functions or interactions\nDiscuss functional analysis using over-representation analysis, functional class scoring, and pathway topology methods\nIdentify popular functional analysis tools for over-representation analysis\nThe output of RNA-seq differential expression analysis is a list of significant differentially expressed genes (DEGs). To gain greater biological insight on the differentially expressed genes there are various analyses that can be done:\nGenerally for any differential expression analysis, it is useful to interpret the resulting gene lists using freely available web- and R-based tools. While tools for functional analysis span a wide variety of techniques, they can loosely be categorized into three main types: over-representation analysis, functional class scoring, and pathway topology. See more here.\nThe goal of functional analysis is to provide biological insight, so it‚Äôs necessary to analyze our results in the context of our experimental hypothesis: What is the function of the genes dysregulated by Vampirium?. Therefore, based on the authors‚Äô hypothesis and observations, we may expect the enrichment of processes/pathways related to blood production and behaviour control, which we would need to validate experimentally.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Functional Analysis"
    ]
  },
  {
    "objectID": "develop/08b_FA_overrepresentation.html#over-representation-analysis",
    "href": "develop/08b_FA_overrepresentation.html#over-representation-analysis",
    "title": "Functional Analysis",
    "section": "Over-representation analysis",
    "text": "Over-representation analysis\nThere are a plethora of functional enrichment tools that perform some type of ‚Äúover-representation‚Äù analysis by querying databases containing information about gene function and interactions.\nThese databases typically categorize genes into groups (gene sets) based on shared function, or involvement in a pathway, or presence in a specific cellular location, or other categorizations, e.g.¬†functional pathways, etc. Essentially, known genes are binned into categories that have been consistently named (controlled vocabulary) based on how the gene has been annotated functionally. These categories are independent of any organism, however each organism has distinct categorizations available.\nTo determine whether any categories are over-represented, you can determine the probability of having the observed proportion of genes associated with a specific category in your gene list based on the proportion of genes associated with the same category in the background set (gene categorizations for the appropriate organism). \n\n\n\n\n\nThe statistical test that will determine whether something is actually over-represented is the Hypergeometric test.\n\nHypergeometric testing\nUsing the example of the first functional category above, hypergeometric distribution is a probability distribution that describes the probability of 25 genes (k) being associated with ‚ÄúFunctional category 1‚Äù, for all genes in our gene list (n=1000), from a population of all of the genes in entire genome (N=23,000) which contains 35 genes (K) associated with ‚ÄúFunctional category 1‚Äù [4].\n\n\n\n\n\nThe calculation of probability of k successes follows the formula:\n\\[Pr(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n-k}}{\\binom{N}{n}}\\]\n\n\n\n\n\nThis test will result in an adjusted p-value (after multiple test correction) for each category tested.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Functional Analysis"
    ]
  },
  {
    "objectID": "develop/08b_FA_overrepresentation.html#gene-ontology-project",
    "href": "develop/08b_FA_overrepresentation.html#gene-ontology-project",
    "title": "Functional Analysis",
    "section": "Gene Ontology project",
    "text": "Gene Ontology project\nOne of the most widely-used categorizations is the Gene Ontology (GO) established by the Gene Ontology project.\n\n\n\n\n\n\ndefinition\n\n\n\n\n\n\n\n‚ÄúThe Gene Ontology project is a collaborative effort to address the need for consistent descriptions of gene products across databases‚Äù.\n\n\n\n\n\nThe Gene Ontology Consortium maintains the GO terms, and these GO terms are incorporated into gene annotations in many of the popular repositories for animal, plant, and microbial genomes.\nTools that investigate enrichment of biological functions or interactions often use the Gene Ontology (GO) categorizations, i.e.¬†the GO terms to determine whether any have significantly modified representation in a given list of genes. Therefore, to best use and interpret the results from these functional analysis tools, it is helpful to have a good understanding of the GO terms themselves and their organization.\n\nGO Ontologies\nTo describe the roles of genes and gene products, GO terms are organized into three independent controlled vocabularies (ontologies) in a species-independent manner:\n\nBiological process: refers to the biological role involving the gene or gene product, and could include ‚Äútranscription‚Äù, ‚Äúsignal transduction‚Äù, and ‚Äúapoptosis‚Äù. A biological process generally involves a chemical or physical change of the starting material or input.\nMolecular function: represents the biochemical activity of the gene product, such activities could include ‚Äúligand‚Äù, ‚ÄúGTPase‚Äù, and ‚Äútransporter‚Äù.\nCellular component: refers to the location in the cell of the gene product. Cellular components could include ‚Äúnucleus‚Äù, ‚Äúlysosome‚Äù, and ‚Äúplasma membrane‚Äù.\n\nEach GO term has a term name (e.g.¬†DNA repair) and a unique term accession number (GO:0005125), and a single gene product can be associated with many GO terms, since a single gene product ‚Äúmay function in several processes, contain domains that carry out diverse molecular functions, and participate in multiple alternative interactions with other proteins, organelles or locations in the cell‚Äù. See more here.\n\n\nGO term hierarchy\nSome gene products are well-researched, with vast quantities of data available regarding their biological processes and functions. However, other gene products have very little data available about their roles in the cell.\nFor example, the protein, ‚Äúp53‚Äù, would contain a wealth of information on it‚Äôs roles in the cell, whereas another protein might only be known as a ‚Äúmembrane-bound protein‚Äù with no other information available.\nThe GO ontologies were developed to describe and query biological knowledge with differing levels of information available. To do this, GO ontologies are loosely hierarchical, ranging from general, ‚Äòparent‚Äô, terms to more specific, ‚Äòchild‚Äô terms. The GO ontologies are ‚Äúloosely‚Äù hierarchical since ‚Äòchild‚Äô terms can have multiple ‚Äòparent‚Äô terms.\nSome genes with less information may only be associated with general ‚Äòparent‚Äô terms or no terms at all, while other genes with a lot of information be associated with many terms.\n\n\n\nFrom Nature Reviews Cancer 7, 23-34 (January 2007)\n\n\n\n\n\n\n\n\nTip\n\n\n\nMore tips for working with GO can be found here",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Functional Analysis"
    ]
  },
  {
    "objectID": "develop/08b_FA_overrepresentation.html#clusterprofiler",
    "href": "develop/08b_FA_overrepresentation.html#clusterprofiler",
    "title": "Functional Analysis",
    "section": "clusterProfiler",
    "text": "clusterProfiler\nWe will be using clusterProfiler to perform over-representation analysis on GO terms associated with our list of significant genes. The tool takes as input a significant gene list and a background gene list and performs statistical enrichment analysis using hypergeometric testing. The basic arguments allow the user to select the appropriate organism and GO ontology (BP, CC, MF) to test.\n\nRunning clusterProfiler\nTo run clusterProfiler GO over-representation analysis, we will change our gene names into Ensembl IDs, since the tool works a bit easier with the Ensembl IDs. Then load the following libraries:\n# Load libraries\nlibrary(DOSE)\nlibrary(pathview)\nlibrary(clusterProfiler)\nlibrary(org.Hs.eg.db)\nTo perform the over-representation analysis, we need a list of background genes and a list of significant genes. For our background dataset we will use all genes tested for differential expression (all genes in our results table). For our significant gene list we will use genes with p-adjusted values less than 0.05 (we could include a fold change threshold too if we have many DE genes).\n## Create background dataset for hypergeometric testing using all genes tested for significance in the results\nallCont_genes &lt;- dplyr::filter(res_ids, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n## Extract significant results\nsigCont &lt;- dplyr::filter(res_ids, padj &lt; 0.05 & !is.na(gene))\n\nsigCont_genes &lt;- sigCont %&gt;% \n  pull(gene) %&gt;% \n  as.character()\nNow we can perform the GO enrichment analysis and save the results:\n## Run GO enrichment analysis \nego &lt;- enrichGO(gene = sigCont_genes, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n\n\n\n\n\n\nNote\n\n\n\nThe different organisms with annotation databases available to use with for the OrgDb argument can be found here.\nAlso, the keyType argument may be coded as keytype in different versions of clusterProfiler.\nFinally, the ont argument can accept either ‚ÄúBP‚Äù (Biological Process), ‚ÄúMF‚Äù (Molecular Function), and ‚ÄúCC‚Äù (Cellular Component) subontologies, or ‚ÄúALL‚Äù for all three.\n\n\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\ncluster_summary\n\nwrite.csv(cluster_summary, \"../Results/clusterProfiler_Cont-Vamp.csv\")\n\n\n\n\n\n\n\n\n\nID\nDescription\nGeneRatio\nBgRatio\npvalue\np.adjust\nqvalue\ngeneID\nCount\n\n\n\n\nGO:0002449\nlymphocyte mediated immunity\n259/3662\n436/15509\n3.554730e-59\n2.203222e-55\n1.324979e-55\nBTK/HFE/CD74/SLAMF7/TNFRSF1B/PARP3/FOXP3/FCGR2B/TBX21/MLH1/IL4R/PTPRC/P2RX7/ICAM1/MSH2/IL12RB1/GZMB/CD40/SMAD7/TLR8/CD40LG/IL21R/NBN/FCER2/LILRB1/IL27RA/CLC/TGFB1/AHR/EXOSC3/GATA3/PPP3CB/C1QBP/NSD2/CTSC/HPX/CD81/AICDA/IL12B/IL4/BCL6/CD160/PRDX1/ARG1/CLU/SHLD2/IL1B/CD70/C3/FOXJ1/NECTIN2/ULBP2/NDFIP1/ARL8B/IL6/IL10/IL21/SLC15A4/ARRB2/LYST/IL18/MR1/CD96/CD8A/CD1A/CD1C/CD1B/TNFSF13/PRKCD/MBL2/NOD2/KIR3DL1/FADD/INPP5D/LGALS9/SERPINB9/KIF5B/STAT5B/LIG4/LEP/CD19/CD28/HLA-DQB1/PRF1/MICA/BTN3A2/LILRB4/CARD9/ZP3/KIR2DL4/HMGB1/TUBB/CD55/HLA-DQA1/CR1/HLA-DOA/TAP2/TNF/HLA-E/HLA-G/TAP2/HLA-DQB1/TAP2/HLA-DQA1/HLA-DQB1/HLA-DQA1/HLA-DRA/TNF/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/HLA-DPB1/TAP2/HLA-B/HLA-DQA1/NCR3/HLA-DPB1/LTA/TNF/HLA-A/HLA-A/HLA-B/HLA-DQA1/HLA-E/NCR3/HLA-DQB1/HLA-DQA1/TAP2/LTA/HLA-DPB1/HLA-A/HLA-DRA/HLA-DQA1/TNF/TAP2/TNF/HLA-B/TNF/HLA-DRA/HLA-A/HLA-E/HLA-DPB1/TNF/HLA-E/LTA/HLA-G/HLA-DPB1/HLA-DPB1/HLA-DRB4/MICA/HLA-DQB1/HLA-DPA1/LTA/HLA-DQA1/HLA-A/HLA-DQB1/HLA-DQA1/HLA-B/TAP2/TNF/MICA/HLA-G/HLA-DQA1/HLA-DQB1/HLA-E/HLA-DRA/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-DQA1/HLA-E/HLA-DPB1/NCR3/NCR3/HLA-G/HLA-DQA1/TAP2/HLA-DPB1/NCR3/LTA/HLA-DQA1/PTPRC/KIR2DL4/B2M/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/LILRB4/KIR2DL4/HLA-G/KIR3DL1/KIR3DL1/KIR3DL1/LILRB1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/LILRB4/KIR3DL1/KIR2DL4/LILRB4/KIR2DL4/KIR3DL1/INPP5D/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1\n259\n\n\nGO:0001906\ncell killing\n180/3662\n281/15509\n6.369002e-48\n1.973754e-44\n1.186981e-44\nTYROBP/LTF/SLAMF7/SPI1/FCGR2B/STXBP2/PTPRC/CD59/P2RX7/ICAM1/LYZ/HSP90AB1/IL12RB1/CTSG/GZMB/FCER2/LILRB1/HAMP/MAPK8/PPP3CB/CTSC/IFNG/IL12B/IL4/GNLY/CD160/PRDX1/ARG1/SEMG1/H2BC11/C3/NECTIN2/ULBP2/ARL8B/IL21/ARRB2/LYST/IL18/MR1/CD1A/CD1C/CD1B/SYK/KIR3DL1/FADD/CX3CR1/LGALS9/ITGAM/SERPINB9/KIF5B/BCL2L1/STAT5B/LEP/F2/PRF1/MICA/KIR2DL4/TUBB/CD55/ELANE/HMGN2/TAP2/HLA-E/HLA-G/TAP2/TAP2/HLA-DRA/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/TAP2/HLA-B/NCR3/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/TAP2/HLA-A/HLA-DRA/TAP2/HLA-B/HLA-DRA/HLA-A/HLA-E/HLA-E/HLA-G/MICA/HLA-A/HLA-B/TAP2/MICA/HLA-G/HLA-E/HLA-DRA/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/TAP2/NCR3/PTPRC/KIR2DL4/B2M/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/HLA-G/KIR3DL1/KIR3DL1/KIR3DL1/LILRB1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/ELANE/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1\n180\n\n\nGO:1903039\npositive regulation of leukocyte cell-cell adhesion\n196/3662\n320/15509\n1.026091e-47\n2.119904e-44\n1.274873e-44\nBAD/SELE/CD4/IGF1/CD74/RUNX3/CD44/ARID1B/FOXP3/AP3D1/RHOA/CBFB/GLI2/ACTB/IL4R/PTPRC/IL12RB1/ABL1/SMARCB1/XBP1/CD40LG/NFAT5/PYCARD/CSK/IL7/LILRB1/IL27RA/JAK3/GATA3/MAP3K8/ZMIZ1/SMARCD2/CCL2/EFNB3/ZBTB16/CD5/MDK/CD81/IFNG/IL12B/IL4/BCL6/CD86/HES1/IL1A/ZAP70/ITGA4/IGFBP2/CD160/ARID1A/MYB/FOXO3/NR4A3/GPAM/CD80/SOX4/IL1B/TNFSF9/CD70/CCR7/EPO/RARA/FLOT2/AP3B1/PTPN22/IL2RA/IL6/HLX/CCL21/IL21/SLC7A1/ABL2/DUSP10/IL2RG/IL18/RUNX1/TNFRSF13C/ICOSLG/ITGB2/VCAM1/TGFBR2/SPTA1/SHH/SYK/BRD7/RAG1/NOD2/FADD/RHOH/LGALS9/PTAFR/CCL19/KAT5/RELA/STAT5B/SELP/LEP/CD28/PTPN11/HLA-DQB1/KLHL25/IRAK1/SOCS1/LILRB4/ZP3/ARID2/HMGB1/CD55/FUT4/HLA-DQA1/CD47/SRC/ELANE/DPP4/PDCD1LG2/PNP/CR1/HLA-DOA/TNF/HLA-E/HLA-G/HLA-DQB1/HLA-DQA1/HLA-DQB1/HLA-DQA1/HLA-DRA/TNF/HLA-E/HLA-A/HLA-A/HLA-G/HLA-DPB1/HLA-DQA1/HLA-DPB1/TNF/HLA-A/HLA-A/HLA-DQA1/HLA-E/HLA-DQB1/HLA-DQA1/HLA-DPB1/HLA-A/HLA-DRA/HLA-DQA1/TNF/TNF/TNF/HLA-DRA/HLA-A/HLA-E/HLA-DPB1/TNF/HLA-E/HLA-G/HLA-DPB1/HLA-DPB1/HLA-DRB4/HLA-DQB1/HLA-DPA1/HLA-DQA1/HLA-A/HLA-DQB1/HLA-DQA1/TNF/HLA-G/HLA-DQA1/HLA-DQB1/HLA-E/HLA-DRA/HLA-G/HLA-A/HLA-G/HLA-DQA1/HLA-E/HLA-DPB1/HLA-G/HLA-DQA1/HLA-DPB1/HLA-DQA1/PTPRC/CCL5/CD24/B2M/CCL5/LILRB1/SMARCB1/LILRB4/HLA-G/LILRB1/LILRB1/HLA-DRA/ELANE/LILRB1/LILRB4/LILRB4\n196\n\n\nGO:0001909\nleukocyte mediated cytotoxicity\n163/3662\n243/15509\n1.806781e-47\n2.799607e-44\n1.683634e-44\nTYROBP/SLAMF7/SPI1/FCGR2B/STXBP2/PTPRC/P2RX7/ICAM1/IL12RB1/CTSG/GZMB/LILRB1/PPP3CB/CTSC/IL12B/CD160/PRDX1/ARG1/NECTIN2/ULBP2/ARL8B/IL21/ARRB2/LYST/IL18/MR1/CD1A/CD1C/CD1B/KIR3DL1/FADD/CX3CR1/LGALS9/ITGAM/SERPINB9/KIF5B/STAT5B/LEP/F2/PRF1/MICA/KIR2DL4/TUBB/ELANE/TAP2/HLA-E/HLA-G/TAP2/TAP2/HLA-DRA/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/TAP2/HLA-B/NCR3/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/TAP2/HLA-A/HLA-DRA/TAP2/HLA-B/HLA-DRA/HLA-A/HLA-E/HLA-E/HLA-G/MICA/HLA-A/HLA-B/TAP2/MICA/HLA-G/HLA-E/HLA-DRA/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/TAP2/NCR3/PTPRC/KIR2DL4/B2M/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/HLA-G/KIR3DL1/KIR3DL1/KIR3DL1/LILRB1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/ELANE/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1\n163\n\n\nGO:0022409\npositive regulation of cell-cell adhesion\n214/3662\n367/15509\n4.544981e-47\n5.633959e-44\n3.388164e-44\nBAD/SELE/CD4/IGF1/CD74/RUNX3/CD44/ARID1B/FOXP3/PTPRU/AP3D1/RHOA/CBFB/GLI2/ACTB/IL4R/PTPRC/CEACAM6/IL12RB1/ABL1/SMARCB1/XBP1/CTSG/BMP7/SMAD7/CD40LG/NFAT5/PIEZO1/PYCARD/CSK/TJP1/IL7/LILRB1/IL27RA/JAK3/GATA3/MAP3K8/ZMIZ1/SMARCD2/CCL2/EFNB3/ZBTB16/CD5/MDK/CD81/IFNG/IL12B/IL4/BCL6/CD86/WNT5A/HES1/IL1A/ZAP70/ITGA4/IGFBP2/CD160/ARID1A/MYB/FOXO3/NR4A3/GPAM/CD80/SOX4/IL1B/TNFSF9/CD70/CCR7/PODXL/EPO/RARA/FLOT2/AP3B1/PTPN22/IL2RA/ADAM19/IL6/HLX/IL10/CCL21/IL21/SLC7A1/ABL2/DUSP10/IL2RG/IL18/CXCL13/NODAL/DMTN/RUNX1/TNFRSF13C/ICOSLG/ITGB2/JAK1/VCAM1/TGFBR2/SPTA1/SHH/SYK/BRD7/RAG1/NOD2/FADD/RHOH/LGALS9/PTAFR/FGG/FGA/CCL19/KAT5/RELA/STAT5B/SELP/LEP/CD28/PTPN11/HLA-DQB1/KLHL25/IRAK1/SOCS1/LILRB4/ZP3/ARID2/HMGB1/CD55/FUT4/HLA-DQA1/CD47/SRC/ELANE/DPP4/PDCD1LG2/PNP/CR1/HLA-DOA/TNF/HLA-E/HLA-G/HLA-DQB1/HLA-DQA1/HLA-DQB1/HLA-DQA1/HLA-DRA/TNF/HLA-E/HLA-A/HLA-A/HLA-G/HLA-DPB1/HLA-DQA1/HLA-DPB1/TNF/HLA-A/HLA-A/HLA-DQA1/HLA-E/HLA-DQB1/HLA-DQA1/HLA-DPB1/HLA-A/HLA-DRA/HLA-DQA1/TNF/TNF/TNF/HLA-DRA/HLA-A/HLA-E/HLA-DPB1/TNF/HLA-E/HLA-G/HLA-DPB1/HLA-DPB1/HLA-DRB4/HLA-DQB1/HLA-DPA1/HLA-DQA1/HLA-A/HLA-DQB1/HLA-DQA1/TNF/HLA-G/HLA-DQA1/HLA-DQB1/HLA-E/HLA-DRA/HLA-G/HLA-A/HLA-G/HLA-DQA1/HLA-E/HLA-DPB1/HLA-G/HLA-DQA1/HLA-DPB1/HLA-DQA1/PTPRC/CCL5/CD24/B2M/CCL5/LILRB1/SMARCB1/LILRB4/HLA-G/LILRB1/LILRB1/HLA-DRA/TJP1/ELANE/LILRB1/LILRB4/LILRB4\n214\n\n\nGO:0002706\nregulation of lymphocyte mediated immunity\n172/3662\n268/15509\n5.399968e-46\n5.578166e-43\n3.354611e-43\nBTK/HFE/TNFRSF1B/PARP3/FOXP3/FCGR2B/TBX21/MLH1/PTPRC/P2RX7/MSH2/IL12RB1/CD40/SMAD7/FCER2/LILRB1/IL27RA/CLC/TGFB1/AHR/EXOSC3/GATA3/PPP3CB/NSD2/HPX/CD81/IL12B/IL4/BCL6/CD160/ARG1/SHLD2/IL1B/C3/FOXJ1/NECTIN2/NDFIP1/IL6/IL10/IL21/SLC15A4/ARRB2/IL18/MR1/CD96/CD1A/CD1C/CD1B/TNFSF13/NOD2/FADD/LGALS9/SERPINB9/STAT5B/LEP/CD28/MICA/LILRB4/ZP3/KIR2DL4/HMGB1/CD55/CR1/TAP2/TNF/HLA-E/HLA-G/TAP2/TAP2/HLA-DRA/TNF/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/TAP2/HLA-B/NCR3/LTA/TNF/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/TAP2/LTA/HLA-A/HLA-DRA/TNF/TAP2/TNF/HLA-B/TNF/HLA-DRA/HLA-A/HLA-E/TNF/HLA-E/LTA/HLA-G/MICA/LTA/HLA-A/HLA-B/TAP2/TNF/MICA/HLA-G/HLA-E/HLA-DRA/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/TAP2/NCR3/LTA/PTPRC/KIR2DL4/B2M/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/HLA-G/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/LILRB4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4\n172\n\n\nGO:0002703\nregulation of leukocyte mediated immunity\n196/3662\n327/15509\n1.129551e-45\n1.000137e-42\n6.014646e-43\nBTK/HFE/TYROBP/TNFRSF1B/GAB2/PARP3/C12orf4/FOXP3/SPI1/FCGR2B/TBX21/MLH1/STXBP2/IL4R/DDX1/PTPRC/MAVS/P2RX7/ICAM1/MSH2/IL12RB1/CD40/SMAD7/GATA1/FCER2/LILRB1/IL27RA/CLC/TGFB1/JAK3/AHR/EXOSC3/GATA3/PPP3CB/NSD2/HPX/CD81/IL12B/IL4/BCL6/SNX4/CD160/ARG1/VAMP8/SHLD2/IL1B/C3/RAC2/FOXJ1/NECTIN2/NDFIP1/IL6/IL10/STXBP1/TLR4/IL21/SLC15A4/ARRB2/IL18/MR1/CD96/KIT/CD1A/CD1C/CD1B/ITGB2/TNFSF13/TLR3/SYK/NOD2/FADD/CX3CR1/LGALS9/PTAFR/ITGAM/SERPINB9/STAT5B/LEP/CD28/MICA/LILRB4/ZP3/KIR2DL4/HMGB1/CD55/CR1/TAP2/TNF/HLA-E/HLA-G/CD177/TAP2/TAP2/HLA-DRA/TNF/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/TAP2/HLA-B/NCR3/LTA/TNF/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/TAP2/LTA/HLA-A/HLA-DRA/TNF/TAP2/TNF/HLA-B/TNF/HLA-DRA/HLA-A/HLA-E/TNF/HLA-E/LTA/HLA-G/MICA/LTA/HLA-A/HLA-B/TAP2/TNF/MICA/HLA-G/HLA-E/HLA-DRA/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/TAP2/NCR3/LTA/PTPRC/KIR2DL4/B2M/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/HLA-G/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/LILRB4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4\n196\n\n\nGO:0002440\nproduction of molecular mediator of immune response\n198/3662\n336/15509\n1.491876e-44\n1.042108e-41\n6.267055e-42\nBTK/HFE/CD22/CD74/TNFRSF1B/PARP3/FOXP3/TCF3/FCGR2B/TBX21/MLH1/IL4R/DDX1/PTPRC/MAVS/P2RX7/MSH2/XBP1/CD40/SMAD7/CD40LG/ACP5/PYCARD/NBN/LILRB1/IL27RA/CLC/GPI/TGFB1/JAK3/NOD1/EXOSC3/GATA3/NSD2/HPX/CD81/AICDA/IL17A/IL4/IL5/BCL6/CD86/WNT5A/CD160/ARG1/NR4A3/CD244/SHLD2/PKN1/IL1B/NDFIP1/EPHB2/PTPN22/CD36/IL6/IL10/TLR4/IL21/SLC15A4/TMBIM6/IL18/CD96/KIT/NLRX1/TNFSF13/MAPKAPK2/TLR3/SYK/NOD2/LIG4/CD28/HLA-DQB1/LACC1/PRG2/LILRB4/CARD9/KIR2DL4/CD55/TLR7/HLA-DQA1/ELANE/UBE2J1/CR1/HLA-DOA/TNF/HLA-E/HLA-G/HLA-DQB1/HLA-DQA1/HLA-DQB1/HLA-DQA1/HLA-DRA/TNF/HLA-E/HLA-A/HLA-A/HLA-G/HLA-DPB1/HLA-DQA1/HLA-DPB1/TNF/HLA-A/HLA-A/HLA-DQA1/HLA-E/HLA-DQB1/HLA-DQA1/HLA-DPB1/HLA-A/HLA-DRA/HLA-DQA1/TNF/TNF/TNF/HLA-DRA/HLA-A/HLA-E/HLA-DPB1/TNF/HLA-E/HLA-G/HLA-DPB1/HLA-DPB1/HLA-DRB4/HLA-DQB1/HLA-DPA1/HLA-DQA1/HLA-A/HLA-DQB1/HLA-DQA1/TNF/HLA-G/HLA-DQA1/HLA-DQB1/HLA-E/HLA-DRA/HLA-G/HLA-A/HLA-G/HLA-DQA1/HLA-E/HLA-DPB1/HLA-G/HLA-DQA1/HLA-DPB1/PRKDC/HLA-DQA1/PTPRC/KIR2DL4/B2M/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/HLA-G/LILRB1/MIF/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/ELANE/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/LILRB4/KIR2DL4/GPI/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4\n198\n\n\nGO:0002697\nregulation of immune effector process\n240/3662\n445/15509\n1.513226e-44\n1.042108e-41\n6.267055e-42\nBTK/HFE/TYROBP/CD22/CD74/TNFRSF1B/GRN/GAB2/PARP3/C12orf4/FOXP3/SPI1/FCGR2B/TBX21/MLH1/STXBP2/IL4R/DDX1/PTPRC/CD59/MAVS/P2RX7/ICAM1/MSH2/IL12RB1/XBP1/CD40/SMAD7/GATA1/CD40LG/ACP5/PYCARD/FCER2/LILRB1/IL27RA/CLC/GPI/TGFB1/JAK3/NOD1/AHR/EXOSC3/GATA3/PPP3CB/C1QBP/NSD2/HPX/CD81/IFNG/IL17A/IL12B/IL4/IL5/BCL6/CD86/WNT5A/SNX4/CD160/MYB/ARG1/VAMP8/NR4A3/CD80/CD244/SHLD2/PKN1/IL1B/C3/RAC2/FOXJ1/NECTIN2/NDFIP1/RARA/EPHB2/PTPN22/CD36/IL6/HLX/IL10/STXBP1/TLR4/IRF4/DDX60/IL21/SLC15A4/TMBIM6/ARRB2/DUSP10/IL18/MR1/CD96/KIT/APPL1/CD1A/CD1C/CD1B/ITGB2/NLRX1/TNFSF13/MAPKAPK2/TLR3/SYK/MBL2/NOD2/FADD/CX3CR1/LGALS9/PTAFR/ITGAM/SERPINB9/CCL19/STAT5B/LEP/A2M/CD28/LACC1/MICA/PRG2/LILRB4/CARD9/ZP3/KIR2DL4/HMGB1/CD55/TLR7/CD47/UBE2J1/CR1/TAP2/TNF/HLA-E/HLA-G/CD177/TAP2/TAP2/HLA-DRA/TNF/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/TAP2/HLA-B/NCR3/LTA/TNF/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/TAP2/LTA/HLA-A/HLA-DRA/TNF/TAP2/TNF/HLA-B/TNF/HLA-DRA/HLA-A/HLA-E/TNF/HLA-E/LTA/HLA-G/MICA/LTA/HLA-A/HLA-B/TAP2/TNF/MICA/HLA-G/HLA-E/HLA-DRA/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/TAP2/NCR3/LTA/PTPRC/KIR2DL4/B2M/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/HLA-G/LILRB1/MIF/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB4/KIR2DL4/LILRB4/KIR2DL4/GPI/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4\n240\n\n\nGO:0002699\npositive regulation of immune effector process\n195/3662\n332/15509\n1.370637e-43\n8.495209e-41\n5.108870e-41\nBTK/TYROBP/CD74/GAB2/C12orf4/FOXP3/SPI1/TBX21/MLH1/STXBP2/IL4R/DDX1/PTPRC/MAVS/P2RX7/MSH2/IL12RB1/XBP1/CD40/GATA1/PYCARD/FCER2/LILRB1/GPI/TGFB1/NOD1/EXOSC3/GATA3/NSD2/HPX/CD81/IFNG/IL17A/IL12B/IL4/IL5/CD86/WNT5A/SNX4/CD160/MYB/ARG1/VAMP8/NR4A3/CD80/CD244/SHLD2/IL1B/C3/RAC2/NECTIN2/RARA/EPHB2/PTPN22/CD36/IL6/HLX/IL10/STXBP1/TLR4/DDX60/IL21/IL18/MR1/KIT/CD1A/CD1C/CD1B/ITGB2/TNFSF13/MAPKAPK2/TLR3/SYK/MBL2/NOD2/FADD/LGALS9/PTAFR/ITGAM/CCL19/STAT5B/CD28/LACC1/CARD9/ZP3/KIR2DL4/CD55/TLR7/CR1/TAP2/TNF/HLA-E/HLA-G/CD177/TAP2/TAP2/HLA-DRA/TNF/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/TAP2/HLA-B/NCR3/LTA/TNF/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/TAP2/LTA/HLA-A/HLA-DRA/TNF/TAP2/TNF/HLA-B/TNF/HLA-DRA/HLA-A/HLA-E/TNF/HLA-E/LTA/HLA-G/LTA/HLA-A/HLA-B/TAP2/TNF/HLA-G/HLA-E/HLA-DRA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/TAP2/NCR3/LTA/PTPRC/KIR2DL4/B2M/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/HLA-G/LILRB1/MIF/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/HLA-DRA/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/GPI/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4\n195\n\n\nGO:0002696\npositive regulation of leukocyte activation\n228/3662\n418/15509\n2.038126e-43\n1.148391e-40\n6.906224e-41\nBAD/CD38/CD4/BTK/TYROBP/IGF1/CD74/RUNX3/GAB2/ARID1B/FOXP3/AP3D1/KARS1/RHOA/CBFB/TBX21/GLI2/ACTB/MLH1/IL4R/PTPRC/MSH2/IL12RB1/ABL1/SMARCB1/XBP1/CD40/NFATC2/GATA1/CD40LG/PYCARD/CSK/IL7/LILRB1/IL27RA/TGFB1/JAK3/HAMP/EXOSC3/GATA3/MAP3K8/ZMIZ1/SMARCD2/CCL2/EFNB3/NSD2/CTSC/ZBTB16/CD5/MDK/CD81/IFNG/IL12B/IL4/IL5/BCL6/CD86/WNT5A/HES1/SNX4/IL1A/ZAP70/IGFBP2/CD160/MPL/ARID1A/MYB/VAMP8/FOXO3/NR4A3/GPAM/CD80/SHLD2/CDKN1A/SOX4/IL1B/TNFSF9/CD70/CCR7/NECTIN2/EPO/RARA/FLOT2/AP3B1/EPHB2/PTPN22/IL2RA/AKIRIN2/IL6/HLX/IL10/STXBP1/TLR4/CCL21/THBS1/IL21/SLC7A1/ABL2/DUSP10/IL2RG/IL18/MMP14/RUNX1/TNFRSF13C/ICOSLG/ITGB2/TNFSF13/VCAM1/TGFBR2/SPTA1/SHH/SYK/BRD7/RAG1/NOD2/FADD/RHOH/TNIP2/INPP5D/LGALS9/PTAFR/ITGAM/CCL19/KAT5/STAT5B/LEP/CD28/PTPN11/HLA-DQB1/KLHL25/SOCS1/LILRB4/ZP3/ARID2/BLOC1S3/HMGB1/CD55/HLA-DQA1/CD47/SRC/DPP4/PDCD1LG2/PNP/CR1/HLA-DOA/TNF/HLA-E/HLA-G/CD177/CRLF2/HLA-DQB1/HLA-DQA1/HLA-DQB1/HLA-DQA1/HLA-DRA/TNF/HLA-E/HLA-A/HLA-A/HLA-G/HLA-DPB1/HLA-DQA1/HLA-DPB1/TNF/HLA-A/HLA-A/HLA-DQA1/HLA-E/HLA-DQB1/HLA-DQA1/HLA-DPB1/HLA-A/HLA-DRA/HLA-DQA1/TNF/TNF/TNF/HLA-DRA/HLA-A/HLA-E/HLA-DPB1/TNF/HLA-E/HLA-G/HLA-DPB1/HLA-DPB1/HLA-DRB4/HLA-DQB1/HLA-DPA1/HLA-DQA1/HLA-A/HLA-DQB1/HLA-DQA1/TNF/HLA-G/HLA-DQA1/HLA-DQB1/HLA-E/HLA-DRA/HLA-G/HLA-A/HLA-G/HLA-DQA1/HLA-E/HLA-DPB1/HLA-G/HLA-DQA1/HLA-DPB1/CEBPA/PRKDC/HLA-DQA1/PTPRC/CCL5/CD24/B2M/CCL5/LILRB1/SMARCB1/LILRB4/HLA-G/LILRB1/MIF/LILRB1/HLA-DRA/LILRB1/LILRB4/LILRB4/INPP5D\n228\n\n\nGO:0002228\nnatural killer cell mediated immunity\n126/3662\n173/15509\n6.042310e-43\n3.120853e-40\n1.876826e-40\nSLAMF7/GZMB/LILRB1/IL12B/CD160/PRDX1/NECTIN2/ULBP2/ARL8B/IL21/ARRB2/LYST/IL18/CD96/KIR3DL1/LGALS9/SERPINB9/KIF5B/STAT5B/LEP/MICA/KIR2DL4/TUBB/HLA-E/HLA-G/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/HLA-B/NCR3/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/HLA-A/HLA-B/HLA-A/HLA-E/HLA-E/HLA-G/MICA/HLA-A/HLA-B/MICA/HLA-G/HLA-E/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/NCR3/KIR2DL4/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/HLA-G/KIR3DL1/KIR3DL1/KIR3DL1/LILRB1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1\n126\n\n\nGO:0042267\nnatural killer cell mediated cytotoxicity\n125/3662\n171/15509\n6.766033e-43\n3.225836e-40\n1.939961e-40\nSLAMF7/GZMB/LILRB1/IL12B/CD160/PRDX1/NECTIN2/ULBP2/ARL8B/IL21/ARRB2/LYST/IL18/KIR3DL1/LGALS9/SERPINB9/KIF5B/STAT5B/LEP/MICA/KIR2DL4/TUBB/HLA-E/HLA-G/HLA-B/HLA-E/HLA-A/HLA-A/HLA-G/HLA-B/NCR3/HLA-A/HLA-A/HLA-B/HLA-E/NCR3/HLA-A/HLA-B/HLA-A/HLA-E/HLA-E/HLA-G/MICA/HLA-A/HLA-B/MICA/HLA-G/HLA-E/MICA/HLA-G/HLA-A/HLA-G/NCR3/HLA-E/NCR3/NCR3/HLA-G/NCR3/KIR2DL4/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/HLA-G/KIR3DL1/KIR3DL1/KIR3DL1/LILRB1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/LILRB1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR3DL1/KIR3DL1/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1/KIR3DL1/KIR2DL4/KIR2DL4/KIR3DL1/KIR2DL4/KIR2DL4/KIR2DL4/KIR2DL4/KIR3DL1\n125\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInstead of saving just the results summary from the ego object, it might also be beneficial to save the object itself. The save() function enables you to save it as a .rda file, e.g.¬†save(ego, file=\"results/ego.rda\"). The complementary function to save() is the function load(), e.g.¬†\nego &lt;- load(file=\"results/ego.rda\").\nThis is a useful set of functions to know, since it enables one to preserve analyses at specific stages and reload them when needed. More information about these functions can be found here & here.\nYou can also perform GO enrichment analysis with only the up or down regulated genes** in addition to performing it for the full list of significant genes. This can be useful to identify GO terms impacted in one direction and not the other. If very few genes are in any of these lists (&lt; 50, roughly) it may not be possible to get any significant GO terms.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\nCreate two new GO enrichment analyses one with UP and another for DOWN regulated genes for Control vs Vampirium.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nSeparate results into UP and DOWN regulated:\n\nsigCont_UP &lt;- sigCont %&gt;% filter(log2FoldChange &gt; 0)\nsigCont_DOWN &lt;- sigCont %&gt;% filter(log2FoldChange &lt; 0)\n\nRun overrepresentation:\n\nUP regulated genes\nego_UP &lt;- enrichGO(gene = sigCont_UP$gene, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\nDOWN regulated genes\nego_DOWN &lt;- enrichGO(gene = sigCont_DOWN$gene, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n\nCheck results:\n\nhead(ego_UP)\nhead(ego_DOWN)\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing clusterProfiler results\nclusterProfiler has a variety of options for viewing the over-represented GO terms. We will explore the dotplot, enrichment plot, and the category netplot.\nThe dotplot shows the number of genes associated with the first terms (size) and the p-adjusted values for these terms (color). This plot displays the top 20 GO terms by gene ratio (# genes related to GO term / total number of sig genes), not p-adjusted value.\n## Dotplot \ndotplot(ego, showCategory=20)\n\nThe next plot is the enrichment GO plot, which shows the relationship between the top 50 most significantly enriched GO terms (padj.), by grouping similar terms together. Before creating the plot, we will need to obtain the similarity between terms using the pairwise_termsim() function instructions for emapplot. In the enrichment plot, the color represents the p-values relative to the other displayed terms (brighter red is more significant), and the size of the terms represents the number of genes that are significant from our list.\n# Add similarity matrix to the termsim slot of enrichment result\nego &lt;- enrichplot::pairwise_termsim(ego)\n# Enrichmap clusters the 50 most significant (by padj) GO terms to visualize relationships between terms\nemapplot(ego, showCategory = 50)\n\n\n\n\n\nFinally, the category netplot shows the relationships between the genes associated with the top five most significant GO terms and the fold changes of the significant genes associated with these terms (color). The size of the GO terms reflects the pvalues of the terms, with the more significant terms being larger. This plot is particularly useful for hypothesis generation in identifying genes that may be important to several of the most affected processes.\n\n\n\n\n\n\nWarning\n\n\n\nYou may need to install the ggnewscale package using install.packages(\"ggnewscale\") for the cnetplot() function to work.\n\n\n# To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector\nCont_foldchanges &lt;- sigCont$log2FoldChange\n\nnames(Cont_foldchanges) &lt;- sigCont$gene\n# Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj)\ncnetplot(ego, \n         categorySize=\"pvalue\", \n         showCategory = 5, \n         foldChange=Cont_foldchanges, \n         vertex.label.font=6)\n\n\n\n\n\n\nTip\n\n\n\nIf some of the high fold changes are getting drowned out due to a large range, you could set a maximum fold change value\nCont_foldchanges &lt;- ifelse(Cont_foldchanges &gt; 2, 2, Cont_foldchanges)\nCont_foldchanges &lt;- ifelse(Cont_foldchanges &lt; -2, -2, Cont_foldchanges)\ncnetplot(ego, \n        categorySize=\"pvalue\", \n        showCategory = 5, \n        foldChange=Cont_foldchanges, \n        vertex.label.font=6)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are interested in significant processes that are not among the top five, you can subset your ego dataset to only display these processes:\n# Subsetting the ego results without overwriting original `ego` variable\nego2 &lt;- ego\n\nego2@result &lt;- ego@result[c(1,3,4,8,9),]\n\n# Plotting terms of interest\ncnetplot(ego2, \n    categorySize=\"pvalue\", \n    foldChange=Cont_foldchanges, \n    showCategory = 5, \n    vertex.label.font=6)\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\n\nRun a Disease Ontology (DO) overrepresentation analysis using the enrichDO() function. NOTE the arguments are very similar to the previous examples.\n\nDo you find anything interesting?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nFor the DO, we need to use entrez IDs, instead of gene IDs\nAll significantly regulated genes.\nedo &lt;- enrichDO(sigCont$entrez, qvalueCutoff = 1)\nhead(edo)\nUP significantly regulated genes\nedo_UP &lt;- enrichDO(sigCont_UP$entrez)\nhead(edo_UP)\nDOWN significantly regulated genes\nedo_DOWN &lt;- enrichDO(sigCont_DOWN$entrez)\nhead(edo_DOWN)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\n\nRun an enrichment analysis on the results of the DEA for Garlicum vs Vampirium samples. Remember to use the annotated results!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nLet‚Äôs do a simple analysis as an example:\n## Create background dataset for hypergeometric testing using all genes tested for significance in the results\nallGar_genes &lt;- dplyr::filter(res_ids_Gar, !is.na(gene)) %&gt;% \n    pull(gene) %&gt;% \n    as.character()\n\n## Extract significant results\nsigGar &lt;- dplyr::filter(res_ids_Gar, padj &lt; 0.05 & !is.na(gene))\n\nsigGar_genes &lt;- sigGar %&gt;% \n    pull(gene) %&gt;% \n    as.character()\nNow we can perform the GO enrichment analysis and save the results:\n## Run GO enrichment analysis \nego &lt;- enrichGO(gene = sigGar_genes, \n                universe = allGar_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\ncluster_summary\n\nwrite.csv(cluster_summary, \"../Results/clusterProfiler_Gar-Vamp.csv\")\n\n\n\n\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Functional Analysis"
    ]
  },
  {
    "objectID": "develop/cards/AlbaMartinez.html",
    "href": "develop/cards/AlbaMartinez.html",
    "title": "Alba Refoyo Martinez",
    "section": "",
    "text": "Alba is a Sandbox data scientist based at the University of Copenhagen. During her academic background as a PhD and Postdoc she has developed a solid expertise in large-scale genomics and pipelines development on computing clusters."
  },
  {
    "objectID": "develop/cards/extraCards/JenniferBartell.html",
    "href": "develop/cards/extraCards/JenniferBartell.html",
    "title": "Jennifer Bartell",
    "section": "",
    "text": "Jennifer (Jennie) is the Sandbox project manager from the Center for Health Data Science at the University of Copenhagen. Jennie has a broad background in computational systems biology, bioinformatics, and health data modeling. Her PhD and postdoc work also included high throughput phenotyping and long read sequencing of bacterial strain libraries, so she can also speak ‚Äòwet lab‚Äô with experimentalists interested in gaining data science skills. In addition to project management, she‚Äôs developing ‚ÄòHPC lab‚Äô content and is knowledgable about research data management. She has been deeply involved in the synthetic health data arm of the project, which has turned into a collaborative research project between her, Anders Krogh, Martin Boegsted, and Jan Trzaskowski with a 4 year data science grant from the NNF."
  },
  {
    "objectID": "develop/07c_DEA_visualization.html",
    "href": "develop/07c_DEA_visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 75 minutes\nüí¨ Learning Objectives:\n\nExplain log fold change shrinkage\nSetup results data for application of visualization techniques\nDescribe different data visualization useful for exploring results from a DGE analysis\nCreate a volcano plot and MA plot to evaluate relationship among DGE statistics\nCreate a heatmap to illustrate expression changes of differentially expressed genes\nLog Fold Shrinkage and DEA visualizations\nIn the previous lessons, we learned about how to generate a table with Differentially Expressed genes.\nThe problem with these fold change estimates is that they are not entirely accurate as they do not account for the large dispersion we observe with low read counts. To address this, the log2 fold changes need to be adjusted.\nTo generate more accurate log2 fold change (LFC) estimates, DESeq2 allows for the shrinkage of the LFC estimates toward zero when the information for a gene is low, which could include:\nLFC shrinkage uses information from all genes to generate more accurate estimates. Specifically, the distribution of LFC estimates for all genes is used (as a prior) to shrink the LFC estimates of genes with little information or high dispersion toward more likely (lower) LFC estimates.\nIn the figure above, we have an example using two genes: green gene and purple gene. For each gene the expression values are plotted for each sample in the two different mouse strains (C57BL/6J and DBA/2J). Both genes have the same mean values for the two sample groups, but the green gene has little within-group variation while the purple gene has high levels of variation. For the green gene with low within-group variation, the unshrunken LFC estimate (vertex of the green solid line) is very similar to the shrunken LFC estimate (vertex of the green dotted line). However, LFC estimates for the purple gene are quite different due to the high dispersion. So even though two genes can have similar normalized count values, they can have differing degrees of LFC shrinkage. Notice the LFC estimates are shrunken toward the prior (black solid line).\nShrinking the log2 fold changes will not change the total number of genes that are identified as significantly differentially expressed. The shrinkage of fold change is to help with downstream assessment of results. For example, if you wanted to subset your significant genes based on fold change for further evaluation, you may want to use shrunken values. Additionally, for functional analysis tools such as GSEA which require fold change values as input you would want to provide shrunken values.\nTo generate the shrunken log2 fold change estimates, you have to run an additional step on your results object (that we will create below) with the function lfcShrink().\nDepending on the version of DESeq2 you are using the default method for shrinkage estimation will differ. The defaults can be changed by adding the argument type in the lfcShrink() function as we have above. For most recent versions of DESeq2, type=\"normal\" is the default and was the only method in earlier versions. It has been shown that in most situations there are alternative methods that have less bias than the ‚Äònormal‚Äô method, and therefore we chose to use apeglm.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Visualization"
    ]
  },
  {
    "objectID": "develop/07c_DEA_visualization.html#visualizing-the-results",
    "href": "develop/07c_DEA_visualization.html#visualizing-the-results",
    "title": "Visualization",
    "section": "Visualizing the results",
    "text": "Visualizing the results\nWhen we are working with large amounts of data it can be useful to display that information graphically. During this lesson, we will get you started with some basic and more advanced plots commonly used when exploring differential gene expression data, however, many of these plots can be helpful in visualizing other types of data as well.\nWe will be working with three different data objects we have already created in earlier lessons:\n\nMetadata for our samples (a dataframe): meta\nNormalized expression data for every gene in each of our samples (a matrix): normalized_counts\nTibble versions of the DESeq2 results we generated in the last lesson: res_tableCont_tb and res_tableGar_tb\n\nFirst, we already have a metadata tibble.\nmeta %&gt;% head()\nNext, let‚Äôs bring in the normalized_counts object with our gene names.\n# DESeq2 creates a matrix when you use the counts() function\n# First convert normalized_counts to a data frame and transfer the row names to a new column called \"gene\"\nnormalized_counts &lt;- counts(dds, normalized=T) %&gt;% \n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") \n\nMA plot\nA plot that can be useful to exploring our results is the MA plot. The MA plot shows the mean of the normalized counts versus the log2 fold changes for all genes tested. The genes that are significantly DE are colored to be easily identified (adjusted p-value &lt; 0.01 by default). This is also a great way to illustrate the effect of LFC shrinkage. The DESeq2 package offers a simple function to generate an MA plot.\nLet‚Äôs start with the unshrunken results:\n# MA plot using unshrunken fold changes\nplotMA(res_tableCont_unshrunken, ylim=c(-2,2))\nAnd now the shrunken results:\n# MA plot using shrunken fold changes\nplotMA(res_tableCont, ylim=c(-2,2))\nOn the left you have the unshrunken fold change values plotted and you can see the abundance of scatter for the lowly expressed genes. That is, many of these genes exhibit very high fold changes. After shrinkage, we see the fold changes are much smaller estimates.\n\nIn addition to the comparison described above, this plot allows us to evaluate the magnitude of fold changes and how they are distributed relative to mean expression. Generally, we would expect to see significant genes across the full range of expression levels.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\nWhy are there genes with high mean and big log2 fold changes, but are not statistically significant?‚Äù\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nBecause their expression values are very dispersed, and so their p-value will be very high.\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting significant DE genes\nOne way to visualize results would be to simply plot the expression data for a handful of genes. We could do that by picking out specific genes of interest or selecting a range of genes.\nUsing DESeq2 plotCounts() to plot expression of a single gene\nTo pick out a specific gene of interest to plot, for example TSPAN7 (ID ENSG00000156298), we can use the plotCounts() from DESeq2. plotCounts() requires that the gene specified matches the original input to DESeq2.\n# Plot expression for single gene\nplotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\") \n\n\n\n\n\n\n\nTip\n\n\n\nThis DESeq2 function only allows for plotting the counts of a single gene at a time, and is not flexible regarding the appearance.\n\n\nUsing ggplot2 to plot expression of a single gene\nIf you wish to change the appearance of this plot, we can save the output of plotCounts() to a variable specifying the returnData=TRUE argument, then use ggplot():\n# Save plotcounts to a data frame object\nd &lt;- plotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\", returnData=TRUE)\n\n# What is the data output of plotCounts()?\nd %&gt;% head()\n# Plot the MOV10 normalized counts, using the samples (rownames(d) as labels)\nggplot(d, aes(x = condition, y = count, color = condition)) + \ngeom_point(position=position_jitter(w = 0.1,h = 0)) +\ngeom_text_repel(aes(label = rownames(d))) + \ntheme_bw() +\nggtitle(\"TSPAN7\") +\ntheme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\nNote\n\n\n\nNote that in the plot below (code above), we are using geom_text_repel() from the ggrepel package to label our individual points on the plot.\n\n\n\nCreate a translator from gene names to gene IDs\nWhile gene IDs are unique and traceable, it is hard for us humans to memorize a bunch of numbers. Let‚Äôs try to make a translator function that will give you possible gene IDs for a gene name. Then you can use this table to select one of the possible gene_IDs.\nThe function will take as input a vector of gene names of interest, the tx2gene dataframe and the dds object that you analyzed:\nlookup &lt;- function(gene_name, tx2gene, dds){\n  hits &lt;- tx2gene %&gt;% select(gene_symbol, gene_ID) %&gt;% distinct() %&gt;% \n    filter(gene_symbol %in% gene_name & gene_ID %in% rownames(dds))\n  return(hits)\n}\n\nlookup(gene_name = \"TSPAN7\", tx2gene = tx2gene, dds = dds)\nOn the other hand, we can add the information from our tx2gene table, since it has the gene name!\ntx2gene\nHowever, we see that the table has many duplicates per gene, due to the fact that a gene may have several transcripts IDs associated to it. Since our results table has gene IDs, it is important to remove transcript information and remove duplicated rows before merging the information.\nWe remove the transcript ID column and duplicated rows from the tx2gene table using tidyverse syntax. We merge the tables using the merge function, which has many options for merging. Since our tables have different column names for the gene ID variable, we provide them with the by.x and by.y arguments. We also want to keep all of our results, so we use the argument all.x as well.\nres_tableCont_tb &lt;- merge(res_tableCont_tb, tx2gene %&gt;% select(-transcript_ID) %&gt;% distinct(),\n  by.x = \"gene\", by.y = \"gene_ID\", all.x = T)\n\nres_tableCont_tb\n\n\nHeatmap\nIn addition to plotting subsets, we could also extract the normalized values of all the significant genes and plot a heatmap of their expression using pheatmap().\n# Extract normalized expression for significant genes from the vampirium and control samples\n# also get gene name\nnorm_Contsig &lt;- normalized_counts %&gt;% select(gene, starts_with(\"control\"), starts_with(\"vampirium\")) \n  dplyr::filter(gene %in% sigCont$gene)  \nNow let‚Äôs draw the heatmap using pheatmap:\n# Run pheatmap using the metadata data frame for the annotation\npheatmap(norm_Contsig %&gt;% column_to_rownames(\"gene\"), \n  cluster_rows = T, \n  show_rownames = F,\n  annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% \n  select(\"condition\"), border_color = NA, fontsize = 10, \n  scale = \"row\", \n  fontsize_row = 10, \n  height = 20)\n\n\n\n\n\n\n\nNote\n\n\n\nThere are several additional arguments we have included in the function for aesthetics. One important one is scale=\"row\", in which Z-scores are plotted, rather than the actual normalized count value.\nZ-scores are computed on a gene-by-gene basis by subtracting the mean and then dividing by the standard deviation. The Z-scores are computed after the clustering, so that it only affects the graphical aesthetics and the color visualization is improved.\n\n\n\n\nVolcano plot\nThe above plot would be great to look at the expression levels of a good number of genes, but for more of a global view there are other plots we can draw. A commonly used one is a volcano plot; in which you have the log transformed adjusted p-values plotted on the y-axis and log2 fold change values on the x-axis.\nTo generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here.\nTo generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here.\n## Obtain logical vector where TRUE values denote padj values &lt; 0.05 and fold change &gt; 1.5 in either direction\n\nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% \nmutate(threshold_Cont = padj &lt; 0.05 & abs(log2FoldChange) &gt;= 0.58)\nNow we can start plotting. The geom_point object is most applicable, as this is essentially a scatter plot:\n## Volcano plot\nggplot(res_tableCont_tb) + \n  geom_point(aes(x = log2FoldChange, y = -log10(padj), colour = threshold_Cont)) +\n  ggtitle(\"Control vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  #scale_y_continuous(limits = c(0,50)) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25)))  \n\nChecking the top DE genes\nThis is a great way to get an overall picture of what is going on, but what if we also wanted to know where the top 10 genes (lowest padj) in our DE list are located on this plot? We could label those dots with the gene name on the Volcano plot using geom_text_repel().\nFirst, we need to order the res_tableCont tibble by padj, and add an additional column to it, to include on those gene names we want to use to label the plot.\n## Create an empty column to indicate which genes to label\nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% arrange(padj)\n\n## Populate the gene labels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tableCont_tb$genelabels[1:10] &lt;- as.character(res_tableCont_tb$gene_symbol[1:10])\n\nhead(res_tableCont_tb)\nNext, we plot it as before with an additional layer for geom_text_repel() wherein we can specify the column of gene labels we just created.\nggplot(res_tableCont_tb, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold_Cont)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Control vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25)))",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Visualization"
    ]
  },
  {
    "objectID": "develop/07c_DEA_visualization.html#other-visualization-tools",
    "href": "develop/07c_DEA_visualization.html#other-visualization-tools",
    "title": "Visualization",
    "section": "Other visualization tools",
    "text": "Other visualization tools\nIf you use the DESeq2 tool for differential expression analysis, the package ‚ÄòDEGreport‚Äô can use the DESeq2 results output to make the top20 genes and the volcano plots generated above by writing a few lines of simple code. While you can customize the plots above, you may be interested in using the easier code. Below are examples of the code to create these plots:*\nDEGreport::degPlot(dds = dds, res = res, n = 20, xs = \"type\", group = \"condition\") # dds object is output from DESeq2\n\nDEGreport::degVolcano(\n    data.frame(res[,c(\"log2fold change\",\"padj\")]), # table - 2 columns\n    plot_text = data.frame(res[1:10,c(\"log2fold change\",\"padj\",\"id\")])) # table to add names\n    \n# Available in the newer version for R 3.4\nDEGreport::degPlotWide(dds = dds, genes = row.names(res)[1:5], group = \"condition\")\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\n\nCreate visualizations of the results from your DEA between Garlicum samples and Vampirium samples.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nOur Garlicum results are saved in this table. However, we will want to use the LFC shrunken values\n# Normal results\nres_tableGar_unshrunken &lt;- res_tableGar\n\n# Shrunken values\nres_tableGar &lt;- lfcShrink(dds, coef=\"condition_garlicum_vs_vampirium\", type=\"apeglm\")\nWe can plot a MAplot:\nplotMA(res_tableGar, ylim=c(-2,2))\nAnd continue with a volcano plot and a heatmap. But first, let‚Äôs merge our results with with our tx2gene table:\n```{.r code-overflow-wrap} res_tableGar_tb &lt;- merge(res_tableGar_tb, tx2gene %&gt;% select(-transcript_ID) %&gt;% distinct(), by.x = ‚Äúgene‚Äù, by.y = ‚Äúgene_ID‚Äù, all.x = T)\nres_tableGar_tb\nVolcano plot with top 10 genes:\n\n```{.r code-overflow-wrap}\n## Create an empty column to indicate which genes to label\nres_tableGar_tb &lt;- res_tableGar_tb %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tableGar_tb &lt;- res_tableGar_tb %&gt;% arrange(padj)\n\n## Populate the gene labels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tableGar_tb$genelabels[1:10] &lt;- as.character(res_tableGar_tb$gene_symbol[1:10])\n\nhead(res_tableGar_tb)\n{.r code-overflow-wrap} ggplot(res_tableGar_tb, aes(x = log2FoldChange, y = -log10(padj))) +     geom_point(aes(colour = threshold_Cont)) +     geom_text_repel(aes(label = genelabels)) +     ggtitle(\"Garlicum vs Vampirium\") +     xlab(\"log2 fold change\") +      ylab(\"-log10 adjusted p-value\") +     theme(legend.position = \"none\",         plot.title = element_text(size = rel(1.5), hjust = 0.5),         axis.title = element_text(size = rel(1.25))) Heatmap of DE genes for Vampirium vs Garlicum:\n```{.r code-overflow-wrap}\n\nExtract normalized expression for significant genes from the vampirium and garlicum samples, also get gene name\nnorm_Garsig &lt;- normalized_counts %&gt;% select(gene, starts_with(‚Äúgarlicum‚Äù), starts_with(‚Äúvampirium‚Äù)) dplyr::filter(gene %in% sigGar$gene)\n\nNow let's draw the heatmap using `pheatmap`:\n\n\n```{.r code-overflow-wrap}\n# Run pheatmap using the metadata data frame for the annotation\npheatmap(norm_Garsig %&gt;% column_to_rownames(\"gene\"), \n      cluster_rows = T, \n      show_rownames = F,\n      annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% select(\"condition\"), \n      border_color = NA, \n      fontsize = 10, \n      scale = \"row\", \n      fontsize_row = 10, \n      height = 20)\n\n\n\n\n\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nMaterials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Visualization"
    ]
  },
  {
    "objectID": "develop/05c_count_normalization.html",
    "href": "develop/05c_count_normalization.html",
    "title": "Normalization",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 40 minutes\nüí¨ Learning Objectives:\n\nExplore different types of normalization methods\nBecome familiar with the DESeqDataSet object\nUnderstand how to normalize counts using DESeq2\nCount normalization with DESeq2\nThe first step in the DE analysis workflow is count normalization, which is necessary to make accurate comparisons of gene expression between samples.\nThe counts of mapped reads for each gene is proportional to the expression of RNA (‚Äúinteresting‚Äù) in addition to many other factors (‚Äúuninteresting‚Äù). Normalization is the process of scaling raw count values to account for the ‚Äúuninteresting‚Äù factors. In this way the expression levels are more comparable between and/or within samples.\nThe main factors often considered during normalization are:",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Normalization"
    ]
  },
  {
    "objectID": "develop/05c_count_normalization.html#common-normalization-methods",
    "href": "develop/05c_count_normalization.html#common-normalization-methods",
    "title": "Normalization",
    "section": "Common normalization methods",
    "text": "Common normalization methods\nSeveral common normalization methods exist to account for these differences:\n\n\n\n\nNormalization method\nDescription\nAccounted factors\nRecommendations for use\n\n\n\n\nCPM (counts per million)\ncounts scaled by total number of reads\nsequencing depth\ngene count comparisons between replicates of the same samplegroup; NOT for within sample comparisons or DE analysis\n\n\nTPM (transcripts per kilobase million)\ncounts per length of transcript (kb) per million reads mapped\nsequencing depth and gene length\ngene count comparisons within a sample or between samples of the same sample group; NOT for DE analysis\n\n\nRPKM/FPKM (reads/fragments per kilobase of exon per million reads/fragments mapped)\nsimilar to TPM\nsequencing depth and gene length\ngene count comparisons between genes within a sample; NOT for between sample comparisons or DE analysis\n\n\nDESeq2‚Äôs median of ratios\ncounts divided by sample-specific size factors determined by median ratio of gene counts relative to geometric mean per gene\nsequencing depth and RNA composition\ngene count comparisons between samples and for DE analysis; NOT for within sample comparisons\n\n\nEdgeR‚Äôs trimmed mean of M values (TMM)\nuses a weighted trimmed mean of the log expression ratios between samples\nsequencing depth, RNA composition\ngene count comparisons between samples and for DE analysis; NOT for within sample comparisons",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Normalization"
    ]
  },
  {
    "objectID": "develop/05c_count_normalization.html#rpkmfpkm-not-recommended",
    "href": "develop/05c_count_normalization.html#rpkmfpkm-not-recommended",
    "title": "Normalization",
    "section": "RPKM/FPKM (not recommended)",
    "text": "RPKM/FPKM (not recommended)\nWhile TPM and RPKM/FPKM normalization methods both account for sequencing depth and gene length, RPKM/FPKM are not recommended. The reason is that the normalized count values output by the RPKM/FPKM method are not comparable between samples.\nUsing RPKM/FPKM normalization, the total number of RPKM/FPKM normalized counts for each sample will be different. Therefore, you cannot compare the normalized counts for each gene equally between samples.\nRPKM-normalized counts table\n\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nXCR1\n5.5\n5.5\n\n\nWASHC1\n73.4\n21.8\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\nTotal RPKM-normalized counts\n1,000,000\n1,500,000\n\n\n\n\nFor example, in the table above, SampleA has a greater proportion of counts associated with XCR1 (5.5/1,000,000) than does sampleB (5.5/1,500,000) even though the RPKM count values are the same. Therefore, we cannot directly compare the counts for XCR1 (or any other gene) between sampleA and sampleB because the total number of normalized counts are different between samples.",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Normalization"
    ]
  },
  {
    "objectID": "develop/05c_count_normalization.html#deseq2-normalized-counts-median-of-ratios-method",
    "href": "develop/05c_count_normalization.html#deseq2-normalized-counts-median-of-ratios-method",
    "title": "Normalization",
    "section": "DESeq2-normalized counts: Median of ratios method",
    "text": "DESeq2-normalized counts: Median of ratios method\nSince tools for differential expression analysis are comparing the counts between sample groups for the same gene, gene length does not need to be accounted for by the tool. However, sequencing depth and RNA composition do need to be taken into account.\nTo normalize for sequencing depth and RNA composition, DESeq2 uses the median of ratios method. On the user-end there is only one step, but on the back-end there are multiple steps involved, as described below.\n\n\n\n\n\n\nNote on the DESeq2 workflow\n\n\n\nThe steps below describe in detail some of the steps performed by DESeq2 when you run a single function to get DE genes. Basically, for a typical RNA-seq analysis, you would not run these steps individually.\n\n\n\nStep 1: creates a pseudo-reference sample (row-wise geometric mean)\nFor each gene, a pseudo-reference sample is created that is equal to the geometric mean across all samples.\n\n\n\n\ngene\nsampleA\nsampleB\npseudo-reference sample\n\n\n\n\nEF2A\n1489\n906\nsqrt(1489 * 906) = 1161.5\n\n\nABCD1\n22\n13\nsqrt(22 * 13) = 17.7\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\nStep 2: calculates ratio of each sample to the reference\nFor every gene in a sample, the ratios (sample/ref) are calculated (as shown below). This is performed for each sample in the dataset. Since the majority of genes are not differentially expressed, the majority of genes in each sample should have similar ratios within the sample.\n\n\n\n\n\n\n\n\n\n\n\n\ngene\nsampleA\nsampleB\npseudo-reference sample\nratio of sampleA/ref\nratio of sampleB/ref\n\n\n\n\nEF2A\n1489\n906\n1161.5\n1489/1161.5 = 1.28\n906/1161.5 = 0.78\n\n\nABCD1\n22\n13\n16.9\n22/16.9 = 1.30\n13/16.9 = 0.77\n\n\nMEFV\n793\n410\n570.2\n793/570.2 = 1.39\n410/570.2 = 0.72\n\n\nBAG1\n76\n42\n56.5\n76/56.5 = 1.35\n42/56.5 = 0.74\n\n\nMOV10\n521\n1196\n883.7\n521/883.7 = 0.590\n1196/883.7 = 1.35\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\n\n\nStep 3: calculate the normalization factor for each sample (size factor)\nThe median value (column-wise for the above table) of all ratios for a given sample is taken as the normalization factor (size factor) for that sample, as calculated below. Notice that the differentially expressed genes should not affect the median value:\nnormalization_factor_sampleA &lt;- median(c(1.28, 1.3, 1.39, 1.35, 0.59))\nnormalization_factor_sampleB &lt;- median(c(0.78, 0.77, 0.72, 0.74, 1.35))\nThe figure below illustrates the median value for the distribution of all gene ratios for a single sample (frequency is on the y-axis).\n\nThe median of ratios method assumes that not ALL genes are differentially expressed; therefore, the normalization factors should account for sequencing depth and RNA composition of the sample (large outlier genes will not represent the median ratio values). This method is robust to imbalance in up-/down-regulation and large numbers of differentially expressed genes.\n\n\n\n\n\n\nWarning\n\n\n\nUsually, these size factors are around 1, if you see large variations between samples it is important to take note since it might indicate the presence of extreme outliers.\n\n\n\n\nStep 4: calculate the normalized count values using the normalization factor\nThis is performed by dividing each raw count value in a given sample by that sample‚Äôs normalization factor to generate normalized count values. This is performed for all count values (every gene in every sample). For example, if the median ratio for SampleA was 1.3 and the median ratio for SampleB was 0.77, you could calculate normalized counts as follows:\nSampleA median ratio = 1.3\nSampleB median ratio = 0.77\nRaw Counts\n\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nEF2A\n1489\n906\n\n\nABCD1\n22\n13\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\nNormalized Counts\n\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nEF2A\n1489 / 1.3 = 1145.39\n906 / 0.77 = 1176.62\n\n\nABCD1\n22 / 1.3 = 16.92\n13 / 0.77 = 16.88\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that normalized count values are not whole numbers.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\nDetermine the normalized (median of ratios) counts for your gene of interest, PD1, given the raw counts and size factors below.\nNOTE: You will need to run the code below to generate the raw counts dataframe (PD1) and the size factor vector (size_factors), then use these objects to determine the normalized counts values:\n# Raw counts for PD1\nPD1 &lt;- t(c(21, 58, 17, 97, 83, 10)) %&gt;% \n    as_tibble() %&gt;%\n    rename_all(~paste0(\"Sample\", 1:6))\n\n\n# Size factors for each sample\nsize_factors &lt;- c(1.32, 0.70, 1.04, 1.27, 1.11, 0.85)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nLet‚Äôs check first what is PD1\nPD1\nSince we have the size factors per sample, we only need to divide our PD1 counts by the size factors!\nPD1/size_factors",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Normalization"
    ]
  },
  {
    "objectID": "develop/05c_count_normalization.html#count-normalization-of-the-vampirium-dataset-using-deseq2",
    "href": "develop/05c_count_normalization.html#count-normalization-of-the-vampirium-dataset-using-deseq2",
    "title": "Normalization",
    "section": "Count normalization of the Vampirium dataset using DESeq2",
    "text": "Count normalization of the Vampirium dataset using DESeq2\nNow that we know the theory of count normalization, we will normalize the counts for the Vampirium dataset using DESeq2. This requires a few steps:\n\nEnsure the row names of the metadata dataframe are present and in the same order as the column names of the counts dataframe.\nCreate a DESeqDataSet object\nGenerate the normalized counts\n\n\n1. Match the metadata and counts data\nWe should always make sure that we have sample names that match between the two files, and that the samples are in the right order. DESeq2 will output an error if this is not the case. Since we built our txi object from our metadata, everything should be OK.\n### Check that sample names match in both files\nall(colnames(txi$counts) %in% meta$sample)\nall(colnames(txi$counts) == meta$sample)\nIf your data did not match, you could use the match() function to rearrange them to be matching. match() function will take two arguments and find in which order the indexes of the second argument match the first argument.\na &lt;- c(\"a\",\"b\",\"c\")\nb &lt;- c(\"b\",\"c\",\"a\")\n\nreorder &lt;- match(a,b)\nreorder\n\nb[reorder]\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\n\nSuppose we had sample names matching in the txi object and metadata file, but they were out of order. Write the line(s) of code required make the meta_random dataframe with rows ordered such that they were identical to the column names of the txi.\n# randomize metadata rownames\nmeta_random &lt;- meta[sample(1:nrow(meta)),]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nLet‚Äôs check now meta_random order:\nmeta_random\nWe can see that it is all scrambled. We want the rows of meta_random to be the same order as the columns of the txi@counts object (which is not, as you can see below):\n### Check that sample names match in both files\nall(colnames(txi$counts) %in% meta_random$sample) # are all samples in our metadata?\nall(colnames(txi$counts) == meta_random$sample) # are all samples in the same order?\nLet‚Äôs use the match function. First we find the order that meta_random$sample should be to match the columns of txi@counts:\nreorder &lt;- match(colnames(txi$counts),meta_random$sample)\nreorder\nFinally, we change the order of the rows of meta_random:\nmeta_random &lt;- meta_random[reorder,]\nmeta_random\nAnd confirm:\nall(colnames(txi$counts) == meta_random$sample) # are all samples in the same order?\n\n\n\n\n\n\n\n\n\n\n\n\n2. Create DESEq2 object\nBioconductor software packages often define and use a custom class within R for storing data (input data, intermediate data and also results). These custom data structures are similar to lists in that they can contain multiple different data types/structures within them. But, unlike lists they have pre-specified data slots, which hold specific types/classes of data. The data stored in these pre-specified slots can be accessed by using specific package-defined functions.\nLet‚Äôs start by creating the DESeqDataSet object, and then we can talk a bit more about what is stored inside it. To create the object, we will need the txi object and the metadata table as input (colData argument). We will also need to specify a design formula. The design formula specifies which column(s) of our metadata we want to use for statistical testing and modeling (more about that later!). For our dataset we only have one column we are interested in, which is condition. This column has three factor levels, which tells DESeq2 that for each gene we want to evaluate gene expression change with respect to these different levels.\nIt is very important to establish beforehand which sample type will be our ‚Äúbase‚Äù or ‚Äúreference‚Äù level. If nothing is changed, DESeq2 will assume that our reference samples will be the first sample type (in alphabetical order). You can check this using the factor() function.\nfactor(meta$condition)\nWhile in a normal experiment we would use control samples as our reference, in our case we are interested in both checking the differences between control vs.¬†vampirium and garlicum vs.¬†vampirium. Thus, it would be much more convinient to reorganize our factor base level to vampirium. We can do this also with the factor() function, using the levels = argument.\nmeta$condition = factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\nfactor(meta$condition)\nWe can see now that vampirium is the first factor! Meaning that it will be interpreted by DESeq as our reference sample type.\nOur count matrix input is stored in the txi list object. So we need to specify that using the DESeqDataSetFromTximport() function, which will extract the counts component and round the values to the nearest whole number.\n# colData argument requires rownames in order to assess matching sample names\n# meta is a tibble object from tidyverse, so we neeed to add rownames.\n# If you do not do this and the samples do not match, you will add wrong info!\n\ndds &lt;- DESeqDataSetFromTximport(txi,\n    colData = meta %&gt;% column_to_rownames(\"sample\"), \n    design = ~ condition)\n\n\n\n\n\n\nControl is not reference level warning\n\n\n\n\n\nThe warning from the chunk before is telling us that we have setup our vampirium samples as reference, instead of control! This is exactly what we wanted.\n\n\n\n\n\n\n\n\n\nStarting from a traditional count matrix\n\n\n\n\n\nIf you did not create pseudocounts, but a count matrix from aligned BAM files and tools such as featurecounts, you would want to use the DESeqDataSetFromMatrix() function.\n\n\n\n## DO NOT RUN!\n## Create DESeq2Dataset object from traditional count matrix\ndds &lt;- DESeqDataSetFromMatrix(countData = \"../Data/Vampirium_counts_traditional.tsv\", \n        colData = meta %&gt;% column_to_rownames(\"sample\"), \n        design = ~ condition)\nYou can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts() (Note: we nested it within the View() function so that rather than getting printed in the console we can see it in the script editor) :\nView(counts(dds))\nAs we go through the workflow we will use the relevant functions to check what information gets stored inside our object.\nYou can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts():\nhead(counts(dds))\nAs we go through the workflow we will use the relevant functions to check what information gets stored inside our object.\n\nPre-filtering\nWhile it is not necessary to pre-filter low count genes before running the DESeq2 functions, there are two reasons which make pre-filtering useful:\n\nBy removing rows in which there are very few reads, we reduce the memory size of the dds data object, and we increase the speed of the transformation and testing functions within DESeq2.\nIt can also improve visualizations, as features with no information for differential expression are not plotted.\n\nHere we perform a minimal pre-filtering to keep only rows that have at least 10 reads total.\nkeep &lt;- rowSums(counts(dds)) &gt;= 10\ndds &lt;- dds[keep,]\n\n\n\n3. Generate the normalized counts\nThe next step is to normalize the count data in order to be able to make fair gene comparisons between samples.\nTo perform the median of ratios method of normalization, DESeq2 has a single estimateSizeFactors() function that will generate size factors for us. We will use the function in the example below, but in a typical RNA-seq analysis this step is automatically performed by the DESeq() function, which we will see later.\ndds &lt;- estimateSizeFactors(dds)\nBy assigning the results back to the dds object we are filling in the slots of the DESeqDataSet object with the appropriate information. We can take a look at the normalization factor applied to each sample using:\nsizeFactors(dds)\nNow, to retrieve the normalized counts matrix from dds, we use the counts() function and add the argument normalized=TRUE.\nnormalized_counts &lt;- counts(dds, normalized=TRUE)\nWe can save this normalized data matrix to file for later use:\nwrite.table(normalized_counts, file=\"/work/Intro_to_bulkRNAseq/Results/normalized_counts.txt\", sep=\"\\t\", quote=F)\n\n\n\n\n\n\nWarning\n\n\n\nDESeq2 doesn‚Äôt actually use normalized counts, rather it uses the raw counts and models the normalization inside the Generalized Linear Model (GLM). These normalized counts will be useful for downstream visualization of results, but cannot be used as input to DESeq2 or any other tools that perform differential expression analysis which use the negative binomial model.\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Data analyses",
      "Quality control",
      "Normalization"
    ]
  },
  {
    "objectID": "develop/rmd/08c_FA_GSEA.html",
    "href": "develop/rmd/08c_FA_GSEA.html",
    "title": "Functional class scoring",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 40 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1.  Discuss functional class scoring, and pathway topology methods\n2.  Construct a GSEA analysis using GO and KEGG gene sets\n3.  Examine results of a GSEA using pathview package\n4.  List other tools and resources for identifying genes of novel pathways or networks\nOver-representation analysis is only a single type of functional analysis method that is available for teasing apart the biological processes important to your condition of interest. Other types of analyses can be equally important or informative, including functional class scoring methods.\n\n\n\n\n\n\n\n\n\nFunctional class scoring (FCS) tools, such as GSEA, most often use the gene-level statistics or log2 fold changes for all genes from the differential expression results, then look to see whether gene sets for particular biological pathways are enriched among the large positive or negative fold changes.\n\n\n\n\n\n\n\n\n\nThe hypothesis of FCS methods is that although large changes in individual genes can have significant effects on pathways (and will be detected via ORA methods), weaker but coordinated changes in sets of functionally related genes (i.e., pathways) can also have significant effects. Thus, rather than setting an arbitrary threshold to identify ‚Äòsignificant genes‚Äô, all genes are considered in the analysis. The gene-level statistics from the dataset are aggregated to generate a single pathway-level statistic and statistical significance of each pathway is reported. This type of analysis can be particularly helpful if the differential expression analysis only outputs a small list of significant DE genes.\n\n\nUsing the log2 fold changes obtained from the differential expression analysis for every gene, gene set enrichment analysis and pathway analysis can be performed using clusterProfiler and Pathview tools.\nFor a gene set or pathway analysis using clusterProfiler, coordinated differential expression over gene sets is tested instead of changes of individual genes.\n!!! info\nGene sets are pre-defined groups of genes, which are functionally related. Commonly used gene sets include those derived from KEGG pathways, Gene Ontology terms, MSigDB, Reactome, or gene groups that share some other functional annotations, etc. Consistent perturbations over such gene sets frequently suggest mechanistic changes.\n\n\nclusterProfiler offers several functions to perform GSEA using different genes sets, including but not limited to GO, KEGG, and MSigDb. We will use the KEGG gene sets, which identify genes using their Entrez IDs. Therefore, to perform the analysis, we will need to acquire the Entrez IDs. We will also need to remove the Entrez ID NA values and duplicates (due to gene ID conversion) prior to the analysis:\n\n\nCode\n# Remove any NA values (reduces the data by quite a bit) and duplicates\nres_entrez &lt;- dplyr::filter(res_ids, entrez != \"NA\" & duplicated(entrez)==F)\n\n\nFinally, extract and name the fold changes:\n\n\nCode\n# Extract the foldchanges\nfoldchanges &lt;- res_entrez$log2FoldChange\n\n# Name each fold change with the corresponding Entrez ID\nnames(foldchanges) &lt;- res_entrez$entrez\n\n\nNext we need to order the fold changes in decreasing order. To do this we‚Äôll use the sort() function, which takes a vector as input. This is in contrast to Tidyverse‚Äôs arrange(), which requires a data frame.\n\n\nCode\n## Sort fold changes in decreasing order\nfoldchanges &lt;- sort(foldchanges, decreasing = TRUE)\n\nhead(foldchanges)\n\n\n\n\n\nNow we are ready to perform GSEA. The details regarding GSEA can be found in the PNAS paper by Subramanian et al.¬†We will describe briefly the steps outlined in the paper below:\n\n\n\n\n\n\n\n\n\nImage credit: Subramanian et al.¬†Proceedings of the National Academy of Sciences Oct 2005, 102 (43) 15545-15550; DOI: 10.1073/pnas.0506580102\nThis image describes the theory of GSEA, with the ‚Äògene set S‚Äô showing the metric used (in our case, ranked log2 fold changes) to determine enrichment of genes in the gene set. The left-most image is representing this metric used for the GSEA analysis. The log2 fold changes for each gene in the ‚Äògene set S‚Äô is shown as a line in the middle image. The large positive log2 fold changes are at the top of the gene set image, while the largest negative log2 fold changes are at the bottom of the gene set image. In the right-most image, the gene set is turned horizontally, underneath which is an image depicting the calculations involved in determining enrichment, as described below.\n\n\nAn enrichment score for a particular gene set is calculated by walking down the list of log2 fold changes and increasing the running-sum statistic every time a gene in the gene set is encountered and decreasing it when genes are not part of the gene set. The size of the increase/decrease is determined by magnitude of the log2 fold change. Larger (positive or negative) log2 fold changes will result in larger increases or decreases. The final enrichment score is where the running-sum statistic is the largest deviation from zero.\n\n\n\nThe significance of the enrichment score is determined using permutation testing, which performs rearrangements of the data points to determine the likelihood of generating an enrichment score as large as the enrichment score calculated from the observed data. Essentially, for this step, the first permutation would reorder the log2 fold changes and randomly assign them to different genes, reorder the gene ranks based on these new log2 fold changes, and recalculate the enrichment score. The second permutation would reorder the log2 fold changes again and recalculate the enrichment score again, and this would continue for the total number of permutations run. Therefore, the number of permutations run will increase the confidence in the signficance estimates.\n\n\n\nAfter all gene sets are tested, the enrichment scores are normalized for the size of the gene set, then the p-values are corrected for multiple testing.\nThe GSEA output will yield the core genes in the gene sets that most highly contribute to the enrichment score. The genes output are generally the genes at or before the running sum reaches its maximum value (e.g.¬†the most influential genes driving the differences between conditions for that gene set).\n\n\n\n\nTo perform the GSEA using KEGG gene sets with clusterProfiler, we can use the gseKEGG() function:\n\n\nCode\n## GSEA using gene sets from KEGG pathways\ngseaKEGG &lt;- gseKEGG(geneList = foldchanges, # ordered named vector of fold changes (Entrez IDs are the associated names)\n              organism = \"hsa\", # supported organisms listed below\n              pvalueCutoff = 0.05, # padj cutoff value\n              verbose = FALSE)\n\n## Extract the GSEA results\ngseaKEGG_results &lt;- gseaKEGG@result\nhead(gseaKEGG_results)\n\n\n!!! info ‚ÄúOther organisms for KEGG pathways‚Äù\nThe organisms with KEGG pathway information are listed [here](http://www.genome.jp/kegg/catalog/org_list.html).\nHow many pathways are enriched? Let‚Äôs view the enriched pathways:\n\n\nCode\n## Write GSEA results to file\nView(gseaKEGG_results)\n\nwrite.csv(gseaKEGG_results, \"../Results/gseaCont-Vamp_kegg.csv\", quote=F)\n\n\n!!! warning\nWe will can all get different results for the GSEA because the permutations performed use random reordering. If we would like to use the same permutations every time we run a function (i.e. we would like the same results every time we run the function), then we could use the `set.seed(123456)` function prior to running. The input to `set.seed()` could be any number, but if you would want the same results, then you would need to use the same number as input.\nExplore the GSEA plot of enrichment of one of the pathways in the ranked list:\n\n\nCode\n## Plot the GSEA plot for a single enriched pathway:\ngseaplot(gseaKEGG, geneSetID = gseaKEGG_results$ID[1], title = gseaKEGG_results$Description[1])\n\n\n\n\n\n\n\n\n\n\n\nIn this plot, the lines in plot represent the genes in the gene set, and where they occur among the log2 fold changes. The largest positive log2 fold changes are on the left-hand side of the plot, while the largest negative log2 fold changes are on the right. The top plot shows the magnitude of the log2 fold changes for each gene, while the bottom plot shows the running sum, with the enrichment score peaking at the red dotted line (which is among the negative log2 fold changes).\nUse the Pathview R package to integrate the KEGG pathway data from clusterProfiler into pathway images:\n\n\nCode\n## Output images for a single significant KEGG pathway\npathview(gene.data = foldchanges,\n              pathway.id = gseaKEGG_results$ID[1],\n              species = \"hsa\",\n              limit = list(gene = 2, # value gives the max/min limit for foldchanges\n              cpd = 1))\n\n\n\n\n\n\n\n\n\n\n\n!!! tip ‚ÄúPrinting out Pathview images for all significant pathways‚Äù\nPrinting out Pathview images for all significant pathways can be easily performed as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Output images for all significant KEGG pathways\nget_kegg_plots &lt;- function(x) {\n  pathview(gene.data = foldchanges, \n        pathway.id = gseaKEGG_results$ID[x], \n        species = \"hsa\",\n        limit = list(gene = 2, cpd = 1))\n  }\n\npurrr::map(1:length(gseaKEGG_results$ID), \n       get_kegg_plots)\n```\n:::\nInstead of exploring enrichment of KEGG gene sets, we can also explore the enrichment of BP Gene Ontology terms using gene set enrichment analysis:\n\n\nCode\n# GSEA using gene sets associated with BP Gene Ontology terms\ngseaGO &lt;- gseGO(geneList = foldchanges, \n              OrgDb = org.Hs.eg.db, \n              ont = 'BP', \n              minGSSize = 20, \n              pvalueCutoff = 0.05,\n              verbose = FALSE) \n\ngseaGO_results &lt;- gseaGO@result\nhead(gseaGO_results)\n\n\n\n\nCode\ngseaplot(gseaGO, geneSetID = gseaGO_results$ID[1], title = gseaGO_results$Description[1])\n\n\nThere are other gene sets available for GSEA analysis in clusterProfiler (Disease Ontology, Reactome pathways, etc.). You can check out this link for more!\n!!! question ‚ÄúExercise 1‚Äù\nRun a Disease Ontology (DO) GSE analysis using the `gseDO()` function. The arguments are very similar to the previous examples!\n\n-   Do you find anything interesting?\n??? question ‚ÄúSolution to Exercise 1‚Äù\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngseaDO &lt;- gseDO(foldchanges, pvalueCutoff = 1)\n\nhead(gseaDO)\n```\n:::\n\n\nWe see now very similar results to our overrepresentation analysis. This is due to the fact that we have quite big lists of differentially expressed genes. GSEA analysis are most useful when your gene lists are low and normal overrepresentation analysis will perform very poorly.\n!!! question ‚ÄúExercise 2‚Äù\nRun an GSE on the results of the DEA for Garlicum vs Vampirium samples. Remember to use the annotated results!\n??? question ‚ÄúSolution to Exercise 2‚Äù\nWe need to prepare our values for GSEA:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Remove any NA values (reduces the data by quite a bit) and duplicates\nres_entrez_Gar &lt;- dplyr::filter(res_ids_Gar, entrez != \"NA\" & duplicated(entrez)==F)\n```\n:::\n\n\nFinally, extract and name the fold changes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract the foldchanges\nfoldchanges_Gar &lt;- res_entrez_Gar$log2FoldChange\n\n# Name each fold change with the corresponding Entrez ID\nnames(foldchanges_Gar) &lt;- res_entrez_Gar$entrez\n```\n:::\n\n\nNext we need to order the fold changes in decreasing order. To do this we'll use the `sort()` function, which takes a vector as input. This is in contrast to Tidyverse's `arrange()`, which requires a data frame.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Sort fold changes in decreasing order\nfoldchanges_Gar &lt;- sort(foldchanges_Gar, decreasing = TRUE)\n\nhead(foldchanges_Gar)\n```\n:::\n\n\nNe we can perform GSEA. This is an example for GO term analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngseaGO &lt;- gseGO(geneList = foldchanges_Gar, \n              OrgDb = org.Hs.eg.db, \n              ont = 'BP', \n              minGSSize = 20, \n              pvalueCutoff = 0.05,\n              verbose = FALSE) \n\ngseaGO_results &lt;- gseaGO@result\nhead(gseaGO_results)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngseaplot(gseaGO, geneSetID = gseaGO_results$ID[1], title = gseaGO_results$Description[1])\n```\n:::\n\n\n\n\n\n\nIt is possible to supply your own gene set GMT file, such as a GMT for MSigDB using special clusterProfiler functions as shown below:\n\n\nCode\n# DO NOT RUN\nBiocManager::install(\"GSEABase\")\nlibrary(GSEABase)\n\n# Load in GMT file of gene sets (we downloaded from the Broad Institute for MSigDB)\n\nc2 &lt;- read.gmt(\"/data/c2.cp.v6.0.entrez.gmt.txt\")\n\nmsig &lt;- GSEA(foldchanges, TERM2GENE=c2, verbose=FALSE)\n\nmsig_df &lt;- data.frame(msig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPathway topology analysis often takes into account gene interaction information along with the fold changes and adjusted p-values from differential expression analysis to identify dysregulated pathways. Depending on the tool, pathway topology tools explore how genes interact with each other (e.g.¬†activation, inhibition, phosphorylation, ubiquitination, etc.) to determine the pathway-level statistics. Pathway topology-based methods utilize the number and type of interactions between gene product (our DE genes) and other gene products to infer gene function or pathway association.\nFor instance, the SPIA (Signaling Pathway Impact Analysis) tool can be used to integrate the lists of differentially expressed genes, their fold changes, and pathway topology to identify affected pathways.\n\n\n\nCo-expression clustering is often used to identify genes of novel pathways or networks by grouping genes together based on similar trends in expression. These tools are useful in identifying genes in a pathway, when their participation in a pathway and/or the pathway itself is unknown. These tools cluster genes with similar expression patterns to create ‚Äòmodules‚Äô of co-expressed genes which often reflect functionally similar groups of genes. These ‚Äòmodules‚Äô can then be compared across conditions or in a time-course experiment to identify any biologically relevant pathway or network information.\nYou can visualize co-expression clustering using heatmaps, which should be viewed as suggestive only; serious classification of genes needs better methods.\nThe way the tools perform clustering is by taking the entire expression matrix and computing pair-wise co-expression values. A network is then generated from which we explore the topology to make inferences on gene co-regulation. The WGCNA package (in R) is one example of a more sophisticated method for co-expression clustering.\n\n\n\n\n\ng:Profiler - http://biit.cs.ut.ee/gprofiler/index.cgi\nDAVID - http://david.abcc.ncifcrf.gov/tools.jsp\nclusterProfiler - http://bioconductor.org/packages/release/bioc/html/clusterProfiler.html\nGeneMANIA - http://www.genemania.org/\nGenePattern - http://www.broadinstitute.org/cancer/software/genepattern/ (need to register)\nWebGestalt - http://bioinfo.vanderbilt.edu/webgestalt/ (need to register)\nAmiGO - http://amigo.geneontology.org/amigo\nReviGO (visualizing GO analysis, input is GO terms) - http://revigo.irb.hr/\nWGCNA - https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/\nGSEA - http://software.broadinstitute.org/gsea/index.jsp\nSPIA - https://www.bioconductor.org/packages/release/bioc/html/SPIA.html\nGAGE/Pathview - http://www.bioconductor.org/packages/release/bioc/html/gage.html\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/08c_FA_GSEA.html#other-tools",
    "href": "develop/rmd/08c_FA_GSEA.html#other-tools",
    "title": "Functional class scoring",
    "section": "",
    "text": "It is possible to supply your own gene set GMT file, such as a GMT for MSigDB using special clusterProfiler functions as shown below:\n\n\nCode\n# DO NOT RUN\nBiocManager::install(\"GSEABase\")\nlibrary(GSEABase)\n\n# Load in GMT file of gene sets (we downloaded from the Broad Institute for MSigDB)\n\nc2 &lt;- read.gmt(\"/data/c2.cp.v6.0.entrez.gmt.txt\")\n\nmsig &lt;- GSEA(foldchanges, TERM2GENE=c2, verbose=FALSE)\n\nmsig_df &lt;- data.frame(msig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPathway topology analysis often takes into account gene interaction information along with the fold changes and adjusted p-values from differential expression analysis to identify dysregulated pathways. Depending on the tool, pathway topology tools explore how genes interact with each other (e.g.¬†activation, inhibition, phosphorylation, ubiquitination, etc.) to determine the pathway-level statistics. Pathway topology-based methods utilize the number and type of interactions between gene product (our DE genes) and other gene products to infer gene function or pathway association.\nFor instance, the SPIA (Signaling Pathway Impact Analysis) tool can be used to integrate the lists of differentially expressed genes, their fold changes, and pathway topology to identify affected pathways.\n\n\n\nCo-expression clustering is often used to identify genes of novel pathways or networks by grouping genes together based on similar trends in expression. These tools are useful in identifying genes in a pathway, when their participation in a pathway and/or the pathway itself is unknown. These tools cluster genes with similar expression patterns to create ‚Äòmodules‚Äô of co-expressed genes which often reflect functionally similar groups of genes. These ‚Äòmodules‚Äô can then be compared across conditions or in a time-course experiment to identify any biologically relevant pathway or network information.\nYou can visualize co-expression clustering using heatmaps, which should be viewed as suggestive only; serious classification of genes needs better methods.\nThe way the tools perform clustering is by taking the entire expression matrix and computing pair-wise co-expression values. A network is then generated from which we explore the topology to make inferences on gene co-regulation. The WGCNA package (in R) is one example of a more sophisticated method for co-expression clustering."
  },
  {
    "objectID": "develop/rmd/08c_FA_GSEA.html#extra-resources-for-functional-analysis",
    "href": "develop/rmd/08c_FA_GSEA.html#extra-resources-for-functional-analysis",
    "title": "Functional class scoring",
    "section": "",
    "text": "g:Profiler - http://biit.cs.ut.ee/gprofiler/index.cgi\nDAVID - http://david.abcc.ncifcrf.gov/tools.jsp\nclusterProfiler - http://bioconductor.org/packages/release/bioc/html/clusterProfiler.html\nGeneMANIA - http://www.genemania.org/\nGenePattern - http://www.broadinstitute.org/cancer/software/genepattern/ (need to register)\nWebGestalt - http://bioinfo.vanderbilt.edu/webgestalt/ (need to register)\nAmiGO - http://amigo.geneontology.org/amigo\nReviGO (visualizing GO analysis, input is GO terms) - http://revigo.irb.hr/\nWGCNA - https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/\nGSEA - http://software.broadinstitute.org/gsea/index.jsp\nSPIA - https://www.bioconductor.org/packages/release/bioc/html/SPIA.html\nGAGE/Pathview - http://www.bioconductor.org/packages/release/bioc/html/gage.html\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/07a_DEA.html",
    "href": "develop/rmd/07a_DEA.html",
    "title": "Gene-level differential expression analysis with DESeq2",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 60 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1.  Explain the different steps involved in running `DESeq()`\n2.  Examine size factors and undertand the source of differences\n3.  Inspect gene-level dispersion estimates\n4.  Recognize the importance of dispersion during differential expression analysis\nPreviously, we created the DESeq2 object using the appropriate design formula.\n\n\nCode\n# DO NOT RUN\n\n# Create dds object\ndds &lt;- DESeqDataSetFromTximport(txi,\n                                colData = meta %&gt;% column_to_rownames(\"sample\"), \n                                design = ~ condition)\n\n# Filter genes with 0 counts\nkeep &lt;- rowSums(counts(dds)) &gt; 0\ndds &lt;- dds[keep,]\n\n\nThen, to run the actual differential expression analysis, we use a single call to the function DESeq().\n\n\nCode\n## Run analysis\ndds &lt;- DESeq(dds)\n\n\nAnd with that we completed the entire workflow for the differential gene expression analysis with DESeq2! The DESeq() function performs a default analysis through the following steps:\n\nEstimation of size factors: estimateSizeFactors()\nEstimation of dispersion: estimateDispersions()\nNegative Binomial GLM fitting and Wald statistics: nbinomWaldTest()\n\n\n\n\n\n\n\n\n\n\nWe will be taking a detailed look at each of these steps to better understand how DESeq2 is performing the statistical analysis and what metrics we should examine to explore the quality of our analysis.\n\n\nThe first step in the differential expression analysis is to estimate the size factors, which is exactly what we already did to normalize the raw counts.\n\n\n\n\n\n\n\n\n\nDESeq2 will automatically estimate the size factors when performing the differential expression analysis. However, if you have already generated the size factors using estimateSizeFactors(), as we did earlier, then DESeq2 will use these values.\nTo normalize the count data, DESeq2 calculates size factors for each sample using the median of ratios method discussed previously in the count normalization lesson.\n\n\nLet‚Äôs take a quick look at size factor values we have for each sample:\n\n\nCode\n## Check the size factors\nsizeFactors(dds)\n\n\ncontrol_1 control_2 control_3 garlicum_2 garlicum_3 vampirium_1 vampirium_2 \n 1.1149694  0.9606733  0.7492240  1.5633640  0.9359695  1.2262649  1.1405026 \nvampirium_3 \n 0.6542030 \nThese numbers should be identical to those we generated initially when we had run the function estimateSizeFactors(dds). Take a look at the total number of reads for each sample:\n\n\nCode\n## Total number of raw counts per sample\ncolSums(counts(dds))\n\n\n!!! info ‚ÄúHow do the numbers correlate with the size factor?‚Äù\nWe see that the larger size factors correspond to the samples with higher sequencing depth, which makes sense, because to generate our normalized counts we need to divide the counts by the size factors. This accounts for the differences in sequencing depth between samples.\n\nNow take a look at the total depth after normalization using:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Total number of normalized counts per sample\ncolSums(counts(dds, normalized=T))\n```\n:::\n!!! info ‚ÄúHow do the values across samples compare with the total counts taken for each sample?‚Äù\nYou might have expected the counts to be the exact same across the samples after normalization. However, DESeq2 also accounts for RNA composition during the normalization procedure. By using the median ratio value for the size factor, DESeq2 should not be biased to a large number of counts sucked up by a few DE genes; however, this may lead to the size factors being quite different than what would be anticipated just based on sequencing depth.\n\n\n\n\nThe next step in the differential expression analysis is the estimation of gene-wise dispersions. Before we get into the details, we should have a good idea about what dispersion is referring to in DESeq2.\n\n\n\n\n\n\n\n\n\nIn RNA-seq count data, we know:\n\nTo determine differentially expressed genes, we need to identify genes that have significantly different mean expression between groups given the variation within the groups (between replicates).\nThe variation within group (between replicates) needs to account for the fact that variance increases with the mean expression, as shown in the plot below (each black dot is a gene).\n\n\n\n\n\n\n\n\n\n\nTo accurately identify DE genes, DESeq2 needs to account for the relationship between the variance and mean. We don‚Äôt want all of our DE genes to be genes with low counts because the variance is lower for lowly expressed genes.\nInstead of using variance as the measure of variation in the data (since variance correlates with gene expression level), DESeq2 uses a measure of variation called dispersion, which accounts for a gene‚Äôs variance and mean expression level. Dispersion is calculated by:\n\\(Var=\\mu+\\alpha*\\mu^2\\), where:\n\n\\(\\alpha\\) = dispersion\n\\(Var\\) = variance\n\\(\\mu\\) = mean\n\nWhich results in the following relationship:\n\n\n\n\nEffect on dispersion\n\n\n\n\nVariance increases\nDispersion increases\n\n\nMean expression increases\nDispersion decreases\n\n\n\nFor genes with moderate to high count values, the square root of dispersion will be equal to the coefficient of variation. So 0.01 dispersion means 10% variation around the mean expected across biological replicates. The dispersion estimates for genes with the same mean will differ only based on their variance. Therefore, the dispersion estimates reflect the variance in gene expression for a given mean value. In the plot below, each black dot is a gene, and the dispersion is plotted against the mean expression (across within-group replicates) for each gene.\n\n\n\n\n\n\n\n\n\n!!! info ‚ÄúHow does the dispersion relate to our model?‚Äù\nTo accurately model sequencing counts, we need to generate accurate estimates of within-group variation (variation between replicates of the same sample group) for each gene. With only a few (3-6) replicates per group, the **estimates of variation for each gene are often unreliable**. \n\nTo address this problem, DESeq2 **shares information across genes** to generate more accurate estimates of variation based on the mean expression level of the gene using a method called 'shrinkage'. **DESeq2 assumes that genes with similar expression levels should have similar dispersion.** \n!!! note ‚ÄúNote on estimating gene dispersion‚Äù\nDESeq2 estimates the dispersion for each gene separately, based on the gene's expression level (mean counts of within-group replicates) and variance.\n\n\n\nThe next step in the workflow is to fit a curve to the gene-wise dispersion estimates and then shrink those estimates towards the curve. The idea behind fitting a curve to the data is that different genes will have different scales of biological variability, but, across all genes, there will be a distribution of reasonable estimates of dispersion.\n\n\n\n\n\n\n\n\n\nThis curve is displayed as a red line in the figure below, which plots the estimate for the expected dispersion value for genes of a given expression strength. Each black dot is a gene with an associated mean expression level and maximum likelihood estimation (MLE) of the dispersion (Step 1).\n\n\n\n\n\n\n\n\n\nAfter we fit the curve to the estimates, we can shrink the gene-wise dispersion estimates toward the expected dispersion values. Thanks to the curve we can identify more accurately differentially expressed genes when sample sizes are small, and the strength of the shrinkage for each gene depends on:\n\nhow close gene dispersions are from the curve\nsample size (more samples = less shrinkage)\n\nThis shrinkage method is particularly important to reduce false positives in the differential expression analysis. Genes with low dispersion estimates are shrunken towards the curve, and the more accurate, higher shrunken values are output for fitting of the model and differential expression testing. These shrunken estimates represent the within-group variation that is needed to determine whether the gene expression across groups is significantly different.\nDispersion estimates that are slightly above the curve are also shrunk toward the curve for better dispersion estimation; however, genes with extremely high dispersion values are not. This is due to the likelihood that the gene does not follow the modeling assumptions and has higher variability than others for biological or technical reasons [1]. Shrinking the values toward the curve could result in false positives, so these values are not shrunken. These genes are shown surrounded by blue circles below.\n\n\n\n\n\n\n\n\n\nThis is a good plot to evaluate whether your data is a good fit for the DESeq2 model. You expect your data to generally scatter around the curve, with the dispersion decreasing with increasing mean expression levels. If you see a cloud or different shapes, then you might want to explore your data more to see if you have contamination or outlier samples. Note how much shrinkage you get across the whole range of means in the plotDispEsts() plot for any experiment with low degrees of freedom.\nExamples of worrisome dispersion plots are shown below:\nThe plot below shows a cloud of dispersion values, which do not generally follow the curve. This would suggest a bad fit of the data to the model.\n\n\n\n\n\n\n\n\n\nThe next plot shows the dispersion values initially decreasing, then increasing with larger expression values. The larger mean expression values should not have larger dispersions based on our expectations - we expect decreasing dispersions with increasing mean. This indicates that there is less variation for more highly expressed genes than expected. This also indicates that there could be an outlier sample or contamination present in our analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs take a look at the dispersion estimates for our Vampirium data:\n\n\nCode\n## Plot dispersion estimates\nplotDispEsts(dds)\n\n\n\n\n\n\n\n\n\n\n\n??? note ‚ÄúSince we have a small sample size, for many genes we see quite a bit of shrinkage. Do you think our data are a good fit for the model?‚Äù\nWe see a nice decrease in dispersion with increasing mean expression, which is good. We also see the dispersion estimates generally surround the curve, which is also expected. Overall, this plot looks good. We do see strong shrinkage, which is likely due to the fact that we have only two replicates for one of our sample groups. The more replicates we have, the less shrinkage is applied to the dispersion estimates, and the more DE genes are able to be identified. We would generally recommend having at least 4 biological replicates per condition for better estimation of variation.\n!!! question ‚ÄúExercise 1‚Äù\nGiven the dispersion plot below, would you have any concerns regarding the fit of your data to the model? \n\n- If not, what aspects of the plot makes you feel confident about your data?\n- If so, what are your concerns? What would you do to address them?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/07a_DEA/exercise_dispersion.png){fig-align='center'}\n:::\n:::\n??? question ‚ÄúSolution to Exercise 1‚Äù\nThe plot looks really bad. The fitted line (red) is above the final dispersions, and there is a \"rainfall\" coming from the cloud of dispersions. This could mean that our original data has some sort of contamination or outlier, or that the count matrix is not a \"raw\" count matrix.\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nSome materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website"
  },
  {
    "objectID": "develop/rmd/07a_DEA.html#step-1-estimate-size-factors",
    "href": "develop/rmd/07a_DEA.html#step-1-estimate-size-factors",
    "title": "Gene-level differential expression analysis with DESeq2",
    "section": "",
    "text": "The first step in the differential expression analysis is to estimate the size factors, which is exactly what we already did to normalize the raw counts.\n\n\n\n\n\n\n\n\n\nDESeq2 will automatically estimate the size factors when performing the differential expression analysis. However, if you have already generated the size factors using estimateSizeFactors(), as we did earlier, then DESeq2 will use these values.\nTo normalize the count data, DESeq2 calculates size factors for each sample using the median of ratios method discussed previously in the count normalization lesson.\n\n\nLet‚Äôs take a quick look at size factor values we have for each sample:\n\n\nCode\n## Check the size factors\nsizeFactors(dds)\n\n\ncontrol_1 control_2 control_3 garlicum_2 garlicum_3 vampirium_1 vampirium_2 \n 1.1149694  0.9606733  0.7492240  1.5633640  0.9359695  1.2262649  1.1405026 \nvampirium_3 \n 0.6542030 \nThese numbers should be identical to those we generated initially when we had run the function estimateSizeFactors(dds). Take a look at the total number of reads for each sample:\n\n\nCode\n## Total number of raw counts per sample\ncolSums(counts(dds))\n\n\n!!! info ‚ÄúHow do the numbers correlate with the size factor?‚Äù\nWe see that the larger size factors correspond to the samples with higher sequencing depth, which makes sense, because to generate our normalized counts we need to divide the counts by the size factors. This accounts for the differences in sequencing depth between samples.\n\nNow take a look at the total depth after normalization using:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Total number of normalized counts per sample\ncolSums(counts(dds, normalized=T))\n```\n:::\n!!! info ‚ÄúHow do the values across samples compare with the total counts taken for each sample?‚Äù\nYou might have expected the counts to be the exact same across the samples after normalization. However, DESeq2 also accounts for RNA composition during the normalization procedure. By using the median ratio value for the size factor, DESeq2 should not be biased to a large number of counts sucked up by a few DE genes; however, this may lead to the size factors being quite different than what would be anticipated just based on sequencing depth."
  },
  {
    "objectID": "develop/rmd/07a_DEA.html#step-2-estimate-gene-wise-dispersion",
    "href": "develop/rmd/07a_DEA.html#step-2-estimate-gene-wise-dispersion",
    "title": "Gene-level differential expression analysis with DESeq2",
    "section": "",
    "text": "The next step in the differential expression analysis is the estimation of gene-wise dispersions. Before we get into the details, we should have a good idea about what dispersion is referring to in DESeq2.\n\n\n\n\n\n\n\n\n\nIn RNA-seq count data, we know:\n\nTo determine differentially expressed genes, we need to identify genes that have significantly different mean expression between groups given the variation within the groups (between replicates).\nThe variation within group (between replicates) needs to account for the fact that variance increases with the mean expression, as shown in the plot below (each black dot is a gene).\n\n\n\n\n\n\n\n\n\n\nTo accurately identify DE genes, DESeq2 needs to account for the relationship between the variance and mean. We don‚Äôt want all of our DE genes to be genes with low counts because the variance is lower for lowly expressed genes.\nInstead of using variance as the measure of variation in the data (since variance correlates with gene expression level), DESeq2 uses a measure of variation called dispersion, which accounts for a gene‚Äôs variance and mean expression level. Dispersion is calculated by:\n\\(Var=\\mu+\\alpha*\\mu^2\\), where:\n\n\\(\\alpha\\) = dispersion\n\\(Var\\) = variance\n\\(\\mu\\) = mean\n\nWhich results in the following relationship:\n\n\n\n\nEffect on dispersion\n\n\n\n\nVariance increases\nDispersion increases\n\n\nMean expression increases\nDispersion decreases\n\n\n\nFor genes with moderate to high count values, the square root of dispersion will be equal to the coefficient of variation. So 0.01 dispersion means 10% variation around the mean expected across biological replicates. The dispersion estimates for genes with the same mean will differ only based on their variance. Therefore, the dispersion estimates reflect the variance in gene expression for a given mean value. In the plot below, each black dot is a gene, and the dispersion is plotted against the mean expression (across within-group replicates) for each gene.\n\n\n\n\n\n\n\n\n\n!!! info ‚ÄúHow does the dispersion relate to our model?‚Äù\nTo accurately model sequencing counts, we need to generate accurate estimates of within-group variation (variation between replicates of the same sample group) for each gene. With only a few (3-6) replicates per group, the **estimates of variation for each gene are often unreliable**. \n\nTo address this problem, DESeq2 **shares information across genes** to generate more accurate estimates of variation based on the mean expression level of the gene using a method called 'shrinkage'. **DESeq2 assumes that genes with similar expression levels should have similar dispersion.** \n!!! note ‚ÄúNote on estimating gene dispersion‚Äù\nDESeq2 estimates the dispersion for each gene separately, based on the gene's expression level (mean counts of within-group replicates) and variance."
  },
  {
    "objectID": "develop/rmd/07a_DEA.html#step-3-fit-and-shrink-gene-wise-dispersion-estimates",
    "href": "develop/rmd/07a_DEA.html#step-3-fit-and-shrink-gene-wise-dispersion-estimates",
    "title": "Gene-level differential expression analysis with DESeq2",
    "section": "",
    "text": "The next step in the workflow is to fit a curve to the gene-wise dispersion estimates and then shrink those estimates towards the curve. The idea behind fitting a curve to the data is that different genes will have different scales of biological variability, but, across all genes, there will be a distribution of reasonable estimates of dispersion.\n\n\n\n\n\n\n\n\n\nThis curve is displayed as a red line in the figure below, which plots the estimate for the expected dispersion value for genes of a given expression strength. Each black dot is a gene with an associated mean expression level and maximum likelihood estimation (MLE) of the dispersion (Step 1).\n\n\n\n\n\n\n\n\n\nAfter we fit the curve to the estimates, we can shrink the gene-wise dispersion estimates toward the expected dispersion values. Thanks to the curve we can identify more accurately differentially expressed genes when sample sizes are small, and the strength of the shrinkage for each gene depends on:\n\nhow close gene dispersions are from the curve\nsample size (more samples = less shrinkage)\n\nThis shrinkage method is particularly important to reduce false positives in the differential expression analysis. Genes with low dispersion estimates are shrunken towards the curve, and the more accurate, higher shrunken values are output for fitting of the model and differential expression testing. These shrunken estimates represent the within-group variation that is needed to determine whether the gene expression across groups is significantly different.\nDispersion estimates that are slightly above the curve are also shrunk toward the curve for better dispersion estimation; however, genes with extremely high dispersion values are not. This is due to the likelihood that the gene does not follow the modeling assumptions and has higher variability than others for biological or technical reasons [1]. Shrinking the values toward the curve could result in false positives, so these values are not shrunken. These genes are shown surrounded by blue circles below.\n\n\n\n\n\n\n\n\n\nThis is a good plot to evaluate whether your data is a good fit for the DESeq2 model. You expect your data to generally scatter around the curve, with the dispersion decreasing with increasing mean expression levels. If you see a cloud or different shapes, then you might want to explore your data more to see if you have contamination or outlier samples. Note how much shrinkage you get across the whole range of means in the plotDispEsts() plot for any experiment with low degrees of freedom.\nExamples of worrisome dispersion plots are shown below:\nThe plot below shows a cloud of dispersion values, which do not generally follow the curve. This would suggest a bad fit of the data to the model.\n\n\n\n\n\n\n\n\n\nThe next plot shows the dispersion values initially decreasing, then increasing with larger expression values. The larger mean expression values should not have larger dispersions based on our expectations - we expect decreasing dispersions with increasing mean. This indicates that there is less variation for more highly expressed genes than expected. This also indicates that there could be an outlier sample or contamination present in our analysis."
  },
  {
    "objectID": "develop/rmd/07a_DEA.html#vampirium-de-analysis-exploring-the-dispersion-estimates-and-assessing-model-fit",
    "href": "develop/rmd/07a_DEA.html#vampirium-de-analysis-exploring-the-dispersion-estimates-and-assessing-model-fit",
    "title": "Gene-level differential expression analysis with DESeq2",
    "section": "",
    "text": "Let‚Äôs take a look at the dispersion estimates for our Vampirium data:\n\n\nCode\n## Plot dispersion estimates\nplotDispEsts(dds)\n\n\n\n\n\n\n\n\n\n\n\n??? note ‚ÄúSince we have a small sample size, for many genes we see quite a bit of shrinkage. Do you think our data are a good fit for the model?‚Äù\nWe see a nice decrease in dispersion with increasing mean expression, which is good. We also see the dispersion estimates generally surround the curve, which is also expected. Overall, this plot looks good. We do see strong shrinkage, which is likely due to the fact that we have only two replicates for one of our sample groups. The more replicates we have, the less shrinkage is applied to the dispersion estimates, and the more DE genes are able to be identified. We would generally recommend having at least 4 biological replicates per condition for better estimation of variation.\n!!! question ‚ÄúExercise 1‚Äù\nGiven the dispersion plot below, would you have any concerns regarding the fit of your data to the model? \n\n- If not, what aspects of the plot makes you feel confident about your data?\n- If so, what are your concerns? What would you do to address them?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/07a_DEA/exercise_dispersion.png){fig-align='center'}\n:::\n:::\n??? question ‚ÄúSolution to Exercise 1‚Äù\nThe plot looks really bad. The fitted line (red) is above the final dispersions, and there is a \"rainfall\" coming from the cloud of dispersions. This could mean that our original data has some sort of contamination or outlier, or that the count matrix is not a \"raw\" count matrix.\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nSome materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website"
  },
  {
    "objectID": "develop/rmd/07c_DEA_visualization.html",
    "href": "develop/rmd/07c_DEA_visualization.html",
    "title": "Log Fold Shrinkage and DEA visualizations",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 75 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1.  Explain log fold change shrinkage\n2.  Setup results data for application of visualization techniques\n3.  Describe different data visualization useful for exploring results from a DGE analysis\n4.  Create a volcano plot and MA plot to evaluate relationship among DGE statistics\n5.  Create a heatmap to illustrate expression changes of differentially expressed genes\nIn the previous lessons, we learned about how to generate a table with Differentially Expressed genes.\n\n\nCode\n# DO NOT RUN\nres_tableCont &lt;- results(dds, contrast=contrast_cont, alpha = 0.05)\n\nhead(res_tableCont)\n\n\nThe problem with these fold change estimates is that they are not entirely accurate as they do not account for the large dispersion we observe with low read counts. To address this, the log2 fold changes need to be adjusted.\nTo generate more accurate log2 fold change (LFC) estimates, DESeq2 allows for the shrinkage of the LFC estimates toward zero when the information for a gene is low, which could include:\n\nLow counts\nHigh dispersion values\n\nLFC shrinkage uses information from all genes to generate more accurate estimates. Specifically, the distribution of LFC estimates for all genes is used (as a prior) to shrink the LFC estimates of genes with little information or high dispersion toward more likely (lower) LFC estimates.\n\n\n\n\n\n\n\n\n\nIllustration taken from the DESeq2 paper.\nIn the figure above, we have an example using two genes: green gene and purple gene. For each gene the expression values are plotted for each sample in the two different mouse strains (C57BL/6J and DBA/2J). Both genes have the same mean values for the two sample groups, but the green gene has little within-group variation while the purple gene has high levels of variation. For the green gene with low within-group variation, the unshrunken LFC estimate (vertex of the green solid line) is very similar to the shrunken LFC estimate (vertex of the green dotted line). However, LFC estimates for the purple gene are quite different due to the high dispersion. So even though two genes can have similar normalized count values, they can have differing degrees of LFC shrinkage. Notice the LFC estimates are shrunken toward the prior (black solid line).\nShrinking the log2 fold changes will not change the total number of genes that are identified as significantly differentially expressed. The shrinkage of fold change is to help with downstream assessment of results. For example, if you wanted to subset your significant genes based on fold change for further evaluation, you may want to use shrunken values. Additionally, for functional analysis tools such as GSEA which require fold change values as input you would want to provide shrunken values.\nTo generate the shrunken log2 fold change estimates, you have to run an additional step on your results object (that we will create below) with the function lfcShrink().\n\n\nCode\n# Save the unshrunken results to compare\nres_tableCont_unshrunken &lt;- res_tableCont\n\n# Apply fold change shrinkage\nres_tableCont &lt;- lfcShrink(dds, coef=\"condition_control_vs_vampirium\", type=\"apeglm\")\n\n\nDepending on the version of DESeq2 you are using the default method for shrinkage estimation will differ. The defaults can be changed by adding the argument type in the lfcShrink() function as we have above. For most recent versions of DESeq2, type=\"normal\" is the default and was the only method in earlier versions. It has been shown that in most situations there are alternative methods that have less bias than the ‚Äònormal‚Äô method, and therefore we chose to use apeglm.\n??? info ‚ÄúMore information on shrinkage‚Äù\nThe DESeq2 vignette has an [Extended section on shrinkage estimators](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#extended-section-on-shrinkage-estimators) that is quite useful.\n!!! note ‚Äúcontrast vs coef‚Äù\nWhen using the shrinkage method, rather than using the `contrast` argument you will be required to specify `coef`. Using contrast forms an expanded model matrix, treating all factor levels equally, and averages over all distances between all pairs of factor levels to estimate the prior. Using coef, means looking only at that column of the model matrix (so usually that would be one level against the reference level) and estimates the prior for that coefficient from the distribution of those MLE of coefficients. When using coef, the shrinkage depends on which level is chosen as reference.\n!!! note ‚ÄúHow do I know what to value to provide to the coef argument?‚Äù\nThe value you provide here needs to match identically to what is stored in the column header of the coefficients table. To see what values you have to work with you can use `resultsNames(dds)`.\n\n\nWhen we are working with large amounts of data it can be useful to display that information graphically. During this lesson, we will get you started with some basic and more advanced plots commonly used when exploring differential gene expression data, however, many of these plots can be helpful in visualizing other types of data as well.\nWe will be working with three different data objects we have already created in earlier lessons:\n\nMetadata for our samples (a dataframe): meta\nNormalized expression data for every gene in each of our samples (a matrix): normalized_counts\nTibble versions of the DESeq2 results we generated in the last lesson: res_tableCont_tb and res_tableGar_tb\n\nFirst, we already have a metadata tibble.\n\n\nCode\nmeta %&gt;% head()\n\n\nNext, let‚Äôs bring in the normalized_counts object with our gene names.\n\n\nCode\n# DESeq2 creates a matrix when you use the counts() function\n# First convert normalized_counts to a data frame and transfer the row names to a new column called \"gene\"\nnormalized_counts &lt;- counts(dds, normalized=T) %&gt;% \n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") \n\n\n\n\nA plot that can be useful to exploring our results is the MA plot. The MA plot shows the mean of the normalized counts versus the log2 fold changes for all genes tested. The genes that are significantly DE are colored to be easily identified (adjusted p-value &lt; 0.01 by default). This is also a great way to illustrate the effect of LFC shrinkage. The DESeq2 package offers a simple function to generate an MA plot.\nLet‚Äôs start with the unshrunken results:\n\n\nCode\n# MA plot using unshrunken fold changes\nplotMA(res_tableCont_unshrunken, ylim=c(-2,2))\n\n\nAnd now the shrunken results:\n\n\nCode\n# MA plot using shrunken fold changes\nplotMA(res_tableCont, ylim=c(-2,2))\n\n\nOn the left you have the unshrunken fold change values plotted and you can see the abundance of scatter for the lowly expressed genes. That is, many of these genes exhibit very high fold changes. After shrinkage, we see the fold changes are much smaller estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn addition to the comparison described above, this plot allows us to evaluate the magnitude of fold changes and how they are distributed relative to mean expression. Generally, we would expect to see significant genes across the full range of expression levels.\n!!! question ‚ÄúExercise 1‚Äù\nWhy are there genes with high mean and big log2 fold changes, but are not statistically significant?\"\n??? question ‚ÄúSolution to Exercise 1‚Äù\nBecause their expression values are very dispersed, and so their p-value will be very high.\n\n\n\nOne way to visualize results would be to simply plot the expression data for a handful of genes. We could do that by picking out specific genes of interest or selecting a range of genes.\nUsing DESeq2 plotCounts() to plot expression of a single gene\nTo pick out a specific gene of interest to plot, for example TSPAN7 (ID ENSG00000156298), we can use the plotCounts() from DESeq2. plotCounts() requires that the gene specified matches the original input to DESeq2.\n\n\nCode\n# Plot expression for single gene\nplotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\") \n\n\n\n\n\n\n\n\n\n\n\n!!! info\nThis DESeq2 function only allows for plotting the counts of a single gene at a time, and is not flexible regarding the appearance.\nUsing ggplot2 to plot expression of a single gene\nIf you wish to change the appearance of this plot, we can save the output of plotCounts() to a variable specifying the returnData=TRUE argument, then use ggplot():\n\n\nCode\n# Save plotcounts to a data frame object\nd &lt;- plotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\", returnData=TRUE)\n\n# What is the data output of plotCounts()?\nd %&gt;% head()\n\n\n\n\nCode\n# Plot the MOV10 normalized counts, using the samples (rownames(d) as labels)\nggplot(d, aes(x = condition, y = count, color = condition)) + \ngeom_point(position=position_jitter(w = 0.1,h = 0)) +\ngeom_text_repel(aes(label = rownames(d))) + \ntheme_bw() +\nggtitle(\"TSPAN7\") +\ntheme(plot.title = element_text(hjust = 0.5))\n\n\n!!! note\nNote that in the plot below (code above), we are using `geom_text_repel()` from the `ggrepel` package to label our individual points on the plot.\n\n\n\n\n\n\n\n\n\nCreate a translator from gene names to gene IDs\nWhile gene IDs are unique and traceable, it is hard for us humans to memorize a bunch of numbers. Let‚Äôs try to make a translator function that will give you possible gene IDs for a gene name. Then you can use this table to select one of the possible gene_IDs.\nThe function will take as input a vector of gene names of interest, the tx2gene dataframe and the dds object that you analyzed:\n\n\nCode\nlookup &lt;- function(gene_name, tx2gene, dds){\n  hits &lt;- tx2gene %&gt;% select(gene_symbol, gene_ID) %&gt;% distinct() %&gt;% \n    filter(gene_symbol %in% gene_name & gene_ID %in% rownames(dds))\n  return(hits)\n}\n\nlookup(gene_name = \"TSPAN7\", tx2gene = tx2gene, dds = dds)\n\n\nOn the other hand, we can add the information from our tx2gene table, since it has the gene name!\n\n\nCode\ntx2gene\n\n\nHowever, we see that the table has many duplicates per gene, due to the fact that a gene may have several transcripts IDs associated to it. Since our results table has gene IDs, it is important to remove transcript information and remove duplicated rows before merging the information.\nWe remove the transcript ID column and duplicated rows from the tx2gene table using tidyverse syntax. We merge the tables using the merge function, which has many options for merging. Since our tables have different column names for the gene ID variable, we provide them with the by.x and by.y arguments. We also want to keep all of our results, so we use the argument all.x as well.\n\n\nCode\nres_tableCont_tb &lt;- merge(res_tableCont_tb, tx2gene %&gt;% select(-transcript_ID) %&gt;% distinct(),\n                        by.x = \"gene\", by.y = \"gene_ID\", all.x = T)\n\nres_tableCont_tb\n\n\n\n\n\nIn addition to plotting subsets, we could also extract the normalized values of all the significant genes and plot a heatmap of their expression using pheatmap().\n\n\nCode\n# Extract normalized expression for significant genes from the vampirium and control samples\n# also get gene name\nnorm_Contsig &lt;- normalized_counts %&gt;% select(gene, starts_with(\"control\"), starts_with(\"vampirium\")) \n  dplyr::filter(gene %in% sigCont$gene)  \n\n\nNow let‚Äôs draw the heatmap using pheatmap:\n\n\nCode\n# Run pheatmap using the metadata data frame for the annotation\npheatmap(norm_Contsig %&gt;% column_to_rownames(\"gene\"), \n         cluster_rows = T, \n         show_rownames = F,\n         annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% select(\"condition\"), \n         border_color = NA, \n         fontsize = 10, \n         scale = \"row\", \n         fontsize_row = 10, \n         height = 20)\n\n\n\n\n\n\n\n\n\n\n\n!!! note\nThere are several additional arguments we have included in the function for aesthetics. One important one is `scale=\"row\"`, in which Z-scores are plotted, rather than the actual normalized count value. \n\nZ-scores are computed on a gene-by-gene basis by subtracting the mean and then dividing by the standard deviation. The Z-scores are computed **after the clustering**, so that it only affects the graphical aesthetics and the color visualization is improved.\n\n\n\nThe above plot would be great to look at the expression levels of a good number of genes, but for more of a global view there are other plots we can draw. A commonly used one is a volcano plot; in which you have the log transformed adjusted p-values plotted on the y-axis and log2 fold change values on the x-axis.\nTo generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here.\nTo generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here.\n\n\nCode\n## Obtain logical vector where TRUE values denote padj values &lt; 0.05 and fold change &gt; 1.5 in either direction\n\nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% \nmutate(threshold_Cont = padj &lt; 0.05 & abs(log2FoldChange) &gt;= 0.58)\n\n\nNow we can start plotting. The geom_point object is most applicable, as this is essentially a scatter plot:\n\n\nCode\n## Volcano plot\nggplot(res_tableCont_tb) + \n  geom_point(aes(x = log2FoldChange, y = -log10(padj), colour = threshold_Cont)) +\n  ggtitle(\"Control vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  #scale_y_continuous(limits = c(0,50)) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25)))  \n\n\n\n\n\n\n\n\n\n\n\nChecking the top DE genes\nThis is a great way to get an overall picture of what is going on, but what if we also wanted to know where the top 10 genes (lowest padj) in our DE list are located on this plot? We could label those dots with the gene name on the Volcano plot using geom_text_repel().\nFirst, we need to order the res_tableCont tibble by padj, and add an additional column to it, to include on those gene names we want to use to label the plot.\n\n\nCode\n## Create an empty column to indicate which genes to label\nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% arrange(padj)\n\n## Populate the gene labels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tableCont_tb$genelabels[1:10] &lt;- as.character(res_tableCont_tb$gene_symbol[1:10])\n\nhead(res_tableCont_tb)\n\n\nNext, we plot it as before with an additional layer for geom_text_repel() wherein we can specify the column of gene labels we just created.\n\n\nCode\nggplot(res_tableCont_tb, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold_Cont)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Control vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25))) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you use the DESeq2 tool for differential expression analysis, the package ‚ÄòDEGreport‚Äô can use the DESeq2 results output to make the top20 genes and the volcano plots generated above by writing a few lines of simple code. While you can customize the plots above, you may be interested in using the easier code. Below are examples of the code to create these plots:*\n\n\nCode\nDEGreport::degPlot(dds = dds, res = res, n = 20, xs = \"type\", group = \"condition\") # dds object is output from DESeq2\n\nDEGreport::degVolcano(\n    data.frame(res[,c(\"log2fold change\",\"padj\")]), # table - 2 columns\n    plot_text = data.frame(res[1:10,c(\"log2fold change\",\"padj\",\"id\")])) # table to add names\n    \n# Available in the newer version for R 3.4\nDEGreport::degPlotWide(dds = dds, genes = row.names(res)[1:5], group = \"condition\")\n\n\n!!! question ‚ÄúExercise 2‚Äù\nCreate visualizations of the results from your DEA between Garlicum samples and Vampirium samples.\n??? question ‚ÄúSolution to Exercise 2‚Äù\nOur Garlicum results are saved in this table. However, we will want to use the LFC shrunken values\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Normal results\nres_tableGar_unshrunken &lt;- res_tableGar\n\n# Shrunken values\nres_tableGar &lt;- lfcShrink(dds, coef=\"condition_garlicum_vs_vampirium\", type=\"apeglm\")\n```\n:::\n\n\nWe can plot a MAplot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotMA(res_tableGar, ylim=c(-2,2))\n```\n:::\n\n\nAnd continue with a volcano plot and a heatmap. But first, let's merge our results with with our tx2gene table:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_tableGar_tb &lt;- merge(res_tableGar_tb, tx2gene %&gt;% select(-transcript_ID) %&gt;% distinct(),\n                        by.x = \"gene\", by.y = \"gene_ID\", all.x = T)\n\nres_tableGar_tb\n```\n:::\n\n\nVolcano plot with top 10 genes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Create an empty column to indicate which genes to label\nres_tableGar_tb &lt;- res_tableGar_tb %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tableGar_tb &lt;- res_tableGar_tb %&gt;% arrange(padj)\n\n## Populate the gene labels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tableGar_tb$genelabels[1:10] &lt;- as.character(res_tableGar_tb$gene_symbol[1:10])\n\nhead(res_tableGar_tb)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(res_tableGar_tb, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold_Cont)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Garlicum vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25))) \n```\n:::\n\n\nHeatmap of DE genes for Vampirium vs Garlicum:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract normalized expression for significant genes from the vampirium and garlicum samples\n# also get gene name\nnorm_Garsig &lt;- normalized_counts %&gt;% select(gene, starts_with(\"garlicum\"), starts_with(\"vampirium\")) \n  dplyr::filter(gene %in% sigGar$gene)  \n```\n:::\n\n\nNow let's draw the heatmap using `pheatmap`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run pheatmap using the metadata data frame for the annotation\npheatmap(norm_Garsig %&gt;% column_to_rownames(\"gene\"), \n         cluster_rows = T, \n         show_rownames = F,\n         annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% select(\"condition\"), \n         border_color = NA, \n         fontsize = 10, \n         scale = \"row\", \n         fontsize_row = 10, \n         height = 20)\n```\n:::\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nMaterials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website"
  },
  {
    "objectID": "develop/rmd/07c_DEA_visualization.html#visualizing-the-results",
    "href": "develop/rmd/07c_DEA_visualization.html#visualizing-the-results",
    "title": "Log Fold Shrinkage and DEA visualizations",
    "section": "",
    "text": "When we are working with large amounts of data it can be useful to display that information graphically. During this lesson, we will get you started with some basic and more advanced plots commonly used when exploring differential gene expression data, however, many of these plots can be helpful in visualizing other types of data as well.\nWe will be working with three different data objects we have already created in earlier lessons:\n\nMetadata for our samples (a dataframe): meta\nNormalized expression data for every gene in each of our samples (a matrix): normalized_counts\nTibble versions of the DESeq2 results we generated in the last lesson: res_tableCont_tb and res_tableGar_tb\n\nFirst, we already have a metadata tibble.\n\n\nCode\nmeta %&gt;% head()\n\n\nNext, let‚Äôs bring in the normalized_counts object with our gene names.\n\n\nCode\n# DESeq2 creates a matrix when you use the counts() function\n# First convert normalized_counts to a data frame and transfer the row names to a new column called \"gene\"\nnormalized_counts &lt;- counts(dds, normalized=T) %&gt;% \n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") \n\n\n\n\nA plot that can be useful to exploring our results is the MA plot. The MA plot shows the mean of the normalized counts versus the log2 fold changes for all genes tested. The genes that are significantly DE are colored to be easily identified (adjusted p-value &lt; 0.01 by default). This is also a great way to illustrate the effect of LFC shrinkage. The DESeq2 package offers a simple function to generate an MA plot.\nLet‚Äôs start with the unshrunken results:\n\n\nCode\n# MA plot using unshrunken fold changes\nplotMA(res_tableCont_unshrunken, ylim=c(-2,2))\n\n\nAnd now the shrunken results:\n\n\nCode\n# MA plot using shrunken fold changes\nplotMA(res_tableCont, ylim=c(-2,2))\n\n\nOn the left you have the unshrunken fold change values plotted and you can see the abundance of scatter for the lowly expressed genes. That is, many of these genes exhibit very high fold changes. After shrinkage, we see the fold changes are much smaller estimates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn addition to the comparison described above, this plot allows us to evaluate the magnitude of fold changes and how they are distributed relative to mean expression. Generally, we would expect to see significant genes across the full range of expression levels.\n!!! question ‚ÄúExercise 1‚Äù\nWhy are there genes with high mean and big log2 fold changes, but are not statistically significant?\"\n??? question ‚ÄúSolution to Exercise 1‚Äù\nBecause their expression values are very dispersed, and so their p-value will be very high.\n\n\n\nOne way to visualize results would be to simply plot the expression data for a handful of genes. We could do that by picking out specific genes of interest or selecting a range of genes.\nUsing DESeq2 plotCounts() to plot expression of a single gene\nTo pick out a specific gene of interest to plot, for example TSPAN7 (ID ENSG00000156298), we can use the plotCounts() from DESeq2. plotCounts() requires that the gene specified matches the original input to DESeq2.\n\n\nCode\n# Plot expression for single gene\nplotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\") \n\n\n\n\n\n\n\n\n\n\n\n!!! info\nThis DESeq2 function only allows for plotting the counts of a single gene at a time, and is not flexible regarding the appearance.\nUsing ggplot2 to plot expression of a single gene\nIf you wish to change the appearance of this plot, we can save the output of plotCounts() to a variable specifying the returnData=TRUE argument, then use ggplot():\n\n\nCode\n# Save plotcounts to a data frame object\nd &lt;- plotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\", returnData=TRUE)\n\n# What is the data output of plotCounts()?\nd %&gt;% head()\n\n\n\n\nCode\n# Plot the MOV10 normalized counts, using the samples (rownames(d) as labels)\nggplot(d, aes(x = condition, y = count, color = condition)) + \ngeom_point(position=position_jitter(w = 0.1,h = 0)) +\ngeom_text_repel(aes(label = rownames(d))) + \ntheme_bw() +\nggtitle(\"TSPAN7\") +\ntheme(plot.title = element_text(hjust = 0.5))\n\n\n!!! note\nNote that in the plot below (code above), we are using `geom_text_repel()` from the `ggrepel` package to label our individual points on the plot.\n\n\n\n\n\n\n\n\n\nCreate a translator from gene names to gene IDs\nWhile gene IDs are unique and traceable, it is hard for us humans to memorize a bunch of numbers. Let‚Äôs try to make a translator function that will give you possible gene IDs for a gene name. Then you can use this table to select one of the possible gene_IDs.\nThe function will take as input a vector of gene names of interest, the tx2gene dataframe and the dds object that you analyzed:\n\n\nCode\nlookup &lt;- function(gene_name, tx2gene, dds){\n  hits &lt;- tx2gene %&gt;% select(gene_symbol, gene_ID) %&gt;% distinct() %&gt;% \n    filter(gene_symbol %in% gene_name & gene_ID %in% rownames(dds))\n  return(hits)\n}\n\nlookup(gene_name = \"TSPAN7\", tx2gene = tx2gene, dds = dds)\n\n\nOn the other hand, we can add the information from our tx2gene table, since it has the gene name!\n\n\nCode\ntx2gene\n\n\nHowever, we see that the table has many duplicates per gene, due to the fact that a gene may have several transcripts IDs associated to it. Since our results table has gene IDs, it is important to remove transcript information and remove duplicated rows before merging the information.\nWe remove the transcript ID column and duplicated rows from the tx2gene table using tidyverse syntax. We merge the tables using the merge function, which has many options for merging. Since our tables have different column names for the gene ID variable, we provide them with the by.x and by.y arguments. We also want to keep all of our results, so we use the argument all.x as well.\n\n\nCode\nres_tableCont_tb &lt;- merge(res_tableCont_tb, tx2gene %&gt;% select(-transcript_ID) %&gt;% distinct(),\n                        by.x = \"gene\", by.y = \"gene_ID\", all.x = T)\n\nres_tableCont_tb\n\n\n\n\n\nIn addition to plotting subsets, we could also extract the normalized values of all the significant genes and plot a heatmap of their expression using pheatmap().\n\n\nCode\n# Extract normalized expression for significant genes from the vampirium and control samples\n# also get gene name\nnorm_Contsig &lt;- normalized_counts %&gt;% select(gene, starts_with(\"control\"), starts_with(\"vampirium\")) \n  dplyr::filter(gene %in% sigCont$gene)  \n\n\nNow let‚Äôs draw the heatmap using pheatmap:\n\n\nCode\n# Run pheatmap using the metadata data frame for the annotation\npheatmap(norm_Contsig %&gt;% column_to_rownames(\"gene\"), \n         cluster_rows = T, \n         show_rownames = F,\n         annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% select(\"condition\"), \n         border_color = NA, \n         fontsize = 10, \n         scale = \"row\", \n         fontsize_row = 10, \n         height = 20)\n\n\n\n\n\n\n\n\n\n\n\n!!! note\nThere are several additional arguments we have included in the function for aesthetics. One important one is `scale=\"row\"`, in which Z-scores are plotted, rather than the actual normalized count value. \n\nZ-scores are computed on a gene-by-gene basis by subtracting the mean and then dividing by the standard deviation. The Z-scores are computed **after the clustering**, so that it only affects the graphical aesthetics and the color visualization is improved.\n\n\n\nThe above plot would be great to look at the expression levels of a good number of genes, but for more of a global view there are other plots we can draw. A commonly used one is a volcano plot; in which you have the log transformed adjusted p-values plotted on the y-axis and log2 fold change values on the x-axis.\nTo generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here.\nTo generate a volcano plot, we first need to have a column in our results data indicating whether or not the gene is considered differentially expressed based on p-adjusted values and we will include a log2fold change here.\n\n\nCode\n## Obtain logical vector where TRUE values denote padj values &lt; 0.05 and fold change &gt; 1.5 in either direction\n\nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% \nmutate(threshold_Cont = padj &lt; 0.05 & abs(log2FoldChange) &gt;= 0.58)\n\n\nNow we can start plotting. The geom_point object is most applicable, as this is essentially a scatter plot:\n\n\nCode\n## Volcano plot\nggplot(res_tableCont_tb) + \n  geom_point(aes(x = log2FoldChange, y = -log10(padj), colour = threshold_Cont)) +\n  ggtitle(\"Control vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  #scale_y_continuous(limits = c(0,50)) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25)))  \n\n\n\n\n\n\n\n\n\n\n\nChecking the top DE genes\nThis is a great way to get an overall picture of what is going on, but what if we also wanted to know where the top 10 genes (lowest padj) in our DE list are located on this plot? We could label those dots with the gene name on the Volcano plot using geom_text_repel().\nFirst, we need to order the res_tableCont tibble by padj, and add an additional column to it, to include on those gene names we want to use to label the plot.\n\n\nCode\n## Create an empty column to indicate which genes to label\nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tableCont_tb &lt;- res_tableCont_tb %&gt;% arrange(padj)\n\n## Populate the gene labels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tableCont_tb$genelabels[1:10] &lt;- as.character(res_tableCont_tb$gene_symbol[1:10])\n\nhead(res_tableCont_tb)\n\n\nNext, we plot it as before with an additional layer for geom_text_repel() wherein we can specify the column of gene labels we just created.\n\n\nCode\nggplot(res_tableCont_tb, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold_Cont)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Control vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25)))"
  },
  {
    "objectID": "develop/rmd/07c_DEA_visualization.html#other-visualization-tools",
    "href": "develop/rmd/07c_DEA_visualization.html#other-visualization-tools",
    "title": "Log Fold Shrinkage and DEA visualizations",
    "section": "",
    "text": "If you use the DESeq2 tool for differential expression analysis, the package ‚ÄòDEGreport‚Äô can use the DESeq2 results output to make the top20 genes and the volcano plots generated above by writing a few lines of simple code. While you can customize the plots above, you may be interested in using the easier code. Below are examples of the code to create these plots:*\n\n\nCode\nDEGreport::degPlot(dds = dds, res = res, n = 20, xs = \"type\", group = \"condition\") # dds object is output from DESeq2\n\nDEGreport::degVolcano(\n    data.frame(res[,c(\"log2fold change\",\"padj\")]), # table - 2 columns\n    plot_text = data.frame(res[1:10,c(\"log2fold change\",\"padj\",\"id\")])) # table to add names\n    \n# Available in the newer version for R 3.4\nDEGreport::degPlotWide(dds = dds, genes = row.names(res)[1:5], group = \"condition\")\n\n\n!!! question ‚ÄúExercise 2‚Äù\nCreate visualizations of the results from your DEA between Garlicum samples and Vampirium samples.\n??? question ‚ÄúSolution to Exercise 2‚Äù\nOur Garlicum results are saved in this table. However, we will want to use the LFC shrunken values\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Normal results\nres_tableGar_unshrunken &lt;- res_tableGar\n\n# Shrunken values\nres_tableGar &lt;- lfcShrink(dds, coef=\"condition_garlicum_vs_vampirium\", type=\"apeglm\")\n```\n:::\n\n\nWe can plot a MAplot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotMA(res_tableGar, ylim=c(-2,2))\n```\n:::\n\n\nAnd continue with a volcano plot and a heatmap. But first, let's merge our results with with our tx2gene table:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_tableGar_tb &lt;- merge(res_tableGar_tb, tx2gene %&gt;% select(-transcript_ID) %&gt;% distinct(),\n                        by.x = \"gene\", by.y = \"gene_ID\", all.x = T)\n\nres_tableGar_tb\n```\n:::\n\n\nVolcano plot with top 10 genes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Create an empty column to indicate which genes to label\nres_tableGar_tb &lt;- res_tableGar_tb %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tableGar_tb &lt;- res_tableGar_tb %&gt;% arrange(padj)\n\n## Populate the gene labels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tableGar_tb$genelabels[1:10] &lt;- as.character(res_tableGar_tb$gene_symbol[1:10])\n\nhead(res_tableGar_tb)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(res_tableGar_tb, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold_Cont)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Garlicum vs Vampirium\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25))) \n```\n:::\n\n\nHeatmap of DE genes for Vampirium vs Garlicum:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extract normalized expression for significant genes from the vampirium and garlicum samples\n# also get gene name\nnorm_Garsig &lt;- normalized_counts %&gt;% select(gene, starts_with(\"garlicum\"), starts_with(\"vampirium\")) \n  dplyr::filter(gene %in% sigGar$gene)  \n```\n:::\n\n\nNow let's draw the heatmap using `pheatmap`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Run pheatmap using the metadata data frame for the annotation\npheatmap(norm_Garsig %&gt;% column_to_rownames(\"gene\"), \n         cluster_rows = T, \n         show_rownames = F,\n         annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% select(\"condition\"), \n         border_color = NA, \n         fontsize = 10, \n         scale = \"row\", \n         fontsize_row = 10, \n         height = 20)\n```\n:::\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nMaterials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website"
  },
  {
    "objectID": "develop/rmd/06_exploratory_analysis.html",
    "href": "develop/rmd/06_exploratory_analysis.html",
    "title": "Exploratory analysis of bulk RNAseq",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 80 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1. Recognize the importance of methods for count data transformation\n2. Describe the PCA (principal component analysis) technique\n3. Interpret different examples of PCA plots\n4. Evaluate sample quality using PCA and hierarchical clustering\nThe next step in the DESeq2 workflow is QC, which includes sample-level and gene-level steps to perform QC checks on the count data to help us ensure that the samples/replicates look good.\n\n\n\n\n\n\n\n\n\n\n\nA useful initial step in an RNA-seq analysis is often to assess overall similarity between samples:\n\nWhich samples are similar to each other, which are different?\nDoes this fit to the expectation from the experiment‚Äôs design?\nWhat are the major sources of variation in the dataset?\n\nTo explore the similarity of our samples, we will be performing sample-level QC using Principal Component Analysis (PCA) and hierarchical clustering methods. These methods/tools allow us to check how similar the replicates are to each other (clustering) and to make sure that the experimental condition is the major source of variation in the data. Sample-level QC can also help identify any samples behaving like outliers; we can further explore any potential outliers to determine whether they need to be removed prior to DE analysis.\n\n\n\n\n\n\n\n\n\nThese unsupervised clustering methods are run using log2 transformed normalized counts. The log2 transformation improves the sample distances for clustering visualization, i.e., it reduces the impact of large outlier counts. Instead of using a classical log2 transform, we will be using the regularized log transform (rlog). This type of transformation helps to avoid any bias from the abundance of low-count genes.\n\n\n\n\n\n\n\n\n\nImage adapted from ‚ÄúBeginner‚Äôs guide to using the DESeq2 package‚Äù by Love, Anders and Huber, 2014\n!!! note\nMany common statistical methods for exploratory analysis of multidimensional data, especially methods for clustering and ordination (e. g., principal-component analysis and the like), work best for (at least approximately) homoskedastic data; this means that the variance of an observable quantity (i.e., here, the expression strength of a gene) does not depend on the mean. In RNA-Seq data, however, variance grows with the mean. For example, if one performs PCA directly on a matrix of normalized read counts, the result typically depends only on the few most strongly expressed genes because they show the largest absolute differences between samples. A simple and often used strategy to avoid this is to take the logarithm of the normalized count values plus a small pseudocount; however, now the genes with low counts tend to dominate the results because, due to the strong Poisson noise inherent to small count values, they show the strongest relative differences between samples.\n\n*As a solution, DESeq2 offers the regularized-logarithm transformation, or rlog for short. For genes with high counts, the rlog transformation differs not much from an ordinary log2 transformation. For genes with lower counts, however, the values are shrunken towards the genes‚Äô averages across all samples. Using an empirical Bayesian prior in the form of a ridge penality, this is done such that the rlog-transformed data are approximately homoskedastic.\"* - From the \"Beginner's guide to using the DESeq2 package\" by Love, Anders and Huber, 2014 (the [DESeq2 vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) is the updated version of this doc).\n!!! note\nThe [DESeq2 vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) suggests large datasets (100s of samples) to use the variance-stabilizing transformation (vst) instead of rlog for transformation of the counts, since the rlog function might take too long to run and the `vst()` function is faster with similar properties to rlog.\n\n\n\nPrincipal Component Analysis (PCA) is a technique used to represent and visualize the variation in a dataset of high dimensionality. The number of dimensions, d, in a dataset may be thought of as the number of variables it has, e.g., for an RNA-seq dataset with 20.000 different transcripts, d is 20.000. Principally, this means we would need a dimensional space of size d to fully represent that dataset. However, as we are only able to view and comprehend things in 1,2 or 3 dimensions, we would like to project this dataset into a lower dimensional space, a process called dimensionality reduction. This makes PCA is a very important technique used in the QC and analysis of both bulk and single-cell RNAseq data, specially because many their dimensions (transcripts) do not contain any information.\nTo better understand how it works, please go through this YouTube video from StatQuest that explains PCA). After you have gone through the video, please proceed with the interpretation section below.\n\n\nEssentially, if two samples have similar levels of expression for the genes that contribute significantly to the variation represented by a given PC (Principal Component), they will be plotted close together on the axis that represents that PC. Therefore, we would expect that biological replicates to have similar scores (because our expectation is that the same genes are changing) and cluster together. This is easiest to understand by visualizing some example PCA plots.\nWe have an example dataset and a few associated PCA plots below to get a feel for how to interpret them. The metadata for the experiment is displayed below. The main condition of interest is treatment.\n\n\n\n\n\n\n\n\n\nWhen visualizing on PC1 and PC2, we don‚Äôt see the samples separate by treatment, so we decide to explore other sources of variation present in the data. We hope that we have included all possible known sources of variation in our metadata table, and we can use these factors to color the PCA plot.\n\n\n\n\n\n\n\n\n\nWe start with the factor cage, but the cage factor does not seem to explain the variation on PC1 or PC2.\n\n\n\n\n\n\n\n\n\nThen, we color by the sex factor, which appears to separate samples on PC2. This is good information to take note of, as we can use it downstream to account for the variation due to sex in the model and regress it out.\n\n\n\n\n\n\n\n\n\nNext we explore the strain factor and find that it explains the variation on PC1.\n\n\n\n\n\n\n\n\n\nIt‚Äôs great that we have been able to identify the sources of variation for both PC1 and PC2. By accounting for it in our model, we should be able to detect more genes differentially expressed due to treatment.\nSomething worrisome about this plot is that we see two samples that do not cluster with the correct strain. This would indicate a likely sample swap and should be investigated to determine whether these samples are indeed the labeled strains. If we found there was a switch, we could swap the samples in the metadata. However, if we think they are labeled correctly or are unsure, we could just remove the samples from the dataset.\nStill we haven‚Äôt found if treatment is a major source of variation after strain and sex. So, we explore PC3 and PC4 to see if treatment is driving the variation represented by either of these PCs.\n\n\n\n\n\n\n\n\n\nWe find that the samples separate by treatment on PC3, and are optimistic about our DE analysis since our condition of interest, treatment, is separating on PC3 and we can regress out the variation driving PC1 and PC2.\nDepending on how much variation is explained by the first few principal components, you may want to explore more (i.e consider more components and plot pairwise combinations). Even if your samples do not separate clearly by the experimental variable, you may still get biologically relevant results from the DE analysis. If you are expecting very small effect sizes, then it‚Äôs possible the signal is drowned out by extraneous sources of variation. In situations where you can identify those sources, it is important to account for these in your model, as it provides more power to the tool for detecting DE genes.\n\n\n\n\nHierarchical clustering is another method for identifying correlation patterns in a dataset and potential sample outliers. A heatmap displays the correlation of gene expression for all pairwise combinations of samples in the dataset. The hierarchical tree along the axes indicates which samples are more similar to each other, i.e.¬†cluster together. The color blocks at the top indicate substructure in the data, and you would expect to see your replicates cluster together as a block for each sample group. Our expectation would be that the samples cluster together similar to the groupings we‚Äôve observed in the PCA plot.\n\n\n\n\n\n\n\n\n\n!!! note\nIn the example above, we see a clustering of wild-type (Wt) and knock-down (KD) cell line samples and we **would be quite concerned** that the 'Wt_3' and 'KD_3' samples are not clustering with the other replicates. Furthermore, since the majority of genes are not differentially expressed, we observe that the samples generally have high correlations with each other (values higher than 0.80). In this case, samples with correlations below 0.80 may indicate an outlier in your data and/or sample contamination. N.B It is important to stress that these is no universal cut-off for what is a good/bad correlation/distance score, it depends on the particular dataset.\n\n\n\nNow that we have a good understanding of the QC steps normally employed for RNA-seq, let‚Äôs implement them for the Vampirium dataset we are going to be working with.\n\n\nTo improve the distances/clustering for the PCA and hierarchical clustering visualization methods, we need to moderate the variance across the mean by applying the rlog transformation to the normalized counts.\n!!! warning ‚ÄúNote on transformed normalized counts‚Äù\nThe rlog transformation of the normalized counts is only necessary for these visualization methods during this quality assessment. **We will not be using these transformed counts for determining differential expression**.\n\n\nCode\n### Transform counts for data visualization\nrld &lt;- rlog(dds, blind=TRUE)\n\n\nThe blind=TRUE argument is to make sure that the rlog() function does not take our sample groups into account - i.e.¬†does the transformation in an unbiased manner. When performing quality assessment, it is important to include this option. The DESeq2 vignette has more details about this.\nThe rlog() function returns a DESeqTransform object, another type of DESeq-specific object. The reason you don‚Äôt just get a matrix of transformed values is because all of the parameters (i.e.¬†size factors) that went into computing the rlog transform are stored in that object. We use this object to plot the PCA and hierarchical clustering figures for quality assessment.\n!!! note ‚ÄúPerformance of rlog vs vst‚Äù\nThe `rlog()` function can be a bit slow when you have e.g. &gt; 20 samples. In these situations the `vst()` function is much faster and performs a similar transformation appropriate for use with `plotPCA()`. It's typically just a few seconds with `vst()` due to optimizations and the nature of the transformation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Transform counts for data visualization\nvsd &lt;- vst(dds, blind = TRUE)\n```\n:::\n\n\n\nWe are now ready for the QC steps, let‚Äôs start with PCA!\nDESeq2 has a built-in function for generating PCA plots using ggplot2 under the hood. This is great because it saves us having to type out lines of code and having to fiddle with the different ggplot2 layers. In addition, it takes the rlog object as an input directly, hence saving us the trouble of extracting the relevant information from it.\nThe function plotPCA() requires two arguments as input: a DESeqTransform object and the ‚Äúintgroup‚Äù (interesting group), i.e.¬†the name of the column in our metadata that has information about the experimental sample groups.\n\n\nCode\n### Plot PCA \nplotPCA(rld, intgroup=\"condition\")\n\n\n\n\n\n\n\n\n\n\n\n!!! question ‚ÄúExercise 1‚Äù\nBy default `plotPCA()` uses the *top 500 most variable genes*. You can change this by adding the `ntop=` argument and specifying how many of the genes you want the function to consider. For example, try 1000 genes. Did the plot change a lot?\n??? question ‚ÄúSolution to Exercise 1‚Äù\nUsing the 1000 most variable genes, the plot does not change a lot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotPCA(rld, intgroup=\"condition\", ntop = 1000)\n```\n:::\n\n\nWhat about 2000 genes? It seems that the PCs have a bit different % variance explained:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotPCA(rld, intgroup=\"condition\", ntop = 2000)\n```\n:::\n\n\nWhat about all genes? Not much change either!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotPCA(rld, intgroup=\"condition\", ntop = nrow(rld)) #nrow is number of rows and it equals to all genes!\n```\n:::\n\n\nAs you can see, most of the info comes from the top most variable genes. Since PCs are capturing the variation of our data, adding genes that are hardly variable makes any difference to the plot.\n!!! question ‚ÄúExercise 2‚Äù\n1. What does the above plot tell you about the similarity of samples?\n2. Does it fit the expectation from the experimental design?\n3. What do you think the %variance information (in the axes titles) tell you about the data in the context of the PCA?\n??? question ‚ÄúSolutions to Exercise 2‚Äù\n1. It shows that our replicates are very close to each other, and each group far from each other!\n2. Yes, which is great!\n3. It tells us how much is captured by each PC. In our case, PC1 already captures the differences between our conditions!\n\n\nThe plotPCA() function will only return the values for PC1 and PC2. If you would like to explore the additional PCs in your data or if you would like to identify genes that contribute most to the PCs, you can use the prcomp() function. For example, to plot any of the PCs we could run the following code:\n\n\nCode\n# Input is a matrix of log transformed values\nrld_mat &lt;- assay(rld) # extract rlog count matrix\npca &lt;- prcomp(t(rld_mat)) # perform PCA on the transposed (t) matrix of data \n\n\nTo see what the PCA object contains we can use again the attributes() function.\n\n\nCode\nattributes(pca)\n\n\nYou can check the ?prcomp() for more information. The most important variables are: - sdev: standard deviation explained by each PC. - rotation: contribution of each gene to each PC. - x: PC values for each sample (we use this values for our plots).\nWe can create a new object that contains all our metadata information and the PC values.\n\n\nCode\ndf &lt;- cbind(meta, pca$x) # Create data frame with metadata and PC3 and PC4 values for input to ggplot\n\n\n\n\nCode\n# ggplot with info for all PCAs\nggplot(df) + geom_point(aes(x=PC3, y=PC4, color = condition))\n\n\nIf you want to add PC variation information to the plot we can fetch it using the summary() function and take the second row:\n\n\nCode\nsummary(pca)\npca_var &lt;- summary(pca)$importance[2,] # second row is stored in the object \"importance\"\npca_var &lt;- round(pca_var * 100, digits = 2) # make it percentage and round to 2 digits\n\n\nFinally, we can add it to our plot\n\n\nCode\nggplot(df) + geom_point(aes(x=PC3, y=PC4, color = condition)) + \n  xlab(paste0(\"PC3: \",pca_var[\"PC3\"], \"% variance\")) + \n  ylab(paste0(\"PC4: \",pca_var[\"PC4\"], \"% variance\")) \n\n\n\n\nCode\nknitr::include_graphics(\"./img/06_exploratory_analysis/custom_PCA.png\")\n\n\n\n\n\n\nThere is no built-in function in DESeq2 for plotting the heatmap for displaying the pairwise correlation or distances between all the samples and the hierarchical clustering information; we will use the pheatmap() function from the pheatmap package. This function cannot use the DESeqTransform object as input, but requires a matrix or dataframe. So, the first thing to do is retrieve that information from the rld object using a function called assay().\n\n\nCode\n# Extract the rlog matrix from the object\nrld_mat &lt;- assay(rld)    \n\n\nNext, we need to compute the distances values for all the samples. We can do this using the dist function:\n\n\nCode\nsampleDists &lt;- dist(t(rld_mat)) # Distances are computed by rows, so we need to transpose (t) the matrix\nsampleDistMatrix &lt;- as.matrix(sampleDists)\n\n\nLet‚Äôs take a look at the column and row names of the correlation matrix.\n\n\nCode\n# Check the output of sampleDistMatrix, make note of the row names and column names\nhead(sampleDistMatrix)\n\nhead(meta)\n\n\nYou will notice that they match the names we have given our samples in the metadata data frame we started with. It is important that these match, so we can use the annotation argument below to plot a color block across the top. This block enables easy visualization of the hierarchical clustering.\nNow, let‚Äôs plot the heatmap!\n\n\nCode\n# Load pheatmap package\nlibrary(pheatmap)\n\npheatmap(sampleDistMatrix, annotation_col = meta %&gt;% column_to_rownames(\"sample\") %&gt;% \n           select(condition)) # we only want to use the condition column as an annotation\n\n\nWhen you plot using pheatmap() the hierarchical clustering information is used to place similar samples together and this information is represented by the tree structure along the axes. The annotation argument accepts a dataframe as input, in our case it is the meta data frame.\n\n\n\n\n\n\n\n\n\nOverall, we observe pretty high correlations across the board (&gt; 0.999) suggesting no outlying sample(s). Also, similar to the PCA plot you see the samples clustering together by sample group. Together, these plots suggest to us that the data are of good quality and we have the green light to proceed to differential expression analysis.\n!!! question ‚ÄúExercise 3‚Äù\nInstead of using distances between expression patterns, check the Pearson correlation between samples using `cor()`. Use your rlog count matrix as an input.\n??? question ‚ÄúSolution to Exercise 3‚Äù\nFirst, we get pearson correlations between our samples:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npearson &lt;- cor(rld_mat) \npearson\n```\n:::\n\n\nAs you can see, the result of cor for a matrix like `rld_mat` is a square matrix (rows are the same as columns). Each value is the Pearson correlation between a row and a column.\n\nThen, we create the plot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npheatmap(pearson, annotation_col = meta %&gt;% column_to_rownames(\"sample\") %&gt;% \n           dplyr::select(condition)) # we only want to use the condition column as an annotation\n```\n:::\n\n\nThere are many arguments and options for the pheatmap() function. You could, for example, change the color scale used, remove the dendograms, avoid clustering or even scale the values per row or per column.\n\n\nCode\nlibrary(RColorBrewer)\nheat.colors &lt;- brewer.pal(6, \"Blues\") # Colors from the RColorBrewer package (only 6)\nheat.colors &lt;- colorRampPalette(heat.colors)(100) # Interpolate 100 colors\n\n\n\n\nCode\npheatmap(sampleDistMatrix, annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"), \n         color = heat.colors, border_color=NA, fontsize = 10, \n         fontsize_row = 10, height=20)\n\n\nYou can check all the colors that RColorBrewer offers by using the following command: display.brewer.all()\n\n\nCode\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/06_exploratory_analysis.html#sample-level-qc",
    "href": "develop/rmd/06_exploratory_analysis.html#sample-level-qc",
    "title": "Exploratory analysis of bulk RNAseq",
    "section": "",
    "text": "A useful initial step in an RNA-seq analysis is often to assess overall similarity between samples:\n\nWhich samples are similar to each other, which are different?\nDoes this fit to the expectation from the experiment‚Äôs design?\nWhat are the major sources of variation in the dataset?\n\nTo explore the similarity of our samples, we will be performing sample-level QC using Principal Component Analysis (PCA) and hierarchical clustering methods. These methods/tools allow us to check how similar the replicates are to each other (clustering) and to make sure that the experimental condition is the major source of variation in the data. Sample-level QC can also help identify any samples behaving like outliers; we can further explore any potential outliers to determine whether they need to be removed prior to DE analysis.\n\n\n\n\n\n\n\n\n\nThese unsupervised clustering methods are run using log2 transformed normalized counts. The log2 transformation improves the sample distances for clustering visualization, i.e., it reduces the impact of large outlier counts. Instead of using a classical log2 transform, we will be using the regularized log transform (rlog). This type of transformation helps to avoid any bias from the abundance of low-count genes.\n\n\n\n\n\n\n\n\n\nImage adapted from ‚ÄúBeginner‚Äôs guide to using the DESeq2 package‚Äù by Love, Anders and Huber, 2014\n!!! note\nMany common statistical methods for exploratory analysis of multidimensional data, especially methods for clustering and ordination (e. g., principal-component analysis and the like), work best for (at least approximately) homoskedastic data; this means that the variance of an observable quantity (i.e., here, the expression strength of a gene) does not depend on the mean. In RNA-Seq data, however, variance grows with the mean. For example, if one performs PCA directly on a matrix of normalized read counts, the result typically depends only on the few most strongly expressed genes because they show the largest absolute differences between samples. A simple and often used strategy to avoid this is to take the logarithm of the normalized count values plus a small pseudocount; however, now the genes with low counts tend to dominate the results because, due to the strong Poisson noise inherent to small count values, they show the strongest relative differences between samples.\n\n*As a solution, DESeq2 offers the regularized-logarithm transformation, or rlog for short. For genes with high counts, the rlog transformation differs not much from an ordinary log2 transformation. For genes with lower counts, however, the values are shrunken towards the genes‚Äô averages across all samples. Using an empirical Bayesian prior in the form of a ridge penality, this is done such that the rlog-transformed data are approximately homoskedastic.\"* - From the \"Beginner's guide to using the DESeq2 package\" by Love, Anders and Huber, 2014 (the [DESeq2 vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) is the updated version of this doc).\n!!! note\nThe [DESeq2 vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) suggests large datasets (100s of samples) to use the variance-stabilizing transformation (vst) instead of rlog for transformation of the counts, since the rlog function might take too long to run and the `vst()` function is faster with similar properties to rlog."
  },
  {
    "objectID": "develop/rmd/06_exploratory_analysis.html#principal-component-analysis-pca",
    "href": "develop/rmd/06_exploratory_analysis.html#principal-component-analysis-pca",
    "title": "Exploratory analysis of bulk RNAseq",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a technique used to represent and visualize the variation in a dataset of high dimensionality. The number of dimensions, d, in a dataset may be thought of as the number of variables it has, e.g., for an RNA-seq dataset with 20.000 different transcripts, d is 20.000. Principally, this means we would need a dimensional space of size d to fully represent that dataset. However, as we are only able to view and comprehend things in 1,2 or 3 dimensions, we would like to project this dataset into a lower dimensional space, a process called dimensionality reduction. This makes PCA is a very important technique used in the QC and analysis of both bulk and single-cell RNAseq data, specially because many their dimensions (transcripts) do not contain any information.\nTo better understand how it works, please go through this YouTube video from StatQuest that explains PCA). After you have gone through the video, please proceed with the interpretation section below.\n\n\nEssentially, if two samples have similar levels of expression for the genes that contribute significantly to the variation represented by a given PC (Principal Component), they will be plotted close together on the axis that represents that PC. Therefore, we would expect that biological replicates to have similar scores (because our expectation is that the same genes are changing) and cluster together. This is easiest to understand by visualizing some example PCA plots.\nWe have an example dataset and a few associated PCA plots below to get a feel for how to interpret them. The metadata for the experiment is displayed below. The main condition of interest is treatment.\n\n\n\n\n\n\n\n\n\nWhen visualizing on PC1 and PC2, we don‚Äôt see the samples separate by treatment, so we decide to explore other sources of variation present in the data. We hope that we have included all possible known sources of variation in our metadata table, and we can use these factors to color the PCA plot.\n\n\n\n\n\n\n\n\n\nWe start with the factor cage, but the cage factor does not seem to explain the variation on PC1 or PC2.\n\n\n\n\n\n\n\n\n\nThen, we color by the sex factor, which appears to separate samples on PC2. This is good information to take note of, as we can use it downstream to account for the variation due to sex in the model and regress it out.\n\n\n\n\n\n\n\n\n\nNext we explore the strain factor and find that it explains the variation on PC1.\n\n\n\n\n\n\n\n\n\nIt‚Äôs great that we have been able to identify the sources of variation for both PC1 and PC2. By accounting for it in our model, we should be able to detect more genes differentially expressed due to treatment.\nSomething worrisome about this plot is that we see two samples that do not cluster with the correct strain. This would indicate a likely sample swap and should be investigated to determine whether these samples are indeed the labeled strains. If we found there was a switch, we could swap the samples in the metadata. However, if we think they are labeled correctly or are unsure, we could just remove the samples from the dataset.\nStill we haven‚Äôt found if treatment is a major source of variation after strain and sex. So, we explore PC3 and PC4 to see if treatment is driving the variation represented by either of these PCs.\n\n\n\n\n\n\n\n\n\nWe find that the samples separate by treatment on PC3, and are optimistic about our DE analysis since our condition of interest, treatment, is separating on PC3 and we can regress out the variation driving PC1 and PC2.\nDepending on how much variation is explained by the first few principal components, you may want to explore more (i.e consider more components and plot pairwise combinations). Even if your samples do not separate clearly by the experimental variable, you may still get biologically relevant results from the DE analysis. If you are expecting very small effect sizes, then it‚Äôs possible the signal is drowned out by extraneous sources of variation. In situations where you can identify those sources, it is important to account for these in your model, as it provides more power to the tool for detecting DE genes."
  },
  {
    "objectID": "develop/rmd/06_exploratory_analysis.html#hierarchical-clustering-heatmap",
    "href": "develop/rmd/06_exploratory_analysis.html#hierarchical-clustering-heatmap",
    "title": "Exploratory analysis of bulk RNAseq",
    "section": "",
    "text": "Hierarchical clustering is another method for identifying correlation patterns in a dataset and potential sample outliers. A heatmap displays the correlation of gene expression for all pairwise combinations of samples in the dataset. The hierarchical tree along the axes indicates which samples are more similar to each other, i.e.¬†cluster together. The color blocks at the top indicate substructure in the data, and you would expect to see your replicates cluster together as a block for each sample group. Our expectation would be that the samples cluster together similar to the groupings we‚Äôve observed in the PCA plot.\n\n\n\n\n\n\n\n\n\n!!! note\nIn the example above, we see a clustering of wild-type (Wt) and knock-down (KD) cell line samples and we **would be quite concerned** that the 'Wt_3' and 'KD_3' samples are not clustering with the other replicates. Furthermore, since the majority of genes are not differentially expressed, we observe that the samples generally have high correlations with each other (values higher than 0.80). In this case, samples with correlations below 0.80 may indicate an outlier in your data and/or sample contamination. N.B It is important to stress that these is no universal cut-off for what is a good/bad correlation/distance score, it depends on the particular dataset."
  },
  {
    "objectID": "develop/rmd/06_exploratory_analysis.html#vampirium-quality-assessment-and-exploratory-analysis-using-deseq2",
    "href": "develop/rmd/06_exploratory_analysis.html#vampirium-quality-assessment-and-exploratory-analysis-using-deseq2",
    "title": "Exploratory analysis of bulk RNAseq",
    "section": "",
    "text": "Now that we have a good understanding of the QC steps normally employed for RNA-seq, let‚Äôs implement them for the Vampirium dataset we are going to be working with.\n\n\nTo improve the distances/clustering for the PCA and hierarchical clustering visualization methods, we need to moderate the variance across the mean by applying the rlog transformation to the normalized counts.\n!!! warning ‚ÄúNote on transformed normalized counts‚Äù\nThe rlog transformation of the normalized counts is only necessary for these visualization methods during this quality assessment. **We will not be using these transformed counts for determining differential expression**.\n\n\nCode\n### Transform counts for data visualization\nrld &lt;- rlog(dds, blind=TRUE)\n\n\nThe blind=TRUE argument is to make sure that the rlog() function does not take our sample groups into account - i.e.¬†does the transformation in an unbiased manner. When performing quality assessment, it is important to include this option. The DESeq2 vignette has more details about this.\nThe rlog() function returns a DESeqTransform object, another type of DESeq-specific object. The reason you don‚Äôt just get a matrix of transformed values is because all of the parameters (i.e.¬†size factors) that went into computing the rlog transform are stored in that object. We use this object to plot the PCA and hierarchical clustering figures for quality assessment.\n!!! note ‚ÄúPerformance of rlog vs vst‚Äù\nThe `rlog()` function can be a bit slow when you have e.g. &gt; 20 samples. In these situations the `vst()` function is much faster and performs a similar transformation appropriate for use with `plotPCA()`. It's typically just a few seconds with `vst()` due to optimizations and the nature of the transformation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Transform counts for data visualization\nvsd &lt;- vst(dds, blind = TRUE)\n```\n:::\n\n\n\nWe are now ready for the QC steps, let‚Äôs start with PCA!\nDESeq2 has a built-in function for generating PCA plots using ggplot2 under the hood. This is great because it saves us having to type out lines of code and having to fiddle with the different ggplot2 layers. In addition, it takes the rlog object as an input directly, hence saving us the trouble of extracting the relevant information from it.\nThe function plotPCA() requires two arguments as input: a DESeqTransform object and the ‚Äúintgroup‚Äù (interesting group), i.e.¬†the name of the column in our metadata that has information about the experimental sample groups.\n\n\nCode\n### Plot PCA \nplotPCA(rld, intgroup=\"condition\")\n\n\n\n\n\n\n\n\n\n\n\n!!! question ‚ÄúExercise 1‚Äù\nBy default `plotPCA()` uses the *top 500 most variable genes*. You can change this by adding the `ntop=` argument and specifying how many of the genes you want the function to consider. For example, try 1000 genes. Did the plot change a lot?\n??? question ‚ÄúSolution to Exercise 1‚Äù\nUsing the 1000 most variable genes, the plot does not change a lot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotPCA(rld, intgroup=\"condition\", ntop = 1000)\n```\n:::\n\n\nWhat about 2000 genes? It seems that the PCs have a bit different % variance explained:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotPCA(rld, intgroup=\"condition\", ntop = 2000)\n```\n:::\n\n\nWhat about all genes? Not much change either!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotPCA(rld, intgroup=\"condition\", ntop = nrow(rld)) #nrow is number of rows and it equals to all genes!\n```\n:::\n\n\nAs you can see, most of the info comes from the top most variable genes. Since PCs are capturing the variation of our data, adding genes that are hardly variable makes any difference to the plot.\n!!! question ‚ÄúExercise 2‚Äù\n1. What does the above plot tell you about the similarity of samples?\n2. Does it fit the expectation from the experimental design?\n3. What do you think the %variance information (in the axes titles) tell you about the data in the context of the PCA?\n??? question ‚ÄúSolutions to Exercise 2‚Äù\n1. It shows that our replicates are very close to each other, and each group far from each other!\n2. Yes, which is great!\n3. It tells us how much is captured by each PC. In our case, PC1 already captures the differences between our conditions!\n\n\nThe plotPCA() function will only return the values for PC1 and PC2. If you would like to explore the additional PCs in your data or if you would like to identify genes that contribute most to the PCs, you can use the prcomp() function. For example, to plot any of the PCs we could run the following code:\n\n\nCode\n# Input is a matrix of log transformed values\nrld_mat &lt;- assay(rld) # extract rlog count matrix\npca &lt;- prcomp(t(rld_mat)) # perform PCA on the transposed (t) matrix of data \n\n\nTo see what the PCA object contains we can use again the attributes() function.\n\n\nCode\nattributes(pca)\n\n\nYou can check the ?prcomp() for more information. The most important variables are: - sdev: standard deviation explained by each PC. - rotation: contribution of each gene to each PC. - x: PC values for each sample (we use this values for our plots).\nWe can create a new object that contains all our metadata information and the PC values.\n\n\nCode\ndf &lt;- cbind(meta, pca$x) # Create data frame with metadata and PC3 and PC4 values for input to ggplot\n\n\n\n\nCode\n# ggplot with info for all PCAs\nggplot(df) + geom_point(aes(x=PC3, y=PC4, color = condition))\n\n\nIf you want to add PC variation information to the plot we can fetch it using the summary() function and take the second row:\n\n\nCode\nsummary(pca)\npca_var &lt;- summary(pca)$importance[2,] # second row is stored in the object \"importance\"\npca_var &lt;- round(pca_var * 100, digits = 2) # make it percentage and round to 2 digits\n\n\nFinally, we can add it to our plot\n\n\nCode\nggplot(df) + geom_point(aes(x=PC3, y=PC4, color = condition)) + \n  xlab(paste0(\"PC3: \",pca_var[\"PC3\"], \"% variance\")) + \n  ylab(paste0(\"PC4: \",pca_var[\"PC4\"], \"% variance\")) \n\n\n\n\nCode\nknitr::include_graphics(\"./img/06_exploratory_analysis/custom_PCA.png\")\n\n\n\n\n\n\nThere is no built-in function in DESeq2 for plotting the heatmap for displaying the pairwise correlation or distances between all the samples and the hierarchical clustering information; we will use the pheatmap() function from the pheatmap package. This function cannot use the DESeqTransform object as input, but requires a matrix or dataframe. So, the first thing to do is retrieve that information from the rld object using a function called assay().\n\n\nCode\n# Extract the rlog matrix from the object\nrld_mat &lt;- assay(rld)    \n\n\nNext, we need to compute the distances values for all the samples. We can do this using the dist function:\n\n\nCode\nsampleDists &lt;- dist(t(rld_mat)) # Distances are computed by rows, so we need to transpose (t) the matrix\nsampleDistMatrix &lt;- as.matrix(sampleDists)\n\n\nLet‚Äôs take a look at the column and row names of the correlation matrix.\n\n\nCode\n# Check the output of sampleDistMatrix, make note of the row names and column names\nhead(sampleDistMatrix)\n\nhead(meta)\n\n\nYou will notice that they match the names we have given our samples in the metadata data frame we started with. It is important that these match, so we can use the annotation argument below to plot a color block across the top. This block enables easy visualization of the hierarchical clustering.\nNow, let‚Äôs plot the heatmap!\n\n\nCode\n# Load pheatmap package\nlibrary(pheatmap)\n\npheatmap(sampleDistMatrix, annotation_col = meta %&gt;% column_to_rownames(\"sample\") %&gt;% \n           select(condition)) # we only want to use the condition column as an annotation\n\n\nWhen you plot using pheatmap() the hierarchical clustering information is used to place similar samples together and this information is represented by the tree structure along the axes. The annotation argument accepts a dataframe as input, in our case it is the meta data frame.\n\n\n\n\n\n\n\n\n\nOverall, we observe pretty high correlations across the board (&gt; 0.999) suggesting no outlying sample(s). Also, similar to the PCA plot you see the samples clustering together by sample group. Together, these plots suggest to us that the data are of good quality and we have the green light to proceed to differential expression analysis.\n!!! question ‚ÄúExercise 3‚Äù\nInstead of using distances between expression patterns, check the Pearson correlation between samples using `cor()`. Use your rlog count matrix as an input.\n??? question ‚ÄúSolution to Exercise 3‚Äù\nFirst, we get pearson correlations between our samples:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npearson &lt;- cor(rld_mat) \npearson\n```\n:::\n\n\nAs you can see, the result of cor for a matrix like `rld_mat` is a square matrix (rows are the same as columns). Each value is the Pearson correlation between a row and a column.\n\nThen, we create the plot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npheatmap(pearson, annotation_col = meta %&gt;% column_to_rownames(\"sample\") %&gt;% \n           dplyr::select(condition)) # we only want to use the condition column as an annotation\n```\n:::\n\n\nThere are many arguments and options for the pheatmap() function. You could, for example, change the color scale used, remove the dendograms, avoid clustering or even scale the values per row or per column.\n\n\nCode\nlibrary(RColorBrewer)\nheat.colors &lt;- brewer.pal(6, \"Blues\") # Colors from the RColorBrewer package (only 6)\nheat.colors &lt;- colorRampPalette(heat.colors)(100) # Interpolate 100 colors\n\n\n\n\nCode\npheatmap(sampleDistMatrix, annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"), \n         color = heat.colors, border_color=NA, fontsize = 10, \n         fontsize_row = 10, height=20)\n\n\nYou can check all the colors that RColorBrewer offers by using the following command: display.brewer.all()\n\n\nCode\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/08b_FA_overrepresentation.html",
    "href": "develop/rmd/08b_FA_overrepresentation.html",
    "title": "Functional Analysis for RNA-seq",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 120 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1.  Determine how functions are attributed to genes using Gene Ontology terms\n2.  Describe the theory of how functional enrichment tools yield statistically enriched functions or interactions\n3.  Discuss functional analysis using over-representation analysis, functional class scoring, and pathway topology methods\n4.  Identify popular functional analysis tools for over-representation analysis\nThe output of RNA-seq differential expression analysis is a list of significant differentially expressed genes (DEGs). To gain greater biological insight on the differentially expressed genes there are various analyses that can be done:\n\ndetermine whether there is enrichment of known biological functions, interactions, or pathways\nidentify genes‚Äô involvement in novel pathways or networks by grouping genes together based on similar trends\nuse global changes in gene expression by visualizing all genes being significantly up- or down-regulated in the context of external interaction data\n\nGenerally for any differential expression analysis, it is useful to interpret the resulting gene lists using freely available web- and R-based tools. While tools for functional analysis span a wide variety of techniques, they can loosely be categorized into three main types: over-representation analysis, functional class scoring, and pathway topology. See more here.\n\n\n\n\n\n\n\n\n\nThe goal of functional analysis is to provide biological insight, so it‚Äôs necessary to analyze our results in the context of our experimental hypothesis: What is the function of the genes dysregulated by Vampirium?. Therefore, based on the authors‚Äô hypothesis and observations, we may expect the enrichment of processes/pathways related to blood production and behaviour control, which we would need to validate experimentally.\n!!! note\nAll tools described below are great tools to validate experimental results and to make hypotheses. These tools suggest genes/pathways that may be involved with your condition of interest; however, you should NOT use these tools to make conclusions about the pathways involved in your experimental process. You will need to perform experimental validation of any suggested pathways.\n\n\nThere are a plethora of functional enrichment tools that perform some type of ‚Äúover-representation‚Äù analysis by querying databases containing information about gene function and interactions.\nThese databases typically categorize genes into groups (gene sets) based on shared function, or involvement in a pathway, or presence in a specific cellular location, or other categorizations, e.g.¬†functional pathways, etc. Essentially, known genes are binned into categories that have been consistently named (controlled vocabulary) based on how the gene has been annotated functionally. These categories are independent of any organism, however each organism has distinct categorizations available.\nTo determine whether any categories are over-represented, you can determine the probability of having the observed proportion of genes associated with a specific category in your gene list based on the proportion of genes associated with the same category in the background set (gene categorizations for the appropriate organism).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe statistical test that will determine whether something is actually over-represented is the Hypergeometric test.\n\n\nUsing the example of the first functional category above, hypergeometric distribution is a probability distribution that describes the probability of 25 genes (k) being associated with ‚ÄúFunctional category 1‚Äù, for all genes in our gene list (n=1000), from a population of all of the genes in entire genome (N=23,000) which contains 35 genes (K) associated with ‚ÄúFunctional category 1‚Äù [4].\n\n\n\n\n\n\n\n\n\nThe calculation of probability of k successes follows the formula:\n\\[Pr(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n-k}}{\\binom{N}{n}}\\]\n\n\n\n\n\n\n\n\n\nThis test will result in an adjusted p-value (after multiple test correction) for each category tested.\n\n\n\n\nOne of the most widely-used categorizations is the Gene Ontology (GO) established by the Gene Ontology project.\n!!! quote\n\"The Gene Ontology project is a collaborative effort to address the need for consistent descriptions of gene products across databases\". \nThe Gene Ontology Consortium maintains the GO terms, and these GO terms are incorporated into gene annotations in many of the popular repositories for animal, plant, and microbial genomes.\nTools that investigate enrichment of biological functions or interactions often use the Gene Ontology (GO) categorizations, i.e.¬†the GO terms to determine whether any have significantly modified representation in a given list of genes. Therefore, to best use and interpret the results from these functional analysis tools, it is helpful to have a good understanding of the GO terms themselves and their organization.\n\n\nTo describe the roles of genes and gene products, GO terms are organized into three independent controlled vocabularies (ontologies) in a species-independent manner:\n\nBiological process: refers to the biological role involving the gene or gene product, and could include ‚Äútranscription‚Äù, ‚Äúsignal transduction‚Äù, and ‚Äúapoptosis‚Äù. A biological process generally involves a chemical or physical change of the starting material or input.\nMolecular function: represents the biochemical activity of the gene product, such activities could include ‚Äúligand‚Äù, ‚ÄúGTPase‚Äù, and ‚Äútransporter‚Äù.\nCellular component: refers to the location in the cell of the gene product. Cellular components could include ‚Äúnucleus‚Äù, ‚Äúlysosome‚Äù, and ‚Äúplasma membrane‚Äù.\n\nEach GO term has a term name (e.g.¬†DNA repair) and a unique term accession number (GO:0005125), and a single gene product can be associated with many GO terms, since a single gene product ‚Äúmay function in several processes, contain domains that carry out diverse molecular functions, and participate in multiple alternative interactions with other proteins, organelles or locations in the cell‚Äù. See more here.\n\n\n\nSome gene products are well-researched, with vast quantities of data available regarding their biological processes and functions. However, other gene products have very little data available about their roles in the cell.\nFor example, the protein, ‚Äúp53‚Äù, would contain a wealth of information on it‚Äôs roles in the cell, whereas another protein might only be known as a ‚Äúmembrane-bound protein‚Äù with no other information available.\nThe GO ontologies were developed to describe and query biological knowledge with differing levels of information available. To do this, GO ontologies are loosely hierarchical, ranging from general, ‚Äòparent‚Äô, terms to more specific, ‚Äòchild‚Äô terms. The GO ontologies are ‚Äúloosely‚Äù hierarchical since ‚Äòchild‚Äô terms can have multiple ‚Äòparent‚Äô terms.\nSome genes with less information may only be associated with general ‚Äòparent‚Äô terms or no terms at all, while other genes with a lot of information be associated with many terms.\n\n\n\n\n\n\n\n\n\nFrom Nature Reviews Cancer 7, 23-34 (January 2007)\n!!! tip\nMore tips for working with GO can be found [here](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003343)\n\n\n\n\nWe will be using clusterProfiler to perform over-representation analysis on GO terms associated with our list of significant genes. The tool takes as input a significant gene list and a background gene list and performs statistical enrichment analysis using hypergeometric testing. The basic arguments allow the user to select the appropriate organism and GO ontology (BP, CC, MF) to test.\n\n\nTo run clusterProfiler GO over-representation analysis, we will change our gene names into Ensembl IDs, since the tool works a bit easier with the Ensembl IDs. Then load the following libraries:\n\n\nCode\n# Load libraries\nlibrary(DOSE)\nlibrary(pathview)\nlibrary(clusterProfiler)\nlibrary(org.Hs.eg.db)\n\n\nTo perform the over-representation analysis, we need a list of background genes and a list of significant genes. For our background dataset we will use all genes tested for differential expression (all genes in our results table). For our significant gene list we will use genes with p-adjusted values less than 0.05 (we could include a fold change threshold too if we have many DE genes).\n\n\nCode\n## Create background dataset for hypergeometric testing using all genes tested for significance in the results\nallCont_genes &lt;- dplyr::filter(res_ids, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n## Extract significant results\nsigCont &lt;- dplyr::filter(res_ids, padj &lt; 0.05 & !is.na(gene))\n\nsigCont_genes &lt;- sigCont %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n\nNow we can perform the GO enrichment analysis and save the results:\n\n\nCode\n## Run GO enrichment analysis \nego &lt;- enrichGO(gene = sigCont_genes, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n\n\n!!! note\nThe different organisms with annotation databases available to use with for the `OrgDb` argument can be found [here](./img/08b_FA_overrepresentation/orgdb_annotation_databases.png).\n\nAlso, the `keyType` argument may be coded as `keytype` in different versions of clusterProfiler.\n\nFinally, the `ont` argument can accept either \"BP\" (Biological Process), \"MF\" (Molecular Function), and \"CC\" (Cellular Component) subontologies, or \"ALL\" for all three.\n\n\nCode\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\ncluster_summary\n\nwrite.csv(cluster_summary, \"../Results/clusterProfiler_Cont-Vamp.csv\")\n\n\n{{ read_csv(‚Äò./assets/clusterProfiler_Cont-Vamp.csv‚Äô) }}\n!!! tip\nInstead of saving just the results summary from the `ego` object, it might also be beneficial to save the object itself. The `save()` function enables you to save it as a `.rda` file, e.g. `save(ego, file=\"results/ego.rda\")`. \nThe complementary function to `save()` is the function `load()`, e.g. \n\n`ego &lt;- load(file=\"results/ego.rda\")`.\n\nThis is a useful set of functions to know, since it enables one to preserve analyses at specific stages and reload them when needed. More information about these functions can be found [here](https://www.r-bloggers.com/load-save-and-rda-files/) & [here](http://rpubs.com/euclid/387778).\n!!! tip\nYou can also perform GO enrichment analysis with only the up or down regulated genes** in addition to performing it for the full list of significant genes. This can be useful to identify GO terms impacted in one direction and not the other. If very few genes are in any of these lists (&lt; 50, roughly) it may not be possible to get any significant GO terms.\n!!! question ‚ÄúExercise 1‚Äù\nCreate two new GO enrichment analyses one with UP and another for DOWN regulated genes for Control vs Vampirium.\n??? question ‚ÄúSolution to Exercise 1‚Äù\n1. Separate results into UP and DOWN regulated:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigCont_UP &lt;- sigCont %&gt;% filter(log2FoldChange &gt; 0)\nsigCont_DOWN &lt;- sigCont %&gt;% filter(log2FoldChange &lt; 0)\n```\n:::\n\n\n2. Run overrepresentation:\n\nUP regulated genes\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nego_UP &lt;- enrichGO(gene = sigCont_UP$gene, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n```\n:::\n\n\nDOWN regulated genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nego_DOWN &lt;- enrichGO(gene = sigCont_DOWN$gene, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n```\n:::\n\n\n3. Check results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ego_UP)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ego_DOWN)\n```\n:::\n\n\n\nclusterProfiler has a variety of options for viewing the over-represented GO terms. We will explore the dotplot, enrichment plot, and the category netplot.\nThe dotplot shows the number of genes associated with the first terms (size) and the p-adjusted values for these terms (color). This plot displays the top 20 GO terms by gene ratio (# genes related to GO term / total number of sig genes), not p-adjusted value.\n\n\nCode\n## Dotplot \ndotplot(ego, showCategory=20)\n\n\n\n\n\n\n\n\n\n\n\nThe next plot is the enrichment GO plot, which shows the relationship between the top 50 most significantly enriched GO terms (padj.), by grouping similar terms together. Before creating the plot, we will need to obtain the similarity between terms using the pairwise_termsim() function instructions for emapplot. In the enrichment plot, the color represents the p-values relative to the other displayed terms (brighter red is more significant), and the size of the terms represents the number of genes that are significant from our list.\n\n\nCode\n# Add similarity matrix to the termsim slot of enrichment result\nego &lt;- enrichplot::pairwise_termsim(ego)\n\n\n\n\nCode\n# Enrichmap clusters the 50 most significant (by padj) GO terms to visualize relationships between terms\nemapplot(ego, showCategory = 50)\n\n\n\n\n\n\n\n\n\n\n\nFinally, the category netplot shows the relationships between the genes associated with the top five most significant GO terms and the fold changes of the significant genes associated with these terms (color). The size of the GO terms reflects the pvalues of the terms, with the more significant terms being larger. This plot is particularly useful for hypothesis generation in identifying genes that may be important to several of the most affected processes.\n!!! warning\nYou may need to install the `ggnewscale` package using `install.packages(\"ggnewscale\")` for the `cnetplot()` function to work.\n\n\nCode\n# To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector\nCont_foldchanges &lt;- sigCont$log2FoldChange\n\nnames(Cont_foldchanges) &lt;- sigCont$gene\n\n\n\n\nCode\n# Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj)\ncnetplot(ego, \n         categorySize=\"pvalue\", \n         showCategory = 5, \n         foldChange=Cont_foldchanges, \n         vertex.label.font=6)\n\n\n!!! tip\nIf some of the high fold changes are getting drowned out due to a large range, you could set a maximum fold change value\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nCont_foldchanges &lt;- ifelse(Cont_foldchanges &gt; 2, 2, Cont_foldchanges)\nCont_foldchanges &lt;- ifelse(Cont_foldchanges &lt; -2, -2, Cont_foldchanges)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncnetplot(ego, \n     categorySize=\"pvalue\", \n     showCategory = 5, \n     foldChange=Cont_foldchanges, \n     vertex.label.font=6)\n```\n:::\n\n\n\n\n\n\n\n\n\n!!! tip\nIf you are interested in significant processes that are **not** among the top five, you can subset your `ego` dataset to only display these processes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Subsetting the ego results without overwriting original `ego` variable\nego2 &lt;- ego\n\nego2@result &lt;- ego@result[c(1,3,4,8,9),]\n\n# Plotting terms of interest\ncnetplot(ego2, \n     categorySize=\"pvalue\", \n     foldChange=Cont_foldchanges, \n     showCategory = 5, \n     vertex.label.font=6)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/08b_FA_overrepresentation/cnetplot-2.png){fig-align='center'}\n:::\n:::\n!!! question ‚ÄúExercise 2‚Äù\nRun a Disease Ontology (DO) overrepresentation analysis using the `enrichDO()` function. **NOTE** the arguments are very similar to the previous examples. \n\n- Do you find anything interesting?\n??? question ‚ÄúSolution to Exercise 2‚Äù\nFor the DO, we need to use entrez IDs, instead of gene IDs\n\nAll significantly regulated genes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedo &lt;- enrichDO(sigCont$entrez, qvalueCutoff = 1)\nhead(edo)\n```\n:::\n\n\nUP significantly regulated genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedo_UP &lt;- enrichDO(sigCont_UP$entrez)\nhead(edo_UP)\n```\n:::\n\n\nDOWN significantly regulated genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedo_DOWN &lt;- enrichDO(sigCont_DOWN$entrez)\nhead(edo_DOWN)\n```\n:::\n!!! question ‚ÄúExercise 3‚Äù\nRun an enrichment analysis on the results of the DEA for Garlicum vs Vampirium samples. Remember to use the annotated results!\n??? question ‚ÄúSolution to Exercise 3‚Äù\nLet's do a simple analysis as an example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Create background dataset for hypergeometric testing using all genes tested for significance in the results\nallGar_genes &lt;- dplyr::filter(res_ids_Gar, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n## Extract significant results\nsigGar &lt;- dplyr::filter(res_ids_Gar, padj &lt; 0.05 & !is.na(gene))\n\nsigGar_genes &lt;- sigGar %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n```\n:::\n\n\nNow we can perform the GO enrichment analysis and save the results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Run GO enrichment analysis \nego &lt;- enrichGO(gene = sigGar_genes, \n                universe = allGar_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\ncluster_summary\n\nwrite.csv(cluster_summary, \"../Results/clusterProfiler_Gar-Vamp.csv\")\n```\n:::\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/08b_FA_overrepresentation.html#over-representation-analysis",
    "href": "develop/rmd/08b_FA_overrepresentation.html#over-representation-analysis",
    "title": "Functional Analysis for RNA-seq",
    "section": "",
    "text": "There are a plethora of functional enrichment tools that perform some type of ‚Äúover-representation‚Äù analysis by querying databases containing information about gene function and interactions.\nThese databases typically categorize genes into groups (gene sets) based on shared function, or involvement in a pathway, or presence in a specific cellular location, or other categorizations, e.g.¬†functional pathways, etc. Essentially, known genes are binned into categories that have been consistently named (controlled vocabulary) based on how the gene has been annotated functionally. These categories are independent of any organism, however each organism has distinct categorizations available.\nTo determine whether any categories are over-represented, you can determine the probability of having the observed proportion of genes associated with a specific category in your gene list based on the proportion of genes associated with the same category in the background set (gene categorizations for the appropriate organism).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe statistical test that will determine whether something is actually over-represented is the Hypergeometric test.\n\n\nUsing the example of the first functional category above, hypergeometric distribution is a probability distribution that describes the probability of 25 genes (k) being associated with ‚ÄúFunctional category 1‚Äù, for all genes in our gene list (n=1000), from a population of all of the genes in entire genome (N=23,000) which contains 35 genes (K) associated with ‚ÄúFunctional category 1‚Äù [4].\n\n\n\n\n\n\n\n\n\nThe calculation of probability of k successes follows the formula:\n\\[Pr(X = k) = \\frac{\\binom{K}{k} \\binom{N - K}{n-k}}{\\binom{N}{n}}\\]\n\n\n\n\n\n\n\n\n\nThis test will result in an adjusted p-value (after multiple test correction) for each category tested."
  },
  {
    "objectID": "develop/rmd/08b_FA_overrepresentation.html#gene-ontology-project",
    "href": "develop/rmd/08b_FA_overrepresentation.html#gene-ontology-project",
    "title": "Functional Analysis for RNA-seq",
    "section": "",
    "text": "One of the most widely-used categorizations is the Gene Ontology (GO) established by the Gene Ontology project.\n!!! quote\n\"The Gene Ontology project is a collaborative effort to address the need for consistent descriptions of gene products across databases\". \nThe Gene Ontology Consortium maintains the GO terms, and these GO terms are incorporated into gene annotations in many of the popular repositories for animal, plant, and microbial genomes.\nTools that investigate enrichment of biological functions or interactions often use the Gene Ontology (GO) categorizations, i.e.¬†the GO terms to determine whether any have significantly modified representation in a given list of genes. Therefore, to best use and interpret the results from these functional analysis tools, it is helpful to have a good understanding of the GO terms themselves and their organization.\n\n\nTo describe the roles of genes and gene products, GO terms are organized into three independent controlled vocabularies (ontologies) in a species-independent manner:\n\nBiological process: refers to the biological role involving the gene or gene product, and could include ‚Äútranscription‚Äù, ‚Äúsignal transduction‚Äù, and ‚Äúapoptosis‚Äù. A biological process generally involves a chemical or physical change of the starting material or input.\nMolecular function: represents the biochemical activity of the gene product, such activities could include ‚Äúligand‚Äù, ‚ÄúGTPase‚Äù, and ‚Äútransporter‚Äù.\nCellular component: refers to the location in the cell of the gene product. Cellular components could include ‚Äúnucleus‚Äù, ‚Äúlysosome‚Äù, and ‚Äúplasma membrane‚Äù.\n\nEach GO term has a term name (e.g.¬†DNA repair) and a unique term accession number (GO:0005125), and a single gene product can be associated with many GO terms, since a single gene product ‚Äúmay function in several processes, contain domains that carry out diverse molecular functions, and participate in multiple alternative interactions with other proteins, organelles or locations in the cell‚Äù. See more here.\n\n\n\nSome gene products are well-researched, with vast quantities of data available regarding their biological processes and functions. However, other gene products have very little data available about their roles in the cell.\nFor example, the protein, ‚Äúp53‚Äù, would contain a wealth of information on it‚Äôs roles in the cell, whereas another protein might only be known as a ‚Äúmembrane-bound protein‚Äù with no other information available.\nThe GO ontologies were developed to describe and query biological knowledge with differing levels of information available. To do this, GO ontologies are loosely hierarchical, ranging from general, ‚Äòparent‚Äô, terms to more specific, ‚Äòchild‚Äô terms. The GO ontologies are ‚Äúloosely‚Äù hierarchical since ‚Äòchild‚Äô terms can have multiple ‚Äòparent‚Äô terms.\nSome genes with less information may only be associated with general ‚Äòparent‚Äô terms or no terms at all, while other genes with a lot of information be associated with many terms.\n\n\n\n\n\n\n\n\n\nFrom Nature Reviews Cancer 7, 23-34 (January 2007)\n!!! tip\nMore tips for working with GO can be found [here](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003343)"
  },
  {
    "objectID": "develop/rmd/08b_FA_overrepresentation.html#clusterprofiler",
    "href": "develop/rmd/08b_FA_overrepresentation.html#clusterprofiler",
    "title": "Functional Analysis for RNA-seq",
    "section": "",
    "text": "We will be using clusterProfiler to perform over-representation analysis on GO terms associated with our list of significant genes. The tool takes as input a significant gene list and a background gene list and performs statistical enrichment analysis using hypergeometric testing. The basic arguments allow the user to select the appropriate organism and GO ontology (BP, CC, MF) to test.\n\n\nTo run clusterProfiler GO over-representation analysis, we will change our gene names into Ensembl IDs, since the tool works a bit easier with the Ensembl IDs. Then load the following libraries:\n\n\nCode\n# Load libraries\nlibrary(DOSE)\nlibrary(pathview)\nlibrary(clusterProfiler)\nlibrary(org.Hs.eg.db)\n\n\nTo perform the over-representation analysis, we need a list of background genes and a list of significant genes. For our background dataset we will use all genes tested for differential expression (all genes in our results table). For our significant gene list we will use genes with p-adjusted values less than 0.05 (we could include a fold change threshold too if we have many DE genes).\n\n\nCode\n## Create background dataset for hypergeometric testing using all genes tested for significance in the results\nallCont_genes &lt;- dplyr::filter(res_ids, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n## Extract significant results\nsigCont &lt;- dplyr::filter(res_ids, padj &lt; 0.05 & !is.na(gene))\n\nsigCont_genes &lt;- sigCont %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n\nNow we can perform the GO enrichment analysis and save the results:\n\n\nCode\n## Run GO enrichment analysis \nego &lt;- enrichGO(gene = sigCont_genes, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n\n\n!!! note\nThe different organisms with annotation databases available to use with for the `OrgDb` argument can be found [here](./img/08b_FA_overrepresentation/orgdb_annotation_databases.png).\n\nAlso, the `keyType` argument may be coded as `keytype` in different versions of clusterProfiler.\n\nFinally, the `ont` argument can accept either \"BP\" (Biological Process), \"MF\" (Molecular Function), and \"CC\" (Cellular Component) subontologies, or \"ALL\" for all three.\n\n\nCode\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\ncluster_summary\n\nwrite.csv(cluster_summary, \"../Results/clusterProfiler_Cont-Vamp.csv\")\n\n\n{{ read_csv(‚Äò./assets/clusterProfiler_Cont-Vamp.csv‚Äô) }}\n!!! tip\nInstead of saving just the results summary from the `ego` object, it might also be beneficial to save the object itself. The `save()` function enables you to save it as a `.rda` file, e.g. `save(ego, file=\"results/ego.rda\")`. \nThe complementary function to `save()` is the function `load()`, e.g. \n\n`ego &lt;- load(file=\"results/ego.rda\")`.\n\nThis is a useful set of functions to know, since it enables one to preserve analyses at specific stages and reload them when needed. More information about these functions can be found [here](https://www.r-bloggers.com/load-save-and-rda-files/) & [here](http://rpubs.com/euclid/387778).\n!!! tip\nYou can also perform GO enrichment analysis with only the up or down regulated genes** in addition to performing it for the full list of significant genes. This can be useful to identify GO terms impacted in one direction and not the other. If very few genes are in any of these lists (&lt; 50, roughly) it may not be possible to get any significant GO terms.\n!!! question ‚ÄúExercise 1‚Äù\nCreate two new GO enrichment analyses one with UP and another for DOWN regulated genes for Control vs Vampirium.\n??? question ‚ÄúSolution to Exercise 1‚Äù\n1. Separate results into UP and DOWN regulated:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigCont_UP &lt;- sigCont %&gt;% filter(log2FoldChange &gt; 0)\nsigCont_DOWN &lt;- sigCont %&gt;% filter(log2FoldChange &lt; 0)\n```\n:::\n\n\n2. Run overrepresentation:\n\nUP regulated genes\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nego_UP &lt;- enrichGO(gene = sigCont_UP$gene, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n```\n:::\n\n\nDOWN regulated genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nego_DOWN &lt;- enrichGO(gene = sigCont_DOWN$gene, \n                universe = allCont_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n```\n:::\n\n\n3. Check results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ego_UP)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ego_DOWN)\n```\n:::\n\n\n\nclusterProfiler has a variety of options for viewing the over-represented GO terms. We will explore the dotplot, enrichment plot, and the category netplot.\nThe dotplot shows the number of genes associated with the first terms (size) and the p-adjusted values for these terms (color). This plot displays the top 20 GO terms by gene ratio (# genes related to GO term / total number of sig genes), not p-adjusted value.\n\n\nCode\n## Dotplot \ndotplot(ego, showCategory=20)\n\n\n\n\n\n\n\n\n\n\n\nThe next plot is the enrichment GO plot, which shows the relationship between the top 50 most significantly enriched GO terms (padj.), by grouping similar terms together. Before creating the plot, we will need to obtain the similarity between terms using the pairwise_termsim() function instructions for emapplot. In the enrichment plot, the color represents the p-values relative to the other displayed terms (brighter red is more significant), and the size of the terms represents the number of genes that are significant from our list.\n\n\nCode\n# Add similarity matrix to the termsim slot of enrichment result\nego &lt;- enrichplot::pairwise_termsim(ego)\n\n\n\n\nCode\n# Enrichmap clusters the 50 most significant (by padj) GO terms to visualize relationships between terms\nemapplot(ego, showCategory = 50)\n\n\n\n\n\n\n\n\n\n\n\nFinally, the category netplot shows the relationships between the genes associated with the top five most significant GO terms and the fold changes of the significant genes associated with these terms (color). The size of the GO terms reflects the pvalues of the terms, with the more significant terms being larger. This plot is particularly useful for hypothesis generation in identifying genes that may be important to several of the most affected processes.\n!!! warning\nYou may need to install the `ggnewscale` package using `install.packages(\"ggnewscale\")` for the `cnetplot()` function to work.\n\n\nCode\n# To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector\nCont_foldchanges &lt;- sigCont$log2FoldChange\n\nnames(Cont_foldchanges) &lt;- sigCont$gene\n\n\n\n\nCode\n# Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj)\ncnetplot(ego, \n         categorySize=\"pvalue\", \n         showCategory = 5, \n         foldChange=Cont_foldchanges, \n         vertex.label.font=6)\n\n\n!!! tip\nIf some of the high fold changes are getting drowned out due to a large range, you could set a maximum fold change value\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nCont_foldchanges &lt;- ifelse(Cont_foldchanges &gt; 2, 2, Cont_foldchanges)\nCont_foldchanges &lt;- ifelse(Cont_foldchanges &lt; -2, -2, Cont_foldchanges)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncnetplot(ego, \n     categorySize=\"pvalue\", \n     showCategory = 5, \n     foldChange=Cont_foldchanges, \n     vertex.label.font=6)\n```\n:::\n\n\n\n\n\n\n\n\n\n!!! tip\nIf you are interested in significant processes that are **not** among the top five, you can subset your `ego` dataset to only display these processes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Subsetting the ego results without overwriting original `ego` variable\nego2 &lt;- ego\n\nego2@result &lt;- ego@result[c(1,3,4,8,9),]\n\n# Plotting terms of interest\ncnetplot(ego2, \n     categorySize=\"pvalue\", \n     foldChange=Cont_foldchanges, \n     showCategory = 5, \n     vertex.label.font=6)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/08b_FA_overrepresentation/cnetplot-2.png){fig-align='center'}\n:::\n:::\n!!! question ‚ÄúExercise 2‚Äù\nRun a Disease Ontology (DO) overrepresentation analysis using the `enrichDO()` function. **NOTE** the arguments are very similar to the previous examples. \n\n- Do you find anything interesting?\n??? question ‚ÄúSolution to Exercise 2‚Äù\nFor the DO, we need to use entrez IDs, instead of gene IDs\n\nAll significantly regulated genes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedo &lt;- enrichDO(sigCont$entrez, qvalueCutoff = 1)\nhead(edo)\n```\n:::\n\n\nUP significantly regulated genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedo_UP &lt;- enrichDO(sigCont_UP$entrez)\nhead(edo_UP)\n```\n:::\n\n\nDOWN significantly regulated genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedo_DOWN &lt;- enrichDO(sigCont_DOWN$entrez)\nhead(edo_DOWN)\n```\n:::\n!!! question ‚ÄúExercise 3‚Äù\nRun an enrichment analysis on the results of the DEA for Garlicum vs Vampirium samples. Remember to use the annotated results!\n??? question ‚ÄúSolution to Exercise 3‚Äù\nLet's do a simple analysis as an example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Create background dataset for hypergeometric testing using all genes tested for significance in the results\nallGar_genes &lt;- dplyr::filter(res_ids_Gar, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n## Extract significant results\nsigGar &lt;- dplyr::filter(res_ids_Gar, padj &lt; 0.05 & !is.na(gene))\n\nsigGar_genes &lt;- sigGar %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n```\n:::\n\n\nNow we can perform the GO enrichment analysis and save the results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Run GO enrichment analysis \nego &lt;- enrichGO(gene = sigGar_genes, \n                universe = allGar_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Output results from GO analysis to a table\ncluster_summary &lt;- data.frame(ego)\ncluster_summary\n\nwrite.csv(cluster_summary, \"../Results/clusterProfiler_Gar-Vamp.csv\")\n```\n:::\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/05c_count_normalization.html",
    "href": "develop/rmd/05c_count_normalization.html",
    "title": "Count normalization with DESeq2",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 40 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1. Explore different types of normalization methods\n2. Become familiar with the `DESeqDataSet` object\n3. Understand how to normalize counts using DESeq2\nThe first step in the DE analysis workflow is count normalization, which is necessary to make accurate comparisons of gene expression between samples.\n\n\n\n\n\n\n\n\n\nThe counts of mapped reads for each gene is proportional to the expression of RNA (‚Äúinteresting‚Äù) in addition to many other factors (‚Äúuninteresting‚Äù). Normalization is the process of scaling raw count values to account for the ‚Äúuninteresting‚Äù factors. In this way the expression levels are more comparable between and/or within samples.\nThe main factors often considered during normalization are:\n\nSequencing depth: Accounting for sequencing depth is necessary for comparison of gene expression between samples. In the example below, each gene appears to have doubled in expression in Sample A relative to Sample B, however this is a consequence of Sample A having double the sequencing depth.\n\n\n\n\n\n\n\n\n\n\n!!! note\nIn the figure above, each red rectangle represents a read aligned to a gene. Reads connected by dashed lines connect a read spanning an intron.\n\nGene length: Accounting for gene length is necessary for comparing expression between different genes within the same sample. In the example, Gene 2 and Gene 3 have similar levels of expression, but the number of reads mapped to Gene 2 would be many more than the number mapped to Gene 3 because Gene 2 is longer.\n\n\n\n\n\n\n\n\n\n\n\nGC-content: Genomic features such as GC-content may result in a read count biases, as GC-rich and GC-poor fragments are under-represented in RNAseq experiments. This under-representation is attributed to the fact that fragments with high and low GC-content are not adequately amplified in a standard high throughput sequencing protocol and, subsequently, that the fragments are difficult to align (correctly) to reference genome, i.e.¬†less unique, repeat regions, etc. (Benjamini & Speed, 2012 and Risso et al, 2011).\nRNA composition: A few highly differentially expressed genes between samples, differences in the number of genes expressed between samples, or presence of contamination can skew some types of normalization methods. Accounting for RNA composition is recommended for accurate comparison of expression between samples, and is particularly important when performing differential expression analyses Anders & Huber, 2010.\nIn the example, if we were to divide each sample by the total number of counts to normalize, the counts would be greatly skewed by the DE gene, which takes up most of the counts for Sample A, but not Sample B. Most other genes for Sample A would be divided by the larger number of total counts and appear to be less expressed than those same genes in Sample B.\n\n\n\n\n\n\n\n\n\n\n!!! tip\nWhile normalization is essential for differential expression analyses, it is also necessary for exploratory data analysis, visualization of data, and whenever you are exploring or comparing counts between or within samples.\n\n\nSeveral common normalization methods exist to account for these differences:\n{{ read_table(‚Äò./assets/normalization_methods.tsv‚Äô) }}\n\n\n\nWhile TPM and RPKM/FPKM normalization methods both account for sequencing depth and gene length, RPKM/FPKM are not recommended. The reason is that the normalized count values output by the RPKM/FPKM method are not comparable between samples.\nUsing RPKM/FPKM normalization, the total number of RPKM/FPKM normalized counts for each sample will be different. Therefore, you cannot compare the normalized counts for each gene equally between samples.\nRPKM-normalized counts table\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nXCR1\n5.5\n5.5\n\n\nWASHC1\n73.4\n21.8\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\nTotal RPKM-normalized counts\n1,000,000\n1,500,000\n\n\n\nFor example, in the table above, SampleA has a greater proportion of counts associated with XCR1 (5.5/1,000,000) than does sampleB (5.5/1,500,000) even though the RPKM count values are the same. Therefore, we cannot directly compare the counts for XCR1 (or any other gene) between sampleA and sampleB because the total number of normalized counts are different between samples.\n\n\n\nSince tools for differential expression analysis are comparing the counts between sample groups for the same gene, gene length does not need to be accounted for by the tool. However, sequencing depth and RNA composition do need to be taken into account.\nTo normalize for sequencing depth and RNA composition, DESeq2 uses the median of ratios method. On the user-end there is only one step, but on the back-end there are multiple steps involved, as described below.\n!!! note Note on the DESeq2 workflow\nThe steps below describe in detail some of the steps performed by DESeq2 when you run a single function to get DE genes. Basically, for a typical RNA-seq analysis, **you would not run these steps individually**.\n\n\nFor each gene, a pseudo-reference sample is created that is equal to the geometric mean across all samples.\n\n\n\ngene\nsampleA\nsampleB\npseudo-reference sample\n\n\n\n\nEF2A\n1489\n906\nsqrt(1489 * 906) = 1161.5\n\n\nABCD1\n22\n13\nsqrt(22 * 13) = 17.7\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\nFor every gene in a sample, the ratios (sample/ref) are calculated (as shown below). This is performed for each sample in the dataset. Since the majority of genes are not differentially expressed, the majority of genes in each sample should have similar ratios within the sample.\n\n\n\n\n\n\n\n\n\n\n\ngene\nsampleA\nsampleB\npseudo-reference sample\nratio of sampleA/ref\nratio of sampleB/ref\n\n\n\n\nEF2A\n1489\n906\n1161.5\n1489/1161.5 = 1.28\n906/1161.5 = 0.78\n\n\nABCD1\n22\n13\n16.9\n22/16.9 = 1.30\n13/16.9 = 0.77\n\n\nMEFV\n793\n410\n570.2\n793/570.2 = 1.39\n410/570.2 = 0.72\n\n\nBAG1\n76\n42\n56.5\n76/56.5 = 1.35\n42/56.5 = 0.74\n\n\nMOV10\n521\n1196\n883.7\n521/883.7 = 0.590\n1196/883.7 = 1.35\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\n\n\nThe median value (column-wise for the above table) of all ratios for a given sample is taken as the normalization factor (size factor) for that sample, as calculated below. Notice that the differentially expressed genes should not affect the median value:\nnormalization_factor_sampleA &lt;- median(c(1.28, 1.3, 1.39, 1.35, 0.59))\nnormalization_factor_sampleB &lt;- median(c(0.78, 0.77, 0.72, 0.74, 1.35))\nThe figure below illustrates the median value for the distribution of all gene ratios for a single sample (frequency is on the y-axis).\n\n\n\n\n\n\n\n\n\nThe median of ratios method assumes that not ALL genes are differentially expressed; therefore, the normalization factors should account for sequencing depth and RNA composition of the sample (large outlier genes will not represent the median ratio values). This method is robust to imbalance in up-/down-regulation and large numbers of differentially expressed genes.\n!!! warning\nUsually, these size factors are around 1, if you see large variations between samples it is important to take note since it might indicate the presence of extreme outliers.\n\n\n\nThis is performed by dividing each raw count value in a given sample by that sample‚Äôs normalization factor to generate normalized count values. This is performed for all count values (every gene in every sample). For example, if the median ratio for SampleA was 1.3 and the median ratio for SampleB was 0.77, you could calculate normalized counts as follows:\nSampleA median ratio = 1.3\nSampleB median ratio = 0.77\nRaw Counts\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nEF2A\n1489\n906\n\n\nABCD1\n22\n13\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nNormalized Counts\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nEF2A\n1489 / 1.3 = 1145.39\n906 / 0.77 = 1176.62\n\n\nABCD1\n22 / 1.3 = 16.92\n13 / 0.77 = 16.88\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n!!! warning\nPlease note that normalized count values are not whole numbers.\n!!! question ‚ÄúExercise 1‚Äù\nDetermine the normalized (median of ratios) counts for your gene of interest, PD1, given the raw counts and size factors below. \n\nNOTE: You will need to run the code below to generate the raw counts dataframe (PD1) and the size factor vector (size_factors), then use these objects to determine the normalized counts values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Raw counts for PD1\nPD1 &lt;- t(c(21, 58, 17, 97, 83, 10)) %&gt;% \nas_tibble() %&gt;%\nrename_all(~paste0(\"Sample\", 1:6))\n\n\n# Size factors for each sample\nsize_factors &lt;- c(1.32, 0.70, 1.04, 1.27, 1.11, 0.85)\n```\n:::\n??? question ‚ÄúSolution to Exercise 1‚Äù\nLet's check first what is PD1\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPD1\n```\n:::\n\n\nSince we have the size factors per sample, we only need to divide our PD1 counts by the size factors!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPD1/size_factors\n```\n:::\n\n\n\n\nNow that we know the theory of count normalization, we will normalize the counts for the Vampirium dataset using DESeq2. This requires a few steps:\n\nEnsure the row names of the metadata dataframe are present and in the same order as the column names of the counts dataframe.\nCreate a DESeqDataSet object\nGenerate the normalized counts\n\n\n\nWe should always make sure that we have sample names that match between the two files, and that the samples are in the right order. DESeq2 will output an error if this is not the case. Since we built our txi object from our metadata, everything should be OK.\n\n\nCode\n### Check that sample names match in both files\nall(colnames(txi$counts) %in% meta$sample)\nall(colnames(txi$counts) == meta$sample)\n\n\nIf your data did not match, you could use the match() function to rearrange them to be matching. match() function will take two arguments and find in which order the indexes of the second argument match the first argument.\n\n\nCode\na &lt;- c(\"a\",\"b\",\"c\")\nb &lt;- c(\"b\",\"c\",\"a\")\n\nreorder &lt;- match(a,b)\nreorder\n\nb[reorder]\n\n\n!!! question ‚ÄúExercise 2‚Äù\nSuppose we had sample names matching in the txi object and metadata file, but they were out of order. Write the line(s) of code required make the `meta_random` dataframe with rows ordered such that they were identical to the column names of the `txi`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# randomize metadata rownames\nmeta_random &lt;- meta[sample(1:nrow(meta)),]\n```\n:::\n??? question ‚ÄúSolution to Exercise 2‚Äù\nLet's check now meta_random order:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeta_random\n```\n:::\n\n\nWe can see that it is all scrambled. We want the rows of `meta_random` to be the same order as the columns of the `txi@counts` object (which is not, as you can see below):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Check that sample names match in both files\nall(colnames(txi$counts) %in% meta_random$sample) # are all samples in our metadata?\nall(colnames(txi$counts) == meta_random$sample) # are all samples in the same order?\n```\n:::\n\n\nLet's use the match function. First we find the order that `meta_random$sample` should be to match the columns of `txi@counts`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreorder &lt;- match(colnames(txi$counts),meta_random$sample)\nreorder\n```\n:::\n\n\nFinally, we change the order of the rows of meta_random:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeta_random &lt;- meta_random[reorder,]\nmeta_random\n```\n:::\n\n\nAnd confirm:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nall(colnames(txi$counts) == meta_random$sample) # are all samples in the same order?\n```\n:::\n\n\n\nBioconductor software packages often define and use a custom class within R for storing data (input data, intermediate data and also results). These custom data structures are similar to lists in that they can contain multiple different data types/structures within them. But, unlike lists they have pre-specified data slots, which hold specific types/classes of data. The data stored in these pre-specified slots can be accessed by using specific package-defined functions.\nLet‚Äôs start by creating the DESeqDataSet object, and then we can talk a bit more about what is stored inside it. To create the object, we will need the txi object and the metadata table as input (colData argument). We will also need to specify a design formula. The design formula specifies which column(s) of our metadata we want to use for statistical testing and modeling (more about that later!). For our dataset we only have one column we are interested in, which is condition. This column has three factor levels, which tells DESeq2 that for each gene we want to evaluate gene expression change with respect to these different levels.\nIt is very important to establish beforehand which sample type will be our ‚Äúbase‚Äù or ‚Äúreference‚Äù level. If nothing is changed, DESeq2 will assume that our reference samples will be the first sample type (in alphabetical order). You can check this using the factor() function.\n\n\nCode\nfactor(meta$condition)\n\n\nWhile in a normal experiment we would use control samples as our reference, in our case we are interested in both checking the differences between control vs.¬†vampirium and garlicum vs.¬†vampirium. Thus, it would be much more convinient to reorganize our factor base level to vampirium. We can do this also with the factor() function, using the levels = argument.\n\n\nCode\nmeta$condition = factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\nfactor(meta$condition)\n\n\nWe can see now that vampirium is the first factor! Meaning that it will be interpreted by DESeq as our reference sample type.\nOur count matrix input is stored in the txi list object. So we need to specify that using the DESeqDataSetFromTximport() function, which will extract the counts component and round the values to the nearest whole number.\n\n\nCode\n# colData argument requires rownames in order to assess matching sample names\n# meta is a tibble object from tidyverse, so we neeed to add rownames.\n# If you do not do this and the samples do not match, you will add wrong info!\n\ndds &lt;- DESeqDataSetFromTximport(txi,\n                                   colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)\n\n\n??? note ‚ÄúControl is not reference level warning‚Äù\nThe warning from the chunk before is telling us that we have setup our vampirium samples as reference, instead of control! This is exactly what we wanted.\n??? note ‚ÄúStarting from a traditional count matrix‚Äù\nIf you did not create pseudocounts, but a count matrix from aligned BAM files and tools such as `featurecounts`, you would want to use the `DESeqDataSetFromMatrix()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## DO NOT RUN!\n## Create DESeq2Dataset object from traditional count matrix\ndds &lt;- DESeqDataSetFromMatrix(countData = \"../Data/Vampirium_counts_traditional.tsv\", \n                          colData = meta %&gt;% column_to_rownames(\"sample\"), \n                          design = ~ condition)\n```\n:::\nYou can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts() (Note: we nested it within the View() function so that rather than getting printed in the console we can see it in the script editor) :\n\n\nCode\nView(counts(dds))\n\n\nAs we go through the workflow we will use the relevant functions to check what information gets stored inside our object.\nYou can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts():\n\n\nCode\nhead(counts(dds))\n\n\nAs we go through the workflow we will use the relevant functions to check what information gets stored inside our object.\n\n\nWhile it is not necessary to pre-filter low count genes before running the DESeq2 functions, there are two reasons which make pre-filtering useful:\n\nBy removing rows in which there are very few reads, we reduce the memory size of the dds data object, and we increase the speed of the transformation and testing functions within DESeq2.\nIt can also improve visualizations, as features with no information for differential expression are not plotted.\n\nHere we perform a minimal pre-filtering to keep only rows that have at least 10 reads total.\n\n\nCode\nkeep &lt;- rowSums(counts(dds)) &gt;= 10\ndds &lt;- dds[keep,]\n\n\n\n\n\n\nThe next step is to normalize the count data in order to be able to make fair gene comparisons between samples.\nTo perform the median of ratios method of normalization, DESeq2 has a single estimateSizeFactors() function that will generate size factors for us. We will use the function in the example below, but in a typical RNA-seq analysis this step is automatically performed by the DESeq() function, which we will see later.\n\n\nCode\ndds &lt;- estimateSizeFactors(dds)\n\n\nBy assigning the results back to the dds object we are filling in the slots of the DESeqDataSet object with the appropriate information. We can take a look at the normalization factor applied to each sample using:\n\n\nCode\nsizeFactors(dds)\n\n\nNow, to retrieve the normalized counts matrix from dds, we use the counts() function and add the argument normalized=TRUE.\n\n\nCode\nnormalized_counts &lt;- counts(dds, normalized=TRUE)\n\n\nWe can save this normalized data matrix to file for later use:\n\n\nCode\nwrite.table(normalized_counts, file=\"/work/Intro_to_bulkRNAseq/Results/normalized_counts.txt\", sep=\"\\t\", quote=F)\n\n\n!!! warning\nDESeq2 doesn't actually use normalized counts, rather it uses the raw counts and models the normalization inside the Generalized Linear Model (GLM). These normalized counts will be useful for downstream visualization of results, but cannot be used as input to DESeq2 or any other tools that perform differential expression analysis which use the negative binomial model.\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/05c_count_normalization.html#common-normalization-methods",
    "href": "develop/rmd/05c_count_normalization.html#common-normalization-methods",
    "title": "Count normalization with DESeq2",
    "section": "",
    "text": "Several common normalization methods exist to account for these differences:\n{{ read_table(‚Äò./assets/normalization_methods.tsv‚Äô) }}"
  },
  {
    "objectID": "develop/rmd/05c_count_normalization.html#rpkmfpkm-not-recommended",
    "href": "develop/rmd/05c_count_normalization.html#rpkmfpkm-not-recommended",
    "title": "Count normalization with DESeq2",
    "section": "",
    "text": "While TPM and RPKM/FPKM normalization methods both account for sequencing depth and gene length, RPKM/FPKM are not recommended. The reason is that the normalized count values output by the RPKM/FPKM method are not comparable between samples.\nUsing RPKM/FPKM normalization, the total number of RPKM/FPKM normalized counts for each sample will be different. Therefore, you cannot compare the normalized counts for each gene equally between samples.\nRPKM-normalized counts table\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nXCR1\n5.5\n5.5\n\n\nWASHC1\n73.4\n21.8\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\nTotal RPKM-normalized counts\n1,000,000\n1,500,000\n\n\n\nFor example, in the table above, SampleA has a greater proportion of counts associated with XCR1 (5.5/1,000,000) than does sampleB (5.5/1,500,000) even though the RPKM count values are the same. Therefore, we cannot directly compare the counts for XCR1 (or any other gene) between sampleA and sampleB because the total number of normalized counts are different between samples."
  },
  {
    "objectID": "develop/rmd/05c_count_normalization.html#deseq2-normalized-counts-median-of-ratios-method",
    "href": "develop/rmd/05c_count_normalization.html#deseq2-normalized-counts-median-of-ratios-method",
    "title": "Count normalization with DESeq2",
    "section": "",
    "text": "Since tools for differential expression analysis are comparing the counts between sample groups for the same gene, gene length does not need to be accounted for by the tool. However, sequencing depth and RNA composition do need to be taken into account.\nTo normalize for sequencing depth and RNA composition, DESeq2 uses the median of ratios method. On the user-end there is only one step, but on the back-end there are multiple steps involved, as described below.\n!!! note Note on the DESeq2 workflow\nThe steps below describe in detail some of the steps performed by DESeq2 when you run a single function to get DE genes. Basically, for a typical RNA-seq analysis, **you would not run these steps individually**.\n\n\nFor each gene, a pseudo-reference sample is created that is equal to the geometric mean across all samples.\n\n\n\ngene\nsampleA\nsampleB\npseudo-reference sample\n\n\n\n\nEF2A\n1489\n906\nsqrt(1489 * 906) = 1161.5\n\n\nABCD1\n22\n13\nsqrt(22 * 13) = 17.7\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\nFor every gene in a sample, the ratios (sample/ref) are calculated (as shown below). This is performed for each sample in the dataset. Since the majority of genes are not differentially expressed, the majority of genes in each sample should have similar ratios within the sample.\n\n\n\n\n\n\n\n\n\n\n\ngene\nsampleA\nsampleB\npseudo-reference sample\nratio of sampleA/ref\nratio of sampleB/ref\n\n\n\n\nEF2A\n1489\n906\n1161.5\n1489/1161.5 = 1.28\n906/1161.5 = 0.78\n\n\nABCD1\n22\n13\n16.9\n22/16.9 = 1.30\n13/16.9 = 0.77\n\n\nMEFV\n793\n410\n570.2\n793/570.2 = 1.39\n410/570.2 = 0.72\n\n\nBAG1\n76\n42\n56.5\n76/56.5 = 1.35\n42/56.5 = 0.74\n\n\nMOV10\n521\n1196\n883.7\n521/883.7 = 0.590\n1196/883.7 = 1.35\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\n\n\nThe median value (column-wise for the above table) of all ratios for a given sample is taken as the normalization factor (size factor) for that sample, as calculated below. Notice that the differentially expressed genes should not affect the median value:\nnormalization_factor_sampleA &lt;- median(c(1.28, 1.3, 1.39, 1.35, 0.59))\nnormalization_factor_sampleB &lt;- median(c(0.78, 0.77, 0.72, 0.74, 1.35))\nThe figure below illustrates the median value for the distribution of all gene ratios for a single sample (frequency is on the y-axis).\n\n\n\n\n\n\n\n\n\nThe median of ratios method assumes that not ALL genes are differentially expressed; therefore, the normalization factors should account for sequencing depth and RNA composition of the sample (large outlier genes will not represent the median ratio values). This method is robust to imbalance in up-/down-regulation and large numbers of differentially expressed genes.\n!!! warning\nUsually, these size factors are around 1, if you see large variations between samples it is important to take note since it might indicate the presence of extreme outliers.\n\n\n\nThis is performed by dividing each raw count value in a given sample by that sample‚Äôs normalization factor to generate normalized count values. This is performed for all count values (every gene in every sample). For example, if the median ratio for SampleA was 1.3 and the median ratio for SampleB was 0.77, you could calculate normalized counts as follows:\nSampleA median ratio = 1.3\nSampleB median ratio = 0.77\nRaw Counts\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nEF2A\n1489\n906\n\n\nABCD1\n22\n13\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nNormalized Counts\n\n\n\ngene\nsampleA\nsampleB\n\n\n\n\nEF2A\n1489 / 1.3 = 1145.39\n906 / 0.77 = 1176.62\n\n\nABCD1\n22 / 1.3 = 16.92\n13 / 0.77 = 16.88\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n!!! warning\nPlease note that normalized count values are not whole numbers.\n!!! question ‚ÄúExercise 1‚Äù\nDetermine the normalized (median of ratios) counts for your gene of interest, PD1, given the raw counts and size factors below. \n\nNOTE: You will need to run the code below to generate the raw counts dataframe (PD1) and the size factor vector (size_factors), then use these objects to determine the normalized counts values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Raw counts for PD1\nPD1 &lt;- t(c(21, 58, 17, 97, 83, 10)) %&gt;% \nas_tibble() %&gt;%\nrename_all(~paste0(\"Sample\", 1:6))\n\n\n# Size factors for each sample\nsize_factors &lt;- c(1.32, 0.70, 1.04, 1.27, 1.11, 0.85)\n```\n:::\n??? question ‚ÄúSolution to Exercise 1‚Äù\nLet's check first what is PD1\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPD1\n```\n:::\n\n\nSince we have the size factors per sample, we only need to divide our PD1 counts by the size factors!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPD1/size_factors\n```\n:::"
  },
  {
    "objectID": "develop/rmd/05c_count_normalization.html#count-normalization-of-the-vampirium-dataset-using-deseq2",
    "href": "develop/rmd/05c_count_normalization.html#count-normalization-of-the-vampirium-dataset-using-deseq2",
    "title": "Count normalization with DESeq2",
    "section": "",
    "text": "Now that we know the theory of count normalization, we will normalize the counts for the Vampirium dataset using DESeq2. This requires a few steps:\n\nEnsure the row names of the metadata dataframe are present and in the same order as the column names of the counts dataframe.\nCreate a DESeqDataSet object\nGenerate the normalized counts\n\n\n\nWe should always make sure that we have sample names that match between the two files, and that the samples are in the right order. DESeq2 will output an error if this is not the case. Since we built our txi object from our metadata, everything should be OK.\n\n\nCode\n### Check that sample names match in both files\nall(colnames(txi$counts) %in% meta$sample)\nall(colnames(txi$counts) == meta$sample)\n\n\nIf your data did not match, you could use the match() function to rearrange them to be matching. match() function will take two arguments and find in which order the indexes of the second argument match the first argument.\n\n\nCode\na &lt;- c(\"a\",\"b\",\"c\")\nb &lt;- c(\"b\",\"c\",\"a\")\n\nreorder &lt;- match(a,b)\nreorder\n\nb[reorder]\n\n\n!!! question ‚ÄúExercise 2‚Äù\nSuppose we had sample names matching in the txi object and metadata file, but they were out of order. Write the line(s) of code required make the `meta_random` dataframe with rows ordered such that they were identical to the column names of the `txi`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# randomize metadata rownames\nmeta_random &lt;- meta[sample(1:nrow(meta)),]\n```\n:::\n??? question ‚ÄúSolution to Exercise 2‚Äù\nLet's check now meta_random order:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeta_random\n```\n:::\n\n\nWe can see that it is all scrambled. We want the rows of `meta_random` to be the same order as the columns of the `txi@counts` object (which is not, as you can see below):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Check that sample names match in both files\nall(colnames(txi$counts) %in% meta_random$sample) # are all samples in our metadata?\nall(colnames(txi$counts) == meta_random$sample) # are all samples in the same order?\n```\n:::\n\n\nLet's use the match function. First we find the order that `meta_random$sample` should be to match the columns of `txi@counts`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreorder &lt;- match(colnames(txi$counts),meta_random$sample)\nreorder\n```\n:::\n\n\nFinally, we change the order of the rows of meta_random:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeta_random &lt;- meta_random[reorder,]\nmeta_random\n```\n:::\n\n\nAnd confirm:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nall(colnames(txi$counts) == meta_random$sample) # are all samples in the same order?\n```\n:::\n\n\n\nBioconductor software packages often define and use a custom class within R for storing data (input data, intermediate data and also results). These custom data structures are similar to lists in that they can contain multiple different data types/structures within them. But, unlike lists they have pre-specified data slots, which hold specific types/classes of data. The data stored in these pre-specified slots can be accessed by using specific package-defined functions.\nLet‚Äôs start by creating the DESeqDataSet object, and then we can talk a bit more about what is stored inside it. To create the object, we will need the txi object and the metadata table as input (colData argument). We will also need to specify a design formula. The design formula specifies which column(s) of our metadata we want to use for statistical testing and modeling (more about that later!). For our dataset we only have one column we are interested in, which is condition. This column has three factor levels, which tells DESeq2 that for each gene we want to evaluate gene expression change with respect to these different levels.\nIt is very important to establish beforehand which sample type will be our ‚Äúbase‚Äù or ‚Äúreference‚Äù level. If nothing is changed, DESeq2 will assume that our reference samples will be the first sample type (in alphabetical order). You can check this using the factor() function.\n\n\nCode\nfactor(meta$condition)\n\n\nWhile in a normal experiment we would use control samples as our reference, in our case we are interested in both checking the differences between control vs.¬†vampirium and garlicum vs.¬†vampirium. Thus, it would be much more convinient to reorganize our factor base level to vampirium. We can do this also with the factor() function, using the levels = argument.\n\n\nCode\nmeta$condition = factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\nfactor(meta$condition)\n\n\nWe can see now that vampirium is the first factor! Meaning that it will be interpreted by DESeq as our reference sample type.\nOur count matrix input is stored in the txi list object. So we need to specify that using the DESeqDataSetFromTximport() function, which will extract the counts component and round the values to the nearest whole number.\n\n\nCode\n# colData argument requires rownames in order to assess matching sample names\n# meta is a tibble object from tidyverse, so we neeed to add rownames.\n# If you do not do this and the samples do not match, you will add wrong info!\n\ndds &lt;- DESeqDataSetFromTximport(txi,\n                                   colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)\n\n\n??? note ‚ÄúControl is not reference level warning‚Äù\nThe warning from the chunk before is telling us that we have setup our vampirium samples as reference, instead of control! This is exactly what we wanted.\n??? note ‚ÄúStarting from a traditional count matrix‚Äù\nIf you did not create pseudocounts, but a count matrix from aligned BAM files and tools such as `featurecounts`, you would want to use the `DESeqDataSetFromMatrix()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## DO NOT RUN!\n## Create DESeq2Dataset object from traditional count matrix\ndds &lt;- DESeqDataSetFromMatrix(countData = \"../Data/Vampirium_counts_traditional.tsv\", \n                          colData = meta %&gt;% column_to_rownames(\"sample\"), \n                          design = ~ condition)\n```\n:::\nYou can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts() (Note: we nested it within the View() function so that rather than getting printed in the console we can see it in the script editor) :\n\n\nCode\nView(counts(dds))\n\n\nAs we go through the workflow we will use the relevant functions to check what information gets stored inside our object.\nYou can use DESeq-specific functions to access the different slots and retrieve information, if you wish. For example, suppose we wanted the original count matrix we would use counts():\n\n\nCode\nhead(counts(dds))\n\n\nAs we go through the workflow we will use the relevant functions to check what information gets stored inside our object.\n\n\nWhile it is not necessary to pre-filter low count genes before running the DESeq2 functions, there are two reasons which make pre-filtering useful:\n\nBy removing rows in which there are very few reads, we reduce the memory size of the dds data object, and we increase the speed of the transformation and testing functions within DESeq2.\nIt can also improve visualizations, as features with no information for differential expression are not plotted.\n\nHere we perform a minimal pre-filtering to keep only rows that have at least 10 reads total.\n\n\nCode\nkeep &lt;- rowSums(counts(dds)) &gt;= 10\ndds &lt;- dds[keep,]\n\n\n\n\n\n\nThe next step is to normalize the count data in order to be able to make fair gene comparisons between samples.\nTo perform the median of ratios method of normalization, DESeq2 has a single estimateSizeFactors() function that will generate size factors for us. We will use the function in the example below, but in a typical RNA-seq analysis this step is automatically performed by the DESeq() function, which we will see later.\n\n\nCode\ndds &lt;- estimateSizeFactors(dds)\n\n\nBy assigning the results back to the dds object we are filling in the slots of the DESeqDataSet object with the appropriate information. We can take a look at the normalization factor applied to each sample using:\n\n\nCode\nsizeFactors(dds)\n\n\nNow, to retrieve the normalized counts matrix from dds, we use the counts() function and add the argument normalized=TRUE.\n\n\nCode\nnormalized_counts &lt;- counts(dds, normalized=TRUE)\n\n\nWe can save this normalized data matrix to file for later use:\n\n\nCode\nwrite.table(normalized_counts, file=\"/work/Intro_to_bulkRNAseq/Results/normalized_counts.txt\", sep=\"\\t\", quote=F)\n\n\n!!! warning\nDESeq2 doesn't actually use normalized counts, rather it uses the raw counts and models the normalization inside the Generalized Linear Model (GLM). These normalized counts will be useful for downstream visualization of results, but cannot be used as input to DESeq2 or any other tools that perform differential expression analysis which use the negative binomial model.\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/02_experimental_planning.html",
    "href": "develop/02_experimental_planning.html",
    "title": "Experimental design considerations",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 30 minutes\nüí¨ Learning Objectives:\n\nDescribe the importance of replicates for RNA-seq differential expression experiments.\nExplain the relationship between the number of biological replicates, sequencing depth and the differentially expressed genes identified.\nDemonstrate how to design an RNA-seq experiment that avoids confounding and batch effects.\n\n\n\nUnderstanding the steps in the experimental process of RNA extraction and preparation of RNA-Seq libraries is helpful for designing an RNA-Seq experiment, but there are special considerations that should be highlighted that can greatly affect the quality of a differential expression analysis.\nThese important considerations include:\n\nProper experiment controls\nNumber and type of replicates\nIssues related to confounding\nAddressing batch effects\n\nWe will go over each of these considerations in detail, discussing best practice and optimal design.\n\n\nExperimental controls must be used in order to minimize the effect of variables which are not the interest of the study. Thus, it allows the experiment to minimize the changes in all other variables except the one being tested, and help us ensure that there have been no deviations in the environment of the experiment that could end up influencing the outcome of the experiment, besides the variable they are investigating.\nThere are different types of controls, but we will mainly see positive and negative controls:\n\nNegative: The negative control is a variable or group of samples where no response is expected.\nPositive: A positive control is a variable or group of samples that receives a treatment with a known positive result.\n\nIt is very important that you give serious thought about proper controls of your experiment so you can control as many sources of variation as possible. This will greatly strengthen the results of your experiment.\n\n\n\nExperimental replicates can be performed as technical replicates or biological replicates.\n\n\n\n\n\n\nFigure¬†1: Types of experimental replicates. Image credit: Klaus B., EMBO J (2015) 34: 2727-2730\n\n\n\n\nTechnical replicates: use the same biological sample to repeat the technical or experimental steps in order to accurately measure technical variation and remove it during analysis.\nBiological replicates use different biological samples of the same condition to measure the biological variation between samples.\n\nIn the days of microarrays, technical replicates were considered a necessity; however, with the current RNA-Seq technologies, technical variation is much lower than biological variation and technical replicates are unnecessary.\nIn contrast, biological replicates are absolutely essential for differential expression analysis. For mice or rats, it might be easy to determine what constitutes a different biological sample, but it‚Äôs a bit more difficult to determine for cell lines. This article gives some great recommendations for cell line replicates.\nFor differential expression analysis, the more biological replicates, the better the estimates of biological variation and the more precise our estimates of the mean expression levels. This leads to more accurate modeling of our data and identification of more differentially expressed genes.\n\n\n\n\n\n\nFigure¬†2: Image credit: Liu, Y., et al., Bioinformatics (2014) 30(3): 301‚Äì304\n\n\n\nAs the figure above illustrates, biological replicates are of greater importance than sequencing depth, which is the total number of reads sequenced per sample. The figure shows the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified [1]. Note that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth. Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression.\n\n\n\n\n\n\nTip\n\n\n\nSample pooling: Try to avoid pooling of individuals/experiments, if possible; however, if absolutely necessary, then each pooled set of samples would count as a single replicate. To ensure similar amounts of variation between replicates, you would want to pool the same number of individuals for each pooled set of samples.\nFor example, if you need at least 3 individuals to get enough material for your control replicate and at least 5 individuals to get enough material for your treatment replicate, you would pool 5 individuals for the control and 5 individuals for the treatment conditions. You would also make sure that the individuals that are pooled in both conditions are similar in sex, age, etc.\n\n\nReplicates are almost always preferred to greater sequencing depth for bulk RNA-Seq. However, guidelines depend on the experiment performed and the desired analysis. Below we list some general guidelines for replicates and sequencing depth to help with experimental planning:\n\nGeneral gene-level differential expression:\n\nENCODE guidelines suggest 30 million SE reads per sample (stranded).\n15 million reads per sample is often sufficient, if there are a good number of replicates (&gt;3).\nSpend money on more biological replicates, if possible.\nGenerally recommended to have read length &gt;= 50 bp\n\nGene-level differential expression with detection of lowly-expressed genes:\n\nSimilarly benefits from replicates more than sequencing depth.\nSequence deeper with at least 30-60 million reads depending on level of expression (start with 30 million with a good number of replicates).\nGenerally recommended to have read length &gt;= 50 bp\n\nIsoform-level differential expression:\n\nOf known isoforms, suggested to have a depth of at least 30 million reads per sample and paired-end reads.\nOf novel isoforms should have more depth (&gt; 60 million reads per sample).\nChoose biological replicates over paired/deeper sequencing.\nGenerally recommended to have read length &gt;= 50 bp, but longer is better as the reads will be more likely to cross exon junctions\nPerform careful QC of RNA quality. Be careful to use high quality preparation methods and restrict analysis to high quality RIN # samples.\n\nOther types of RNA analyses (intron retention, small RNA-Seq, etc.):\n\nDifferent recommendations depending on the analysis.\nAlmost always more biological replicates are better!\n\n\n\n\n\n\n\n\nWhat is coverage?\n\n\n\nThe metric used to estimate the depth of sequencing for genomes is ‚Äúcoverage‚Äù - the number of times the sequenced nucleotides ‚Äúcover‚Äù the genome. While this metric is commonly used and sufficiently accurate for whole genome sequencing, it does not apply to transcriptomes. This is because, although the percentage of the genome exhibiting transcriptional activity may be known, gene expression levels vary significantly.\n\n\n\n\n\nA confounded RNA-Seq experiment is one where you cannot distinguish the separate effects of two different sources of variation in the data.\nFor example, we know that sex has large effects on gene expression, and if all of our control mice were female and all of the treatment mice were male, then our treatment effect would be confounded by sex. We could not differentiate the effect of treatment from the effect of sex.\n\nTo AVOID confounding:\n\nEnsure animals in each condition are all the same sex, age, litter, and batch, if possible.\nIf not possible, then ensure to split the animals equally between conditions\n\n\n\n\n\nA batch effect appears when variance is introduced into your data as a consequence of technical issues such as sample collection, storage, experimental protocol, etc. Batch effects are problematic for RNA-Seq analyses, since you may see significant differences in expression due solely to the batch effect.\nTo explore the issues generated by poor batch study design, they are highlighted nicely in this paper.\n\n\n\nWere all RNA isolations performed on the same day?\nWere all library preparations performed on the same day?\nDid the same person perform the RNA isolation/library preparation for all samples?\nDid you use the same reagents for all samples?\nDid you perform the RNA isolation/library preparation in the same location?\n\nIf any of the answers is ‚ÄòNo‚Äô, then you have batches.\n\n\n\n\nDesign the experiment in a way to avoid batches, if possible.\nIf unable to avoid batches:\n\nDo NOT confound your experiment by batch (top section in Figure¬†3)\n\nDO split replicates of the different sample groups across batches. The more replicates the better, definitely more than 2 (bottom section in Figure¬†3).\n\n\n\n\n\n\n\nFigure¬†3: The problem of confounding biological variation and batch effects. Image credit: Hicks SC, et al., Biostatistics (2018)\n\n\n\n\nDO make a balanced batch design. For example if you can only prepare a subset of samples in the lab on a given day, do not do 90% of samples on day 1 and the remaining 10% on day 2, aim for balance, 50% each day.\nDO include batch information in your experimental metadata. During the analysis, we can regress out the variation due to batch if not confounded so it doesn‚Äôt affect our results if we have that information.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning on sample preparations\n\n\n\nThe sample preparation of cell line ‚Äúbiological‚Äù replicates ‚Äúshould be performed as independently as possible‚Äù (as batches), ‚Äúmeaning that cell culture media should be prepared freshly for each experiment, different frozen cell stocks and growth factor batches, etc. should be used (read more about it here).‚Äù However, preparation across all conditions should be performed at the same time.\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Experiment design",
      "Experimental design considerations"
    ]
  },
  {
    "objectID": "develop/02_experimental_planning.html#controls",
    "href": "develop/02_experimental_planning.html#controls",
    "title": "Experimental design considerations",
    "section": "",
    "text": "Experimental controls must be used in order to minimize the effect of variables which are not the interest of the study. Thus, it allows the experiment to minimize the changes in all other variables except the one being tested, and help us ensure that there have been no deviations in the environment of the experiment that could end up influencing the outcome of the experiment, besides the variable they are investigating.\nThere are different types of controls, but we will mainly see positive and negative controls:\n\nNegative: The negative control is a variable or group of samples where no response is expected.\nPositive: A positive control is a variable or group of samples that receives a treatment with a known positive result.\n\nIt is very important that you give serious thought about proper controls of your experiment so you can control as many sources of variation as possible. This will greatly strengthen the results of your experiment.",
    "crumbs": [
      "Experiment design",
      "Experimental design considerations"
    ]
  },
  {
    "objectID": "develop/02_experimental_planning.html#replicates",
    "href": "develop/02_experimental_planning.html#replicates",
    "title": "Experimental design considerations",
    "section": "",
    "text": "Experimental replicates can be performed as technical replicates or biological replicates.\n\n\n\n\n\n\nFigure¬†1: Types of experimental replicates. Image credit: Klaus B., EMBO J (2015) 34: 2727-2730\n\n\n\n\nTechnical replicates: use the same biological sample to repeat the technical or experimental steps in order to accurately measure technical variation and remove it during analysis.\nBiological replicates use different biological samples of the same condition to measure the biological variation between samples.\n\nIn the days of microarrays, technical replicates were considered a necessity; however, with the current RNA-Seq technologies, technical variation is much lower than biological variation and technical replicates are unnecessary.\nIn contrast, biological replicates are absolutely essential for differential expression analysis. For mice or rats, it might be easy to determine what constitutes a different biological sample, but it‚Äôs a bit more difficult to determine for cell lines. This article gives some great recommendations for cell line replicates.\nFor differential expression analysis, the more biological replicates, the better the estimates of biological variation and the more precise our estimates of the mean expression levels. This leads to more accurate modeling of our data and identification of more differentially expressed genes.\n\n\n\n\n\n\nFigure¬†2: Image credit: Liu, Y., et al., Bioinformatics (2014) 30(3): 301‚Äì304\n\n\n\nAs the figure above illustrates, biological replicates are of greater importance than sequencing depth, which is the total number of reads sequenced per sample. The figure shows the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified [1]. Note that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth. Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression.\n\n\n\n\n\n\nTip\n\n\n\nSample pooling: Try to avoid pooling of individuals/experiments, if possible; however, if absolutely necessary, then each pooled set of samples would count as a single replicate. To ensure similar amounts of variation between replicates, you would want to pool the same number of individuals for each pooled set of samples.\nFor example, if you need at least 3 individuals to get enough material for your control replicate and at least 5 individuals to get enough material for your treatment replicate, you would pool 5 individuals for the control and 5 individuals for the treatment conditions. You would also make sure that the individuals that are pooled in both conditions are similar in sex, age, etc.\n\n\nReplicates are almost always preferred to greater sequencing depth for bulk RNA-Seq. However, guidelines depend on the experiment performed and the desired analysis. Below we list some general guidelines for replicates and sequencing depth to help with experimental planning:\n\nGeneral gene-level differential expression:\n\nENCODE guidelines suggest 30 million SE reads per sample (stranded).\n15 million reads per sample is often sufficient, if there are a good number of replicates (&gt;3).\nSpend money on more biological replicates, if possible.\nGenerally recommended to have read length &gt;= 50 bp\n\nGene-level differential expression with detection of lowly-expressed genes:\n\nSimilarly benefits from replicates more than sequencing depth.\nSequence deeper with at least 30-60 million reads depending on level of expression (start with 30 million with a good number of replicates).\nGenerally recommended to have read length &gt;= 50 bp\n\nIsoform-level differential expression:\n\nOf known isoforms, suggested to have a depth of at least 30 million reads per sample and paired-end reads.\nOf novel isoforms should have more depth (&gt; 60 million reads per sample).\nChoose biological replicates over paired/deeper sequencing.\nGenerally recommended to have read length &gt;= 50 bp, but longer is better as the reads will be more likely to cross exon junctions\nPerform careful QC of RNA quality. Be careful to use high quality preparation methods and restrict analysis to high quality RIN # samples.\n\nOther types of RNA analyses (intron retention, small RNA-Seq, etc.):\n\nDifferent recommendations depending on the analysis.\nAlmost always more biological replicates are better!\n\n\n\n\n\n\n\n\nWhat is coverage?\n\n\n\nThe metric used to estimate the depth of sequencing for genomes is ‚Äúcoverage‚Äù - the number of times the sequenced nucleotides ‚Äúcover‚Äù the genome. While this metric is commonly used and sufficiently accurate for whole genome sequencing, it does not apply to transcriptomes. This is because, although the percentage of the genome exhibiting transcriptional activity may be known, gene expression levels vary significantly.",
    "crumbs": [
      "Experiment design",
      "Experimental design considerations"
    ]
  },
  {
    "objectID": "develop/02_experimental_planning.html#confounding-variables",
    "href": "develop/02_experimental_planning.html#confounding-variables",
    "title": "Experimental design considerations",
    "section": "",
    "text": "A confounded RNA-Seq experiment is one where you cannot distinguish the separate effects of two different sources of variation in the data.\nFor example, we know that sex has large effects on gene expression, and if all of our control mice were female and all of the treatment mice were male, then our treatment effect would be confounded by sex. We could not differentiate the effect of treatment from the effect of sex.\n\nTo AVOID confounding:\n\nEnsure animals in each condition are all the same sex, age, litter, and batch, if possible.\nIf not possible, then ensure to split the animals equally between conditions",
    "crumbs": [
      "Experiment design",
      "Experimental design considerations"
    ]
  },
  {
    "objectID": "develop/02_experimental_planning.html#batch-effects",
    "href": "develop/02_experimental_planning.html#batch-effects",
    "title": "Experimental design considerations",
    "section": "",
    "text": "A batch effect appears when variance is introduced into your data as a consequence of technical issues such as sample collection, storage, experimental protocol, etc. Batch effects are problematic for RNA-Seq analyses, since you may see significant differences in expression due solely to the batch effect.\nTo explore the issues generated by poor batch study design, they are highlighted nicely in this paper.\n\n\n\nWere all RNA isolations performed on the same day?\nWere all library preparations performed on the same day?\nDid the same person perform the RNA isolation/library preparation for all samples?\nDid you use the same reagents for all samples?\nDid you perform the RNA isolation/library preparation in the same location?\n\nIf any of the answers is ‚ÄòNo‚Äô, then you have batches.\n\n\n\n\nDesign the experiment in a way to avoid batches, if possible.\nIf unable to avoid batches:\n\nDo NOT confound your experiment by batch (top section in Figure¬†3)\n\nDO split replicates of the different sample groups across batches. The more replicates the better, definitely more than 2 (bottom section in Figure¬†3).\n\n\n\n\n\n\n\nFigure¬†3: The problem of confounding biological variation and batch effects. Image credit: Hicks SC, et al., Biostatistics (2018)\n\n\n\n\nDO make a balanced batch design. For example if you can only prepare a subset of samples in the lab on a given day, do not do 90% of samples on day 1 and the remaining 10% on day 2, aim for balance, 50% each day.\nDO include batch information in your experimental metadata. During the analysis, we can regress out the variation due to batch if not confounded so it doesn‚Äôt affect our results if we have that information.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning on sample preparations\n\n\n\nThe sample preparation of cell line ‚Äúbiological‚Äù replicates ‚Äúshould be performed as independently as possible‚Äù (as batches), ‚Äúmeaning that cell culture media should be prepared freshly for each experiment, different frozen cell stocks and growth factor batches, etc. should be used (read more about it here).‚Äù However, preparation across all conditions should be performed at the same time.\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Experiment design",
      "Experimental design considerations"
    ]
  },
  {
    "objectID": "develop/08c_FA_GSEA.html",
    "href": "develop/08c_FA_GSEA.html",
    "title": "Functional class scoring",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 40 minutes\nüí¨ Learning Objectives:\n\nDiscuss functional class scoring, and pathway topology methods\nConstruct a GSEA analysis using GO and KEGG gene sets\nExamine results of a GSEA using pathview package\nList other tools and resources for identifying genes of novel pathways or networks\nOver-representation analysis is only a single type of functional analysis method that is available for teasing apart the biological processes important to your condition of interest. Other types of analyses can be equally important or informative, including functional class scoring methods.\nFunctional class scoring (FCS) tools, such as GSEA, most often use the gene-level statistics or log2 fold changes for all genes from the differential expression results, then look to see whether gene sets for particular biological pathways are enriched among the large positive or negative fold changes.\nThe hypothesis of FCS methods is that although large changes in individual genes can have significant effects on pathways (and will be detected via ORA methods), weaker but coordinated changes in sets of functionally related genes (i.e., pathways) can also have significant effects. Thus, rather than setting an arbitrary threshold to identify ‚Äòsignificant genes‚Äô, all genes are considered in the analysis. The gene-level statistics from the dataset are aggregated to generate a single pathway-level statistic and statistical significance of each pathway is reported. This type of analysis can be particularly helpful if the differential expression analysis only outputs a small list of significant DE genes.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Functional class scoring"
    ]
  },
  {
    "objectID": "develop/08c_FA_GSEA.html#other-tools",
    "href": "develop/08c_FA_GSEA.html#other-tools",
    "title": "Functional class scoring",
    "section": "Other Tools",
    "text": "Other Tools\n\nCreate your own Gene Set and run GSEA\nIt is possible to supply your own gene set GMT file, such as a GMT for MSigDB using special clusterProfiler functions as shown below:\n# DO NOT RUN\nBiocManager::install(\"GSEABase\")\nlibrary(GSEABase)\n\n# Load in GMT file of gene sets (we downloaded from the Broad Institute for MSigDB)\n\nc2 &lt;- read.gmt(\"/data/c2.cp.v6.0.entrez.gmt.txt\")\n\nmsig &lt;- GSEA(foldchanges, TERM2GENE=c2, verbose=FALSE)\n\nmsig_df &lt;- data.frame(msig)\n\n\nPathway topology tools\n\n\n\n\n\nPathway topology analysis often takes into account gene interaction information along with the fold changes and adjusted p-values from differential expression analysis to identify dysregulated pathways. Depending on the tool, pathway topology tools explore how genes interact with each other (e.g.¬†activation, inhibition, phosphorylation, ubiquitination, etc.) to determine the pathway-level statistics. Pathway topology-based methods utilize the number and type of interactions between gene product (our DE genes) and other gene products to infer gene function or pathway association.\nFor instance, the SPIA (Signaling Pathway Impact Analysis) tool can be used to integrate the lists of differentially expressed genes, their fold changes, and pathway topology to identify affected pathways.\n\n\nCo-expression clustering\nCo-expression clustering is often used to identify genes of novel pathways or networks by grouping genes together based on similar trends in expression. These tools are useful in identifying genes in a pathway, when their participation in a pathway and/or the pathway itself is unknown. These tools cluster genes with similar expression patterns to create ‚Äòmodules‚Äô of co-expressed genes which often reflect functionally similar groups of genes. These ‚Äòmodules‚Äô can then be compared across conditions or in a time-course experiment to identify any biologically relevant pathway or network information.\nYou can visualize co-expression clustering using heatmaps, which should be viewed as suggestive only; serious classification of genes needs better methods.\nThe way the tools perform clustering is by taking the entire expression matrix and computing pair-wise co-expression values. A network is then generated from which we explore the topology to make inferences on gene co-regulation. The WGCNA package (in R) is one example of a more sophisticated method for co-expression clustering.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Functional class scoring"
    ]
  },
  {
    "objectID": "develop/08c_FA_GSEA.html#extra-resources-for-functional-analysis",
    "href": "develop/08c_FA_GSEA.html#extra-resources-for-functional-analysis",
    "title": "Functional class scoring",
    "section": "Extra resources for functional analysis",
    "text": "Extra resources for functional analysis\n\ng:Profiler - http://biit.cs.ut.ee/gprofiler/index.cgi\nDAVID - http://david.abcc.ncifcrf.gov/tools.jsp\nclusterProfiler - http://bioconductor.org/packages/release/bioc/html/clusterProfiler.html\nGeneMANIA - http://www.genemania.org/\nGenePattern - http://www.broadinstitute.org/cancer/software/genepattern/ (need to register)\nWebGestalt - http://bioinfo.vanderbilt.edu/webgestalt/ (need to register)\nAmiGO - http://amigo.geneontology.org/amigo\nReviGO (visualizing GO analysis, input is GO terms) - http://revigo.irb.hr/\nWGCNA - https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/\nGSEA - http://software.broadinstitute.org/gsea/index.jsp\nSPIA - https://www.bioconductor.org/packages/release/bioc/html/SPIA.html\nGAGE/Pathview - http://www.bioconductor.org/packages/release/bioc/html/gage.html\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Functional class scoring"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html",
    "href": "develop/07b_hypothesis_testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 90 minutes\nüí¨ Learning Objectives:\n\nDemonstrate the use of the design formula with simple and complex designs\nConstruct R code to execute the differential expression analysis workflow with DESeq2\nDescribe the process of model fitting\nCompare two methods for hypothesis testing (Wald test vs.¬†LRT)\nDiscuss the steps required to generate a results table for pairwise comparisons (Wald test)\nRecognize the importance of multiple test correction\nIdentify different methods for multiple test correction\nSummarize the different levels of gene filtering\nEvaluate the number of differentially expressed genes produced for each comparison\nConstruct R objects containing significant genes from each comparison\nDESeq2: Model fitting and Hypothesis testing\nThe final step in the DESeq2 workflow is taking the counts for each gene and fitting it to the model and testing for differential expression.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#generalized-linear-model",
    "href": "develop/07b_hypothesis_testing.html#generalized-linear-model",
    "title": "Hypothesis testing",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nAs described earlier, the count data generated by RNA-seq exhibits overdispersion (variance &gt; mean) and the statistical distribution used to model the counts needs to account for this. As such, DESeq2 uses a negative binomial distribution to model the RNA-seq counts using the equation below:\n\nThe two parameters required are the size factor, and the dispersion estimate. Next, a generalized linear model (GLM) of the NB family is used to fit the data. Modeling is a mathematically formalized way to approximate how the data behaves given a set of parameters.\n\n\n\n\n\n\nAbout GLMs\n\n\n\n\n\n\n\n‚ÄúIn statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.‚Äù (Wikipedia).\n\n\n\n\n\nAfter the model is fit, coefficients are estimated for each sample group along with their standard error. The coefficients are the estimates for the log2 fold changes, and will be used as input for hypothesis testing.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#hypothesis-testing",
    "href": "develop/07b_hypothesis_testing.html#hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nThe first step in hypothesis testing is to set up a null hypothesis for each gene. In our case, the null hypothesis is that there is no differential expression across the two sample groups (LFC == 0). Notice that we can do this without observing any data, because it is based on a thought experiment. Second, we use a statistical test to determine if based on the observed data, the null hypothesis can be rejected.\n\nWald test\nIn DESeq2, the Wald test is the default used for hypothesis testing when comparing two groups. The Wald test is a test usually performed on parameters that have been estimated by maximum likelihood. In our case we are testing each gene model coefficient (LFC) which was derived using parameters like dispersion, which were estimated using maximum likelihood.\nDESeq2 implements the Wald test by:\n\nTaking the LFC and dividing it by its standard error, resulting in a z-statistic.\nThe z-statistic is compared to a standard normal distribution, and a p-value is computed reporting the probability that a z-statistic at least as extreme as the observed value would be selected at random.\nIf the p-value is small we reject the null hypothesis and state that there is evidence against the null (i.e.¬†the gene is differentially expressed).\n\nThe model fit and Wald test were already run previously as part of the DESeq() function:\n## DO NOT RUN THIS CODE\n\n## Create DESeq2Dataset object\ndds &lt;- DESeqDataSetFromTximport(txi,\n    colData = meta %&gt;% column_to_rownames(\"sample\"), \n    design = ~ condition)\n## Run analysis\ndds &lt;- DESeq(dds)\n\n\nLikelihood ratio test (LRT)\nAn alternative to pair-wise comparisons is to analyze all levels of a factor at once. By default, the Wald test is used to generate the results table, but DESeq2 also offers the LRT which is used to identify any genes that show change in expression across the different levels. This type of test can be especially useful in analyzing time course experiments.\n\n\n\n\n\n\nLRT vs Wald test\n\n\n\nThe Wald test evaluates whether a gene‚Äôs expression is up- or down-regulated in one group compared to another, while the LRT identifies genes that are changing in expression in any combination across the different sample groups. For example:\n\nGene expression is equal between Control and Vampirium, but is less in Garlicum\nGene expression of Vampirium is lower than Control and Control is lower than Garlicum\nGene expression of Garlicum is equal to Control, but less than Vampirium.\n\nUsing LRT, we are evaluating the null hypothesis that the full model fits just as well as the reduced model. If we reject the null hypothesis, this suggests that there is a significant amount of variation explained by the full model (and our main factor of interest), therefore the gene is differentially expressed across the different levels. Generally, this test will result in a larger number of genes than the individual pair-wise comparisons. However, while the LRT is a test of significance for differences of any level of the factor, one should not expect it to be exactly equal to the union of sets of genes using Wald tests (although we do expect a majority overlap).\n\n\nTo use the LRT, we use the DESeq() function but this time adding two arguments:\n\nspecifying that we want to use the LRT test\nthe ‚Äúreduced‚Äù model\n\n# The full model was specified previously with the `design = ~ condition`\n# dds &lt;- DESeqDataSetFromTximport(txi,\n    # colData = meta %&gt;% column_to_rownames(\"sample\"), \n    # design = ~ condition)\n\n# Likelihood ratio test\ndds_lrt &lt;- DESeq(dds, test=\"LRT\", reduced = ~ 1)\nSince our ‚Äúfull‚Äù model only has one factor (sampletype), the ‚Äúreduced‚Äù model (removing that factor) is just the intercept (~ 1). You will find that similar columns are reported for the LRT test. One thing to note is, even though there are fold changes present they are not directly associated with the actual hypothesis test.\n\n\n\n\n\n\nTime course analysis with LTR\n\n\n\n\n\nThe LRT test can be especially helpful when performing time course analyses. We can use the LRT to explore whether there are any significant differences in treatment effect between any of the timepoints.\nFor have an experiment looking at the effect of treatment over time on mice of two different genotypes. We could use a design formula for our ‚Äúfull model‚Äù that would include the major sources of variation in our data: genotype, treatment, time, and our main condition of interest, which is the difference in the effect of treatment over time (treatment:time).\nfull_model &lt;- ~ genotype + treatment + time + treatment:time\nTo perform the LRT test, we can determine all genes that have significant differences in expression between treatments across time points by giving the ‚Äúreduced model‚Äù without the treatment:time term:\nreduced_model &lt;- ~ genotype + treatment + time\nThen, we could run our test by using the following code:\ndds_lrt &lt;- DESeqDataSetFromMatrix(countData = data, colData = metadata, design = ~ genotype + treatment + time + treatment:time)\n\ndds_lrt_time &lt;- DESeq(dds_lrt, test=\"LRT\", reduced = ~ genotype + treatment + time)\nThis analysis will not return genes where the treatment effect does not change over time, even though the genes may be differentially expressed between groups at a particular time point, as shown in the figure below:\n\nThe significant DE genes will represent those genes that have differences in the effect of treatment over time, an example is displayed in the figure below:",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#multiple-test-correction",
    "href": "develop/07b_hypothesis_testing.html#multiple-test-correction",
    "title": "Hypothesis testing",
    "section": "Multiple test correction",
    "text": "Multiple test correction\nRegardless of whether we use the Wald test or the LRT, each gene that has been tested will be associated with a p-value. It is this result which we use to determine which genes are considered significantly differentially expressed. However, we cannot use the p-value directly.\n\nWhat does the p-value mean?\nA gene with a significance cut-off of p &lt; 0.05, means there is a 5% chance it is a false positive. For example, if we test 20,000 genes for differential expression, at p &lt; 0.05 we would expect to find 1,000 DE genes by chance. If we found 3000 genes to be differentially expressed total, roughly one third of our genes are false positives! We would not want to sift through our ‚Äúsignificant‚Äù genes to identify which ones are true positives.\nSince each p-value is the result of a single test (single gene). The more genes we test, the more we inflate the false positive rate. This is the multiple testing problem.\n\n\nCorrecting the p-value for multiple testing\nThere are a few common approaches for multiple test correction:\n\nBonferroni: The adjusted p-value is calculated by: \\(p-value*m\\) (\\(m\\) = total number of tests). This is a very conservative approach with a high probability of false negatives, so is generally not recommended.\nFDR/Benjamini-Hochberg: Benjamini and Hochberg (1995) defined the concept of False Discovery Rate (FDR) and created an algorithm to control the expected FDR below a specified level given a list of independent p-values. More info about BH.\nQ-value / Storey method: The minimum FDR that can be attained when calling that feature significant. For example, if gene X has a q-value of 0.013 it means that 1.3% of genes that show p-values at least as small as gene X are false positives.\n\nDESeq2 helps reduce the number of genes tested by removing those genes unlikely to be significantly DE prior to testing, such as those with low number of counts and outlier samples (see below). However, multiple test correction is also implemented to reduce the False Discovery Rate using an interpretation of the Benjamini-Hochberg procedure.\n\n\n\n\n\n\nSo what does FDR &lt; 0.05 mean?\n\n\n\nBy setting the FDR cutoff to &lt; 0.05, we‚Äôre saying that the proportion of false positives we expect amongst our differentially expressed genes is 5%. For example, if you call 500 genes as differentially expressed with an FDR cutoff of 0.05, you expect 25 of them to be false positives.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#exploring-results-wald-test",
    "href": "develop/07b_hypothesis_testing.html#exploring-results-wald-test",
    "title": "Hypothesis testing",
    "section": "Exploring Results (Wald test)",
    "text": "Exploring Results (Wald test)\nBy default DESeq2 uses the Wald test to identify genes that are differentially expressed between two sample groups. Given the factor(s) used in the design formula, and how many factor levels are present, we can extract results for a number of different comparisons. Here, we will walk you through how to obtain results from the dds object and provide some explanations on how to interpret them.\n\n\n\n\n\n\nWald test on continuous variables‚Äù\n\n\n\nThe Wald test can also be used with continuous variables. If the variable of interest provided in the design formula is continuous-valued, then the reported log2FoldChange is per unit of change of that variable.\n\n\n\nSpecifying contrasts\nIn our dataset, we have three sample groups so we can make three possible pairwise comparisons:\n\nControl vs.¬†Vampirium\nGarlicum vs.¬†Vampirium\nGarlicum vs.¬†Control\n\nWe are really only interested in 1 and 2 from above. When we initially created our dds object we had provided ~ condition as our design formula, indicating that sampletype is our main factor of interest.\nTo indicate which two sample groups we are interested in comparing, we need to specify contrasts. The contrasts are used as input to the DESeq2 results() function to extract the desired results.\nContrasts can be specified in three different ways:\n1.Contrasts can be supplied as a character vector with exactly three elements: the name of the factor (of interest) in the design formula, the name of the two factors levels to compare. The factor level given last is the base level for the comparison. The syntax is given below:\n# DO NOT RUN!\ncontrast &lt;- c(\"condition\", \"level_to_compare\", \"base_level\")\nresults(dds, contrast = contrast)\n2.Contrasts can be given as a list of 2 character vectors: the names of the fold changes for the level of interest, and the names of the fold changes for the base level. These names should match identically to the elements of resultsNames(object). This method can be useful for combining interaction terms and main effects.\n# DO NOT RUN!\nresultsNames(dds) # to see what names to use\ncontrast &lt;- list(resultsNames(dds)[1], resultsNames(dds)[2])\nresults(dds, contrast = contrast)\n3.One of the results from resultsNames(dds) and the name argument. This one is the simplest but it can also be very restrictive:\n# DO NOT RUN!\nresultsNames(dds) # to see what names to use\nresults(dds, name = resultsNames(dds)[2])\nAlternatively, if you only had two factor levels you could do nothing and not worry about specifying contrasts (i.e.¬†results(dds)). In this case, DESeq2 will choose what your base factor level based on alphabetical order of the levels.\nTo start, we want to evaluate expression changes between the control samples and the vampirium samples. As such we will use the first method for specifying contrasts and create a character vector:\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\nDefine contrasts for Vampirium samples using one of the two methods above.\n## your code here\n#contrast_cont &lt;- \n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nFirst let‚Äôs check the metadata:\nmeta\nSince we have only given the condition column in the design formula, the first element should be condition. The second element is the condition we are interested, control and our base level is vampirium.\ncontrast_cont &lt;- c(\"condition\", \"control\", \"vampirium\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes it matter what I choose to be my base level?\n\n\n\nYes, it does matter. Deciding what level is the base level will determine how to interpret the fold change that is reported. So for example, if we observe a log2 fold change of -2 this would mean the gene expression is lower in factor level of interest relative to the base level. Thus, if leaving it up to DESeq2 to decide on the contrasts be sure to check that the alphabetical order coincides with the fold change direction you are anticipating.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#the-results-table",
    "href": "develop/07b_hypothesis_testing.html#the-results-table",
    "title": "Hypothesis testing",
    "section": "The results table",
    "text": "The results table\nNow that we have our contrast created, we can use it as input to the results() function. Let‚Äôs take a quick look at the help manual for the function:\n?results\nYou will see we have the option to provide a wide array of arguments and tweak things from the defaults as needed. As we go through the lesson we will keep coming back to the help documentation to discuss some arguments that are good to know about.\n## Extract results for Control vs Vampirium samples\nres_tableCont &lt;- results(dds, contrast=contrast_cont, alpha = 0.05)\n\n\n\n\n\n\nWarning\n\n\n\nFor our analysis, in addition to the contrast argument we will also provide a value of 0.05 for the alpha argument. We will describe this in more detail when we talk about gene-level filtering.\n\n\nThe results table that is returned to us is a DESeqResults object, which is a simple subclass of DataFrame. In many ways it can be treated like a dataframe (i.e when accessing/subsetting data), however it is important to recognize that there are differences for downstream steps like visualization.\n# Check what type of object is returned\nclass(res_tableCont)\nNow let‚Äôs take a look at what information is stored in the results:\n# What is stored in results?\nres_tableCont %&gt;% \n  data.frame() %&gt;% \n  head()\nWe have six columns of information reported for each gene (row). We can use the mcols() function to extract information on what the values stored in each column represent:\n# Get information on each column in results\nmcols(res_tableCont, use.names=T)\n\nbaseMean: mean of normalized counts for all samples\nlog2FoldChange: log2 fold change\nlfcSE: standard error\nstat: Wald statistic\npvalue: Wald test p-value\npadj: BH adjusted p-values",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#p-values",
    "href": "develop/07b_hypothesis_testing.html#p-values",
    "title": "Hypothesis testing",
    "section": "p-values",
    "text": "p-values\nThe p-value is a probability value used to determine whether there is evidence to reject the null hypothesis. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.\nThe padj column in the results table represents the p-value adjusted for multiple testing, and is the most important column of the results. Typically, a threshold such as padj &lt; 0.05 is a good starting point for identifying significant genes. The default method for multiple test correction in DESeq2 is the FDR. Other methods can be used with the pAdjustMethod argument in the results() function.\n\nGene-level filtering\nLet‚Äôs take a closer look at our results table. As we scroll through it, you will notice that for selected genes there are NA values in the pvalue and padj columns. What does this mean?\n\nThe missing values represent genes that have undergone filtering as part of the DESeq() function. Prior to differential expression analysis it is beneficial to omit genes that have little or no chance of being detected as differentially expressed. This will increase the power to detect differentially expressed genes. DESeq2 does not physically remove any genes from the original counts matrix, and so all genes will be present in your results table. The genes omitted by DESeq2 meet one of the three filtering criteria outlined below:\n1. Genes with zero counts in all samples\nIf within a row, all samples have zero counts there is no expression information and therefore these genes are not tested. In our case, there are no genes that fulfill this criteria, since we have already filtered out these genes ourselves when we created our dds object.\n# Show genes with zero expression\nres_tableCont %&gt;%\n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(baseMean==0) %&gt;%\n  head()\n\n\n\n\n\n\nTip\n\n\n\nIf there would be any genes meeting this criteria, the baseMean column for these genes will be zero, and the log2 fold change estimates, p-value and adjusted p-value will all be set to NA.\n\n\n2. Genes with an extreme count outlier\nThe DESeq() function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook‚Äôs distance. Cook‚Äôs distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook‚Äôs distance is intended to indicate an outlier count. Genes which contain a Cook‚Äôs distance above a threshold are flagged, however at least 3 replicates are required for flagging, as it is difficult to judge which sample might be an outlier with only 2 replicates. We can turn off this filtering by using the cooksCutoff argument in the results() function.\n# Show genes that have an extreme outlier\nres_tableCont %&gt;% \n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(is.na(pvalue) & is.na(padj) & baseMean &gt; 0) %&gt;%\n  head()\nIt seems that we have some genes with outliers!\n\n\n\n\n\n\nTip\n\n\n\nIf a gene contains a sample with an extreme count outlier then the p-value and adjusted p-value will be set to NA.\n\n\n3. Genes with a low mean normalized counts\nDESeq2 defines a low mean threshold, that is empirically determined from your data, in which the fraction of significant genes can be increased by reducing the number of genes that are considered for multiple testing. This is based on the notion that genes with very low counts are not likely to see significant differences typically due to high dispersion.\n\n\n\nImage courtesy of slideshare presentation from Joachim Jacob, 2014.\n\n\nAt a user-specified value (alpha = 0.05), DESeq2 evaluates the change in the number of significant genes as it filters out increasingly bigger portions of genes based on their mean counts, as shown in the figure above. The point at which the number of significant genes reaches a peak is the low mean threshold that is used to filter genes that undergo multiple testing. There is also an argument to turn off the filtering off by setting independentFiltering = F.\n# Filter genes below the low mean threshold\nres_tableCont %&gt;% \n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(!is.na(pvalue) & is.na(padj) & baseMean &gt; 0) %&gt;%\n  head()\n\n\n\n\n\n\nTip\n\n\n\nIf a gene is filtered by independent filtering, then only the adjusted p-value will be set to NA.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDESeq2 will perform the filtering outlined above by default; however other DE tools, such as EdgeR will not. Filtering is a necessary step, even if you are using limma-voom and/or edgeR‚Äôs quasi-likelihood methods. Be sure to follow pre-filtering steps when using other tools, as outlined in their user guides found on Bioconductor as they generally perform much better.*",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#fold-changes",
    "href": "develop/07b_hypothesis_testing.html#fold-changes",
    "title": "Hypothesis testing",
    "section": "Fold changes",
    "text": "Fold changes\nAnother important column in the results table, is the log2FoldChange. With large significant gene lists it can be hard to extract meaningful biological relevance. To help increase stringency, one can also add a fold change threshold. Keep in mind when setting that value that we are working with log2 fold changes, so a cutoff of log2FoldChange &lt; 1 would translate to an actual fold change of 2.\n\n\n\n\n\n\nAn alternative approach to add the fold change threshold\n\n\n\n\n\nThe results() function has an option to add a fold change threshold using the lfcThreshold argument. This method is more statistically motivated, and is recommended when you want a more confident set of genes based on a certain fold-change. It actually performs a statistical test against the desired threshold, by performing a two-tailed test for log2 fold changes greater than the absolute value specified. The user can change the alternative hypothesis using altHypothesis and perform two one-tailed tests as well. This is a more conservative approach, so expect to retrieve a much smaller set of genes!\nres_tableCont_LFC1 &lt;- results(dds, contrast=contrast_cont, alpha = 0.05, lfcThreshold = 1)\n\n\n\nThe fold changes reported in the results table are the coefficients calculated in the GLM mentioned in the previous section.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#summarizing-results",
    "href": "develop/07b_hypothesis_testing.html#summarizing-results",
    "title": "Hypothesis testing",
    "section": "Summarizing results",
    "text": "Summarizing results\nTo summarize the results table, a handy function in DESeq2 is summary(). Confusingly it has the same name as the function used to inspect data frames. This function, when called with a DESeq results table as input, will summarize the results using a default threshold of padj &lt; 0.1. However, since we had set the alpha argument to 0.05 when creating our results table threshold: FDR &lt; 0.05 (padj/FDR is used even though the output says p-value &lt; 0.05). Let‚Äôs start with the OE vs control results:\n## Summarize results\nsummary(res_tableCont, alpha = 0.05)\nIn addition to the number of genes up- and down-regulated at the default threshold, the function also reports the number of genes that were tested (genes with non-zero total read count), and the number of genes not included in multiple test correction due to a low mean count.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/07b_hypothesis_testing.html#extracting-significant-differentially-expressed-genes",
    "href": "develop/07b_hypothesis_testing.html#extracting-significant-differentially-expressed-genes",
    "title": "Hypothesis testing",
    "section": "Extracting significant differentially expressed genes",
    "text": "Extracting significant differentially expressed genes\nLet‚Äôs first create variables that contain our threshold criteria. We will only be using the adjusted p-values in our criteria:\n### Set thresholds\npadj.cutoff &lt;- 0.05\nWe can easily subset the results table to only include those that are significant using the dplyr::filter() function, but first we will convert the results table into a tibble:\n# Create a tibble of results and add gene IDs to new object\nres_tableCont_tb &lt;- res_tableCont %&gt;%\n  as_tibble(rownames = \"gene\") %&gt;%\n  relocate(gene, .before = baseMean)\n\nhead(res_tableCont_tb)\nNow we can subset that table to only keep the significant genes using our pre-defined thresholds:\n# Subset the tibble to keep only significant genes\nsigCont &lt;- res_tableCont_tb %&gt;%\n  dplyr::filter(padj &lt; padj.cutoff)\n# Take a quick look at this tibble\nsigCont\nNow that we have extracted the significant results, we are ready for visualization!\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\n\nVampirium Differential Expression Analysis: Garlicum versus Vampirium\nNow that we have results for the Control vs Vampirium results, do the same for the Garlicum vs Vampirium samples.\n\nCreate a contrast vector called contrast_gar.\nUse contrast vector in the results() to extract a results table and store that to a variable called res_tableGar.\nUsing a p-adjusted threshold of 0.05 (padj.cutoff &lt; 0.05), subset res_tableGar to report the number of genes that are up- and down-regulated in garlicum compared to vampirium.\nHow many genes are differentially expressed in the Garlicum compared to Vampirium? How does this compare to the Control vs Vampirium significant gene list (in terms of numbers)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nContrast for Garlicum vs Vampirium\n\ncontrast_gar &lt;- c(\"condition\", \"garlicum\", \"vampirium\")\n\nExtract results\n\nres_tableGar &lt;- results(dds, contrast = contrast_gar)\n\nSignificant genes\n\npadj.cutoff &lt;- 0.05\nsigGar &lt;- res_tableGar %&gt;% as_tibble(rownames = \"gene\") %&gt;%\nrelocate(gene, .before = baseMean) %&gt;% dplyr::filter(padj &lt; padj.cutoff)\n\nsigGar\n\nComparison against sigGar vs sigCont\n\nnrow(sigGar) #number of significant genes in Garlicum vs Vampirium\nnrow(sigCont) #number of significant genes in Control vs Vampirium\n\nnrow(sigCont) - nrow(sigGar)\nsigCont has almost 2000 more genes that are differentially expressed!\n\n\n\n\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nSome materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "develop/09_summarized_workflow.html",
    "href": "develop/09_summarized_workflow.html",
    "title": "Summarised workflow",
    "section": "",
    "text": "We have detailed the various steps in a differential expression analysis workflow, providing theory with example code. To provide a more succinct reference for the code needed to run a DGE analysis, we have summarized the steps in an analysis below:"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#libraries",
    "href": "develop/09_summarized_workflow.html#libraries",
    "title": "Summarised workflow",
    "section": "Libraries",
    "text": "Libraries\nlibrary(tidyverse)\nlibrary(DESeq2)\nlibrary(ggrepel)\nlibrary(pheatmap)\nlibrary(annotables)\nlibrary(clusterProfiler)\nlibrary(DOSE)\nlibrary(pathview)\nlibrary(org.Hs.eg.db)\nlibrary(tximport)\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#obtaining-gene-level-counts-from-your-preprocessing-and-create-deseq-object",
    "href": "develop/09_summarized_workflow.html#obtaining-gene-level-counts-from-your-preprocessing-and-create-deseq-object",
    "title": "Summarised workflow",
    "section": "Obtaining gene-level counts from your preprocessing and create DESeq object",
    "text": "Obtaining gene-level counts from your preprocessing and create DESeq object\n\nIf you have a traditional raw count matrix\nLoad data and metadata\ndata &lt;- read_table(\"../Data/Vampirium_counts_traditional.tsv\") \n\nmeta &lt;- read_table(\"../Data/samplesheet.csv\")\nmeta$condition &lt;- factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\nCheck that the row names of the metadata equal the column names of the raw counts data\nall(colnames(data)[-1] == meta$sample)\nCreate DESeq2Dataset object\ndds &lt;- DESeqDataSetFromMatrix(countData = data %&gt;% column_to_rownames(\"GeneSymbol\"), \n                      colData = meta %&gt;% column_to_rownames(\"sample\"), \n                      design = ~ condition)\n\n\nIf you have pseudocounts\nLoad samplesheet with all our metadata from our pipeline\n# Load data, metadata and tx2gene and create a txi object\nmeta &lt;- read_csv(\"/work/Intro_bulkRNAseq/Data/samplesheet.csv\")\nmeta$condition &lt;- factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\nCreate a list of salmon results\ndir &lt;- \"/work/Intro_bulkRNAseq/Data/salmon\"\ntx2gene &lt;- read_table(file.path(dir,\"salmon_tx2gene.tsv\"), col_names = c(\"transcript_ID\",\"gene_ID\",\"gene_symbol\"))\n\n# Get all salmon results files\nfiles &lt;- file.path(dir, meta$sample, \"quant.sf\")\nnames(files) &lt;- meta$sample\nCreate txi object\ntxi &lt;- tximport(files, type=\"salmon\", tx2gene=tx2gene, countsFromAbundance = \"lengthScaledTPM\", ignoreTxVersion = TRUE)\nCreate dds object\ndds &lt;- DESeqDataSetFromTximport(txi,\n    colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#exploratory-data-analysis",
    "href": "develop/09_summarized_workflow.html#exploratory-data-analysis",
    "title": "Summarised workflow",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nPrefiltering low count genes + PCA & hierarchical clustering - identifying outliers and sources of variation in the data:\n\nPrefiltering low count genes\nkeep &lt;- rowSums(counts(dds)) &gt; 0\ndds &lt;- dds[keep,]\n\n\nRlog transformation\n# Transform counts for data visualization\nrld &lt;- rlog(dds, \n            blind=TRUE)\n\n\nPCA\nPlot PCA\nplotPCA(rld, \n        intgroup=\"condition\")\n\n\nHeatmaps\nExtract the rlog matrix from the object\nrld_mat &lt;- assay(rld)\n# Pearson correlation betweeen samples\nrld_cor &lt;- cor(rld_mat) \n# distances are computed by rows, so we need to transponse the matrix\nrld_dist &lt;- as.matrix(dist(t(assay(rld))))\nPlot heatmap of correlations\npheatmap(rld_cor, \n         annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"))\nPlot heatmap of distances with a new color range\n# Colors from the RColorBrewer package (only 6)\nheat.colors &lt;- brewer.pal(6, \"Blues\") \n# Interpolate 100 colors\nheat.colors &lt;- colorRampPalette(heat.colors)(100) \n\npheatmap(rld_dist, annotation = meta %&gt;% \n      column_to_rownames(\"sample\") %&gt;% \n      select(\"condition\"), color = heat.colors)"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#run-deseq2",
    "href": "develop/09_summarized_workflow.html#run-deseq2",
    "title": "Summarised workflow",
    "section": "Run DESeq2:",
    "text": "Run DESeq2:\nOptional step - Re-create DESeq2 dataset if the design formula has changed after QC analysis in include other sources of variation using\n# dds &lt;- DESeqDataSetFromMatrix(data, colData = meta, design = ~ covariate + condition)\nRun DEseq2\n# Run DESeq2 differential expression analysis\ndds &lt;- DESeq(dds)\nOptional step - Output normalized counts to save as a file to access outside RStudio using\nnormalized_counts &lt;- counts(dds, normalized=TRUE)"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#check-the-fit-of-the-dispersion-estimates",
    "href": "develop/09_summarized_workflow.html#check-the-fit-of-the-dispersion-estimates",
    "title": "Summarised workflow",
    "section": "Check the fit of the dispersion estimates",
    "text": "Check the fit of the dispersion estimates\nPlot dispersion estimates\nplotDispEsts(dds)"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#create-contrasts-to-perform-wald-testing-or-the-shrunken-log2-fold-changes-between-specific-conditions",
    "href": "develop/09_summarized_workflow.html#create-contrasts-to-perform-wald-testing-or-the-shrunken-log2-fold-changes-between-specific-conditions",
    "title": "Summarised workflow",
    "section": "Create contrasts to perform Wald testing or the shrunken log2 fold changes between specific conditions",
    "text": "Create contrasts to perform Wald testing or the shrunken log2 fold changes between specific conditions\nFormal LFC calculation\n# Specify contrast for comparison of interest\ncontrast &lt;- c(\"condition\", \"control\", \"vampirium\")\n\n# Output results of Wald test for contrast\nres &lt;- results(dds, \n        contrast = contrast, \n        alpha = 0.05)\nShrinkage\n# Get name of the contrast you would like to use\nresultsNames(dds)\n\n# Shrink the log2 fold changes to be more accurate\nres &lt;- lfcShrink(dds, \n      coef = \"condition_control_vs_vampirium\", \n      type = \"apeglm\")"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#output-significant-results",
    "href": "develop/09_summarized_workflow.html#output-significant-results",
    "title": "Summarised workflow",
    "section": "Output significant results:",
    "text": "Output significant results:\n# Set thresholds\npadj.cutoff &lt;- 0.05\n\n# Turn the results object into a tibble for use with tidyverse functions\nres_tbl &lt;- res %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") %&gt;% \n  as_tibble()\n\n# Subset the significant results\nsig_res &lt;- filter(res_tbl, \n            padj &lt; padj.cutoff)"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#visualize-results-volcano-plots-heatmaps-normalized-counts-plots-of-top-genes-etc.",
    "href": "develop/09_summarized_workflow.html#visualize-results-volcano-plots-heatmaps-normalized-counts-plots-of-top-genes-etc.",
    "title": "Summarised workflow",
    "section": "Visualize results: volcano plots, heatmaps, normalized counts plots of top genes, etc.",
    "text": "Visualize results: volcano plots, heatmaps, normalized counts plots of top genes, etc.\nFunction to get gene_IDs based on gene names. The function will take as input a vector of gene names of interest, the tx2gene dataframe and the dds object that you analyzed.\nlookup &lt;- function(gene_name, tx2gene, dds){\n  hits &lt;- tx2gene %&gt;% select(gene_symbol, gene_ID) %&gt;% distinct() %&gt;% \n    filter(gene_symbol %in% gene_name & gene_ID %in% rownames(dds))\n  return(hits)\n}\n\nlookup(gene_name = \"TSPAN7\", tx2gene = tx2gene, dds = dds)\nPlot expression for single gene\nplotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\")\n\nMAplot\nplotMA(res)\n\n\nVolcano plot with labels (top N genes)\n## Obtain logical vector where TRUE values denote padj values &lt; 0.05 and fold change &gt; 1.5 in either direction\nres_tbl &lt;- res_tbl %&gt;% \n  mutate(threshold = padj &lt; 0.05 & abs(log2FoldChange) &gt;= 0.58)\n## Create an empty column to indicate which genes to label\nres_tbl &lt;- res_tbl %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tbl &lt;- res_tbl %&gt;% arrange(padj)\n\n## Populate the genelabels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tbl$genelabels[1:10] &lt;- as.character(res_tbl$gene[1:10])\n\nhead(res_tbl)\nggplot(res_tbl, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Mov10 overexpression\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25))) \n\n\nHeatmap of differentially expressed genes\n# filter significant results from normalized counts\nnorm_sig &lt;- normalized_counts %&gt;% as_tibble(rownames = \"gene\") %&gt;%\n  dplyr::filter(gene %in% sig_res$gene) %&gt;% column_to_rownames(var=\"gene\")\npheatmap(norm_sig, \n  cluster_rows = T, #cluster by expression pattern\n  scale = \"row\", # scale by gene so expression pattern is visible\n  treeheight_row = 0, # dont show the row dendogram\n  show_rownames = F, # remove rownames so it is more clear\n  annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% \n  dplyr::select(\"condition\")\n)"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#perform-analysis-to-extract-functional-significance-of-results-go-or-kegg-enrichment-gsea-etc.",
    "href": "develop/09_summarized_workflow.html#perform-analysis-to-extract-functional-significance-of-results-go-or-kegg-enrichment-gsea-etc.",
    "title": "Summarised workflow",
    "section": "Perform analysis to extract functional significance of results: GO or KEGG enrichment, GSEA, etc.",
    "text": "Perform analysis to extract functional significance of results: GO or KEGG enrichment, GSEA, etc.\n\nAnnotate with annotables\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% res_tbl$gene) \nres_ids &lt;- inner_join(res_tbl, ids, by=c(\"gene\"=\"ensgene\"))\n\n\nPerform enrichment analysis of GO terms (can be done as well with KEGG pathways)\n# Create background dataset for hypergeometric testing using all genes tested for significance in the results\nall_genes &lt;- dplyr::filter(res_ids, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n# Extract significant results\nsig &lt;- dplyr::filter(res_ids, padj &lt; 0.05 & !is.na(gene))\n\nsig_genes &lt;- sig %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n# Perform enrichment analysis\nego &lt;- enrichGO(gene = sig_genes, \n                universe = all_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\n\nego &lt;- enrichplot::pairwise_termsim(ego)\n\n\nVisualize result\ndotplot(ego, showCategory=50)\nemapplot(ego, showCategory = 50)\n\n\nCnetplot\n## To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector\nsig_foldchanges &lt;- sig$log2FoldChange\n\nnames(sig_foldchanges) &lt;- sig$gene\n## Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj)\ncnetplot(ego, \n         categorySize=\"pvalue\", \n         showCategory = 5, \n         foldChange=sig_foldchanges, \n         vertex.label.font=6)\n\n\nPerform GSEA analysis of KEGG pathways (can be done as well with GO terms)\n# Extract entrez IDs. IDs should not be duplicated or NA\nres_entrez &lt;- dplyr::filter(res_ids, entrez != \"NA\" & entrez != \"NULL\" & duplicated(entrez)==F)\n\n## Extract the foldchanges\nfoldchanges &lt;- res_entrez$log2FoldChange\n\n## Name each fold change with the corresponding Entrez ID\nnames(foldchanges) &lt;- res_entrez$entrez\n\n## Sort fold changes in decreasing order\nfoldchanges &lt;- sort(foldchanges, decreasing = TRUE)\n# Run GSEA of KEGG\ngseaKEGG &lt;- gseKEGG(geneList = foldchanges, # ordered named vector of fold changes (Entrez IDs are the associated names)\n            organism = \"hsa\", # supported organisms listed below\n            pvalueCutoff = 0.05, # padj cutoff value\n            verbose = FALSE)\n\ngseaKEGG_results &lt;- gseaKEGG@result\nhead(gseaKEGG_results)\n## Plot the GSEA plot for a single enriched pathway:\ngseaplot(gseaKEGG, geneSetID = gseaKEGG_results$ID[1])\n## Output images for a single significant KEGG pathway\npathview(gene.data = foldchanges,\n        pathway.id = gseaKEGG_results$ID[1],\n        species = \"hsa\",\n        limit = list(gene = 2, # value gives the max/min limit for foldchanges\n        cpd = 1))\nknitr::include_graphics(paste0(\"./\",gseaKEGG_results$ID[1],\".png\"))"
  },
  {
    "objectID": "develop/09_summarized_workflow.html#make-sure-to-output-the-versions-of-all-tools-used-in-the-de-analysis",
    "href": "develop/09_summarized_workflow.html#make-sure-to-output-the-versions-of-all-tools-used-in-the-de-analysis",
    "title": "Summarised workflow",
    "section": "Make sure to output the versions of all tools used in the DE analysis:",
    "text": "Make sure to output the versions of all tools used in the DE analysis:\nsessionInfo()"
  },
  {
    "objectID": "develop/rmd/08a_FA_genomic_annotation.html",
    "href": "develop/rmd/08a_FA_genomic_annotation.html",
    "title": "Genomic annotations for functional analyses",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 30 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1.  Discuss the available genomic annotation databases and the different types of information stored\n2.  Compare and contrast the tools available for accessing genomic annotation databases\n3.  Apply various R packages for retrieval of genomic annotations\nThe analysis of next-generation sequencing results requires associating genes, transcripts, proteins, etc. with functional or regulatory information. To perform functional analysis on gene lists, we often need to obtain gene identifiers that are compatible with the tools we wish to use and this is not always trivial. Here, we discuss ways in which you can obtain gene annotation information and some of the advantages and disadvantages of each method.\n\n\nWe retrieve information on the processes, pathways, etc. (for which a gene is involved in) from the necessary database where the information is stored. The database you choose will be dependent on what type of information you are trying to obtain. Examples of databases that are often queried, include:\nGeneral databases\nOffer comprehensive information on genome features, feature coordinates, homology, variant information, phenotypes, protein domain/family information, associated biological processes/pathways, associated microRNAs, etc.:\n\nEnsembl (use Ensembl gene IDs)\nNCBI (use Entrez gene IDs)\nUCSC\nEMBL-EBI\n\nAnnotation-specific databases\nProvide annotations related to a specific topic:\n\nGene Ontology (GO): database of gene ontology biological processes, cellular components and molecular functions - based on Ensembl or Entrez gene IDs or official gene symbols\nKEGG: database of biological pathways - based on Entrez gene IDs\nMSigDB: database of gene sets\nReactome: database of biological pathways\nHuman Phenotype Ontology: database of genes associated with human disease\nCORUM: database of protein complexes for human, mouse, rat\n‚Ä¶\n\nThis is by no means an exhaustive list, there are many other databases available that are not listed here.\n\n\n\nBefore you begin your search through any of these databases, you should know which build of the genome was used to generate your gene list and make sure you use the same build for the annotations during functional analysis. When a new genome build is acquired, the names and/or coordinate location of genomic features (gene, transcript, exon, etc.) may change. Therefore, the annotations regarding genome features (gene, transcript, exon, etc.) is genome-build specific and we need to make sure that our annotations are obtained from the appropriate resource.\nFor example, we have used the GRCh38 build of the human genome to quantify gene expression for differential expression analysis, so we should use the same GRCh38 build of the genome to convert between gene IDs and to identify annotations for each of the genes.\n\n\n\nWithin R, there are many popular packages used for gene/transcript-level annotation. These packages provide tools that take the list of genes you provide and retrieve information for each gene using one or more of the databases listed above.\n\n\n{{ read_table(‚Äò./assets/annotation_tools.tsv‚Äô) }}\n\n\n\nOther packages are design for accessing/querying annotations from multiple different annotation sources\n\nAnnotationDbi: queries the OrgDb, TxDb, Go.db, EnsDb, and BioMart annotations.\n\nAnnotationHub: queries large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc.\n\n!!! tip\nThese are both packages that can be used to create the `tx2gene` files that salmon gave us in case you did not have them.\n\n\n\n\nAnnotationDbi is an R package that provides an interface for connecting and querying various annotation databases using SQLite data storage. The AnnotationDbi packages can query the OrgDb, TxDb, EnsDb, Go.db, and BioMart annotations. There is helpful documentation available to reference when extracting data from any of these databases.\n\n\n\nAnnotationHub is a wonderful resource for accessing genomic data or querying large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc. All of this information is stored and easily accessible by directly connecting to the database.\nTo get started with AnnotationHub, we first load the library and connect to the database:\n\n\nCode\n# Load libraries\nlibrary(AnnotationHub)\nlibrary(ensembldb)\n\n# Connect to AnnotationHub\nah &lt;- AnnotationHub()\n\n\n!!! warning\nThe script will ask you to create a cache directory, type yes!\n!!! info ‚ÄúWhat is a cache?‚Äù\nA cache is used in R to store data or a copy of the data so that future requests can be served faster without having to re-run a lengthy computation.\n\nThe `AnnotationHub()` command creates a client that manages a local cache of the database, helping with quick and reproducible access. When encountering question `AnnotationHub does not exist, create directory?`, you can anwser either `yes` (create a permanent location to store cache) or `no` (create a temporary location to store cache). `hubCache(ah)` gets the file system location of the local AnnotationHub cache. `hubUrl(ah)` gets the URL for the online hub. \nTo see the types of information stored inside our database, we can just type the name of the object:\n!!! note\nResults here will differ from yours\n\n\nCode\n# Explore the AnnotationHub object\nah\n\n\nUsing the output, you can get an idea of the information that you can query within the AnnotationHub object. (Note that the output below will be different than yours!)\nAnnotationHub with 47240 records\n# snapshotDate(): 2019-10-29 \n# $dataprovider: BroadInstitute, Ensembl, UCSC, ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/, H...\n# $species: Homo sapiens, Mus musculus, Drosophila melanogaster, Bos taurus, Pan troglod...\n# $rdataclass: GRanges, BigWigFile, TwoBitFile, Rle, OrgDb, EnsDb, ChainFile, TxDb, Inpa...\n# additional mcols(): taxonomyid, genome, description, coordinate_1_based,\n#   maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl,\n#   sourcetype \n# retrieve records with, e.g., 'object[[\"AH5012\"]]' \n\n            title                                                                   \n  AH5012  | Chromosome Band                                                         \n  AH5013  | STS Markers                                                             \n  AH5014  | FISH Clones                                                             \n  AH5015  | Recomb Rate                                                             \n  AH5016  | ENCODE Pilot                                                            \n  ...       ...                                                                     \n  AH78364 | Xiphophorus_maculatus.X_maculatus-5.0-male.ncrna.2bit                   \n  AH78365 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.cdna.all.2bit       \n  AH78366 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_rm.toplevel.2bit\n  AH78367 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_sm.toplevel.2bit\n  AH78368 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.ncrna.2bit    \nNotice the note on retrieving records with object[[\"AH5012\"]] - this will be how we can extract a single record from the AnnotationHub object.\nIf you would like to see more information about any of the classes of data you can extract that information as well. For example, if you wanted to determine all species information available, you could explore that within the AnnotationHub object:\n\n\nCode\n# Explore all species information available\nunique(ah$species) %&gt;% head()\n\n\nIn addition to species information, there is also additional information about the type of Data Objects and the Data Providers:\n\n\nCode\n# Explore the types of Data Objects available\nunique(ah$rdataclass) %&gt;% head()\n\n# Explore the Data Providers\nunique(ah$dataprovider) %&gt;% head()\n\n\nNow that we know the types of information available from AnnotationHub we can query it for the information we want using the query() function. Let‚Äôs say we would like to return the Ensembl EnsDb information for Human. To return the records available, we need to use the terms as they are output from the ah object to extract the desired data.\n\n\nCode\n# Query AnnotationHub\nhuman_ens &lt;- query(ah, c(\"Homo sapiens\", \"EnsDb\"))\n\n\nThe query retrieves all hits for the EnsDb objects, and you will see that they are listed by the release number. The most current release for GRCh38 is Ensembl98 and AnnotationHub offers that as an option to use. However, if you look at options for older releases, for Homo sapiens it only go back as far as Ensembl 87. This is fine if you are using GRCh38, however if you were using an older genome build like hg19/GRCh37, you would need to load the EnsDb package if available for that release or you might need to build your own with ensembldb.\n!!! note\nResults here will differ from yours\n\n\nCode\nhuman_ens\n\n\nAnnotationHub with 13 records\n# snapshotDate(): 2019-10-29 \n# $dataprovider: Ensembl\n# $species: Homo sapiens\n# $rdataclass: EnsDb\n# additional mcols(): taxonomyid, genome, description, coordinate_1_based,\n#   maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl,\n#   sourcetype \n# retrieve records with, e.g., 'object[[\"AH53211\"]]' \n\n            title                            \n  AH53211 | Ensembl 87 EnsDb for Homo Sapiens\n  AH53715 | Ensembl 88 EnsDb for Homo Sapiens\n  AH56681 | Ensembl 89 EnsDb for Homo Sapiens\n  AH57757 | Ensembl 90 EnsDb for Homo Sapiens\n  AH60773 | Ensembl 91 EnsDb for Homo Sapiens\n  ...       ...                              \n  AH67950 | Ensembl 95 EnsDb for Homo sapiens\n  AH69187 | Ensembl 96 EnsDb for Homo sapiens\n  AH73881 | Ensembl 97 EnsDb for Homo sapiens\n  AH73986 | Ensembl 79 EnsDb for Homo sapiens\n  AH75011 | Ensembl 98 EnsDb for Homo sapiens\nIn our case, we are looking for the latest Ensembl release so that the annotations are the most up-to-date. To extract this information from AnnotationHub, we can use the AnnotationHub ID to subset the object:\n\n\nCode\n# Extract annotations of interest\nhuman_ens &lt;- human_ens[[length(human_ens)]] # We extract latest\n\n\nNow we can use ensembldb functions to extract the information at the gene, transcript, or exon levels. We are interested in the gene-level annotations, so we can extract that information as follows:\n\n\nCode\n# Extract gene-level information\ngenes(human_ens, return.type = \"data.frame\") %&gt;% head()\n\n\nBut note that it is just as easy to get the transcript- or exon-level information:\n\n\nCode\n# Extract transcript-level information\ntranscripts(human_ens, return.type = \"data.frame\") %&gt;% head()\n\n# Extract exon-level information\nexons(human_ens, return.type = \"data.frame\") %&gt;% head()\n\n\nTo obtain an annotation data frame using AnnotationHub, we‚Äôll use the genes() function, but only keep selected columns and filter out rows to keep those corresponding to our gene identifiers in our results file:\n\n\nCode\n# Create a gene-level dataframe \nannotations_ahb &lt;- genes(human_ens, return.type = \"data.frame\")  %&gt;%\n  dplyr::select(gene_id, gene_name, entrezid, gene_biotype, description) %&gt;% \n  dplyr::filter(gene_id %in% res_tableCont_tb$gene)\n\n\nThis dataframe looks like it should be fine as it is, but we look a little closer we will notice that the column containing Entrez identifiers is a list, and in fact there are many Ensembl identifiers that map to more than one Entrez identifier!\n\n\nCode\n# Wait a second, we don't have one-to-one mappings!\nclass(annotations_ahb$entrezid)\nwhich(map(annotations_ahb$entrezid, length) &gt; 1)\n\n\nSo what do we do here? And why do we have this problem? An answer from the Ensembl Help Desk is that this occurs when we cannot choose a perfect match; ie when we have two good matches, but one does not appear to match with a better percentage than the other. In that case, we assign both matches. What we will do is choose to keep the first identifier for these multiple mapping cases.\n\n\nCode\nannotations_ahb$entrezid &lt;- map(annotations_ahb$entrezid,1) %&gt;%  unlist()\n\n\n!!! info\nNot all databases handle multiple mappings in the same way. For example, if we used the OrgDb instead of the EnsDb:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhuman_orgdb &lt;- query(ah, c(\"Homo sapiens\", \"OrgDb\"))\nhuman_orgdb &lt;- human_ens[[length(human_ens)]]\nannotations_orgdb &lt;- select(human_orgdb, res_tableCont_tb$gene, c(\"SYMBOL\", \"GENENAME\", \"ENTREZID\"), \"ENSEMBL\")\n```\n:::\n\n\nWe would find that multiple mapping entries would be automatically reduced to one-to-one. We would also find that more than half of the input genes do not return any annotations. This is because the OrgDb family of database are primarily based on mapping using Entrez Gene identifiers. Since our data is based on Ensembl mappings, using the OrgDb would result in a loss of information.\nLet‚Äôs take a look and see how many of our Ensembl identifiers have an associated gene symbol, and how many of them are unique:\n\n\nCode\nwhich(is.na(annotations_ahb$gene_name)) %&gt;% length()\n\nwhich(duplicated(annotations_ahb$gene_name)) %&gt;% length()\n\n\nLet‚Äôs identify the non-duplicated genes and only keep the ones that are not duplicated:\n\n\nCode\n# Determine the indices for the non-duplicated genes\nnon_duplicates_idx &lt;- which(duplicated(annotations_ahb$gene_name) == FALSE)\n\n# How many rows does annotations_ahb have?\nannotations_ahb %&gt;% nrow()\n\n# Return only the non-duplicated genes using indices\nannotations_ahb &lt;- annotations_ahb[non_duplicates_idx, ]\n\n# How many rows are we left with after removing?\nannotations_ahb %&gt;% nrow()\n\n\nFinally, it would be good to know what proportion of the Ensembl identifiers map to an Entrez identifier:\n\n\nCode\n# Determine how many of the Entrez column entries are NA\nwhich(is.na(annotations_ahb$entrezid)) %&gt;%  length()\n\n\nThat‚Äôs more than half of our genes! If we plan on using Entrez ID results for downstream analysis, we should definitely keep this in mind. If you look at some of the Ensembl IDs from our query that returned NA, these map to pseudogenes (i.e ENSG00000265439) or non-coding RNAs (i.e.¬†ENSG00000265425). The discrepancy (which we can expect to observe) between databases is due to the fact that each implements its own different computational approaches for generating the gene builds.\n\n\nTo create our tx2gene file, we would need to use a combination of the methods above and merge two dataframes together. For example:\n\n\nCode\n## DO NOT RUN THIS CODE\n\n# Create a transcript dataframe\n txdb &lt;- transcripts(human_ens, return.type = \"data.frame\") %&gt;%\n   dplyr::select(tx_id, gene_id)\n txdb &lt;- txdb[grep(\"ENST\", txdb$tx_id),]\n \n # Create a gene-level dataframe\n genedb &lt;- genes(human_ens, return.type = \"data.frame\")  %&gt;%\n   dplyr::select(gene_id, gene_name)\n \n # Merge the two dataframes together\n annotations &lt;- inner_join(txdb, genedb)\n\n\nIn this lesson our focus has been using annotation packages to extract information mainly just for gene ID conversion for the different tools that we use downstream. Many of the annotation packages we have presented have much more information than what we need for functional analysis and we have only just scratched the surface here. It‚Äôs good to know the capabilities of the tools we use, so we encourage you to spend some time exploring these packages to become more familiar with them.\n\n\n\n\nThe annotables package is a super easy annotation package to use. It is not updated frequently, so it‚Äôs not great for getting the most up-to-date information for the current builds and does not have information for other organisms than human and mouse, but is a quick way to get annotation information.\n\n\nCode\n# Install package\nBiocManager::install(\"annotables\")\n\n# Load library\nlibrary(annotables)\n\n# Access previous build of annotations\ngrch38\n\n\nWe can see that the grch38 object already contains all the information we want in a super easy way. Let‚Äôs annotate the results of our shrunken DEA for Control vs Vampirium:\n\n\nCode\n## Re-run this code if you are unsure that you have the right table\nres_tableCont &lt;- lfcShrink(dds, coef = \"condition_control_vs_vampirium\")\nres_tableCont_tb &lt;- res_tableCont %&gt;%\n    data.frame() %&gt;%\n    rownames_to_column(var=\"gene\") %&gt;% \n    as_tibble()\n\n\n\n\nCode\n## Return the IDs for the gene symbols in the DE results\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableCont))\n\n## Merge the IDs with the results \nres_ids &lt;- inner_join(res_tableCont_tb, ids, by=c(\"gene\"=\"ensgene\"))\n\nhead(res_ids)\n\n\nOur data is now ready to use for functional analysis! We have all the ids necessary to proceed.\n!!! question ‚ÄúExercise 1‚Äù\n- Create a new `res_ids` object using the `annotables` package with the human build grch37. **NOTE** call it `res_ids_grch37`!\n- What are the differences between the `res_id_ahb`object and the `res_ids_grch37`?\n??? question ‚ÄúSolution to Exercise 1‚Äù\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nids_grch37 &lt;- grch37 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableCont))\n\nres_ids_grch37 &lt;- inner_join(res_tableCont_tb, ids_grch38, by=c(\"gene\"=\"ensgene\"))\n  \nhead(res_ids_grch37)\n```\n:::\n\n\nLet's compare it to the `res_ids_ahb` object\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(res_ids_ahb)\n```\n:::\n\n\nWe can see that `res_id_ahb` contains less columns, but this is because we selected fewer columns in our previous steps. What about the the sizes of these tables?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(res_ids_ahb)\nnrow(res_ids_grch37)\nnrow(res_tableCont_tb)\n```\n:::\n\n\nWe see that there is a difference in the number of genes. So what is happening? The gene IDs that we have in our count data are from the genome version grch37, and we are trying to match it to annotations from the more updated version grch38. There will be genes that are missing just because of the version. Then we have also removed duplicated gene names in our `annotation_ahb` object, which may contain different gene IDs. So we may have deleted some gene IDs that are not matching anymore with our results table. In any case, we should always annotate our genes with the version of the reference genome we used for alignment and quantification!\n!!! question ‚ÄúExercise 2‚Äù\nAnnotate the results of your DEA for Garlicum vs Vampirium with grch38.\n??? question ‚ÄúSolution to Exercise 2‚Äù\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Return the IDs for the gene symbols in the DE results\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableGar))\n\n## Merge the IDs with the results \nres_ids_Gar &lt;- inner_join(res_tableGar_tb, ids, by=c(\"gene\"=\"ensgene\"))\n\nhead(res_ids_Gar)\n```\n:::\n\nThis lesson was originally developed by members of the teaching team (Mary Piper) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/08a_FA_genomic_annotation.html#databases",
    "href": "develop/rmd/08a_FA_genomic_annotation.html#databases",
    "title": "Genomic annotations for functional analyses",
    "section": "",
    "text": "We retrieve information on the processes, pathways, etc. (for which a gene is involved in) from the necessary database where the information is stored. The database you choose will be dependent on what type of information you are trying to obtain. Examples of databases that are often queried, include:\nGeneral databases\nOffer comprehensive information on genome features, feature coordinates, homology, variant information, phenotypes, protein domain/family information, associated biological processes/pathways, associated microRNAs, etc.:\n\nEnsembl (use Ensembl gene IDs)\nNCBI (use Entrez gene IDs)\nUCSC\nEMBL-EBI\n\nAnnotation-specific databases\nProvide annotations related to a specific topic:\n\nGene Ontology (GO): database of gene ontology biological processes, cellular components and molecular functions - based on Ensembl or Entrez gene IDs or official gene symbols\nKEGG: database of biological pathways - based on Entrez gene IDs\nMSigDB: database of gene sets\nReactome: database of biological pathways\nHuman Phenotype Ontology: database of genes associated with human disease\nCORUM: database of protein complexes for human, mouse, rat\n‚Ä¶\n\nThis is by no means an exhaustive list, there are many other databases available that are not listed here."
  },
  {
    "objectID": "develop/rmd/08a_FA_genomic_annotation.html#genome-builds",
    "href": "develop/rmd/08a_FA_genomic_annotation.html#genome-builds",
    "title": "Genomic annotations for functional analyses",
    "section": "",
    "text": "Before you begin your search through any of these databases, you should know which build of the genome was used to generate your gene list and make sure you use the same build for the annotations during functional analysis. When a new genome build is acquired, the names and/or coordinate location of genomic features (gene, transcript, exon, etc.) may change. Therefore, the annotations regarding genome features (gene, transcript, exon, etc.) is genome-build specific and we need to make sure that our annotations are obtained from the appropriate resource.\nFor example, we have used the GRCh38 build of the human genome to quantify gene expression for differential expression analysis, so we should use the same GRCh38 build of the genome to convert between gene IDs and to identify annotations for each of the genes."
  },
  {
    "objectID": "develop/rmd/08a_FA_genomic_annotation.html#tools-for-accessing-databases",
    "href": "develop/rmd/08a_FA_genomic_annotation.html#tools-for-accessing-databases",
    "title": "Genomic annotations for functional analyses",
    "section": "",
    "text": "Within R, there are many popular packages used for gene/transcript-level annotation. These packages provide tools that take the list of genes you provide and retrieve information for each gene using one or more of the databases listed above.\n\n\n{{ read_table(‚Äò./assets/annotation_tools.tsv‚Äô) }}\n\n\n\nOther packages are design for accessing/querying annotations from multiple different annotation sources\n\nAnnotationDbi: queries the OrgDb, TxDb, Go.db, EnsDb, and BioMart annotations.\n\nAnnotationHub: queries large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc.\n\n!!! tip\nThese are both packages that can be used to create the `tx2gene` files that salmon gave us in case you did not have them."
  },
  {
    "objectID": "develop/rmd/08a_FA_genomic_annotation.html#annotationdbi",
    "href": "develop/rmd/08a_FA_genomic_annotation.html#annotationdbi",
    "title": "Genomic annotations for functional analyses",
    "section": "",
    "text": "AnnotationDbi is an R package that provides an interface for connecting and querying various annotation databases using SQLite data storage. The AnnotationDbi packages can query the OrgDb, TxDb, EnsDb, Go.db, and BioMart annotations. There is helpful documentation available to reference when extracting data from any of these databases."
  },
  {
    "objectID": "develop/rmd/08a_FA_genomic_annotation.html#annotationhub",
    "href": "develop/rmd/08a_FA_genomic_annotation.html#annotationhub",
    "title": "Genomic annotations for functional analyses",
    "section": "",
    "text": "AnnotationHub is a wonderful resource for accessing genomic data or querying large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc. All of this information is stored and easily accessible by directly connecting to the database.\nTo get started with AnnotationHub, we first load the library and connect to the database:\n\n\nCode\n# Load libraries\nlibrary(AnnotationHub)\nlibrary(ensembldb)\n\n# Connect to AnnotationHub\nah &lt;- AnnotationHub()\n\n\n!!! warning\nThe script will ask you to create a cache directory, type yes!\n!!! info ‚ÄúWhat is a cache?‚Äù\nA cache is used in R to store data or a copy of the data so that future requests can be served faster without having to re-run a lengthy computation.\n\nThe `AnnotationHub()` command creates a client that manages a local cache of the database, helping with quick and reproducible access. When encountering question `AnnotationHub does not exist, create directory?`, you can anwser either `yes` (create a permanent location to store cache) or `no` (create a temporary location to store cache). `hubCache(ah)` gets the file system location of the local AnnotationHub cache. `hubUrl(ah)` gets the URL for the online hub. \nTo see the types of information stored inside our database, we can just type the name of the object:\n!!! note\nResults here will differ from yours\n\n\nCode\n# Explore the AnnotationHub object\nah\n\n\nUsing the output, you can get an idea of the information that you can query within the AnnotationHub object. (Note that the output below will be different than yours!)\nAnnotationHub with 47240 records\n# snapshotDate(): 2019-10-29 \n# $dataprovider: BroadInstitute, Ensembl, UCSC, ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/, H...\n# $species: Homo sapiens, Mus musculus, Drosophila melanogaster, Bos taurus, Pan troglod...\n# $rdataclass: GRanges, BigWigFile, TwoBitFile, Rle, OrgDb, EnsDb, ChainFile, TxDb, Inpa...\n# additional mcols(): taxonomyid, genome, description, coordinate_1_based,\n#   maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl,\n#   sourcetype \n# retrieve records with, e.g., 'object[[\"AH5012\"]]' \n\n            title                                                                   \n  AH5012  | Chromosome Band                                                         \n  AH5013  | STS Markers                                                             \n  AH5014  | FISH Clones                                                             \n  AH5015  | Recomb Rate                                                             \n  AH5016  | ENCODE Pilot                                                            \n  ...       ...                                                                     \n  AH78364 | Xiphophorus_maculatus.X_maculatus-5.0-male.ncrna.2bit                   \n  AH78365 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.cdna.all.2bit       \n  AH78366 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_rm.toplevel.2bit\n  AH78367 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_sm.toplevel.2bit\n  AH78368 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.ncrna.2bit    \nNotice the note on retrieving records with object[[\"AH5012\"]] - this will be how we can extract a single record from the AnnotationHub object.\nIf you would like to see more information about any of the classes of data you can extract that information as well. For example, if you wanted to determine all species information available, you could explore that within the AnnotationHub object:\n\n\nCode\n# Explore all species information available\nunique(ah$species) %&gt;% head()\n\n\nIn addition to species information, there is also additional information about the type of Data Objects and the Data Providers:\n\n\nCode\n# Explore the types of Data Objects available\nunique(ah$rdataclass) %&gt;% head()\n\n# Explore the Data Providers\nunique(ah$dataprovider) %&gt;% head()\n\n\nNow that we know the types of information available from AnnotationHub we can query it for the information we want using the query() function. Let‚Äôs say we would like to return the Ensembl EnsDb information for Human. To return the records available, we need to use the terms as they are output from the ah object to extract the desired data.\n\n\nCode\n# Query AnnotationHub\nhuman_ens &lt;- query(ah, c(\"Homo sapiens\", \"EnsDb\"))\n\n\nThe query retrieves all hits for the EnsDb objects, and you will see that they are listed by the release number. The most current release for GRCh38 is Ensembl98 and AnnotationHub offers that as an option to use. However, if you look at options for older releases, for Homo sapiens it only go back as far as Ensembl 87. This is fine if you are using GRCh38, however if you were using an older genome build like hg19/GRCh37, you would need to load the EnsDb package if available for that release or you might need to build your own with ensembldb.\n!!! note\nResults here will differ from yours\n\n\nCode\nhuman_ens\n\n\nAnnotationHub with 13 records\n# snapshotDate(): 2019-10-29 \n# $dataprovider: Ensembl\n# $species: Homo sapiens\n# $rdataclass: EnsDb\n# additional mcols(): taxonomyid, genome, description, coordinate_1_based,\n#   maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl,\n#   sourcetype \n# retrieve records with, e.g., 'object[[\"AH53211\"]]' \n\n            title                            \n  AH53211 | Ensembl 87 EnsDb for Homo Sapiens\n  AH53715 | Ensembl 88 EnsDb for Homo Sapiens\n  AH56681 | Ensembl 89 EnsDb for Homo Sapiens\n  AH57757 | Ensembl 90 EnsDb for Homo Sapiens\n  AH60773 | Ensembl 91 EnsDb for Homo Sapiens\n  ...       ...                              \n  AH67950 | Ensembl 95 EnsDb for Homo sapiens\n  AH69187 | Ensembl 96 EnsDb for Homo sapiens\n  AH73881 | Ensembl 97 EnsDb for Homo sapiens\n  AH73986 | Ensembl 79 EnsDb for Homo sapiens\n  AH75011 | Ensembl 98 EnsDb for Homo sapiens\nIn our case, we are looking for the latest Ensembl release so that the annotations are the most up-to-date. To extract this information from AnnotationHub, we can use the AnnotationHub ID to subset the object:\n\n\nCode\n# Extract annotations of interest\nhuman_ens &lt;- human_ens[[length(human_ens)]] # We extract latest\n\n\nNow we can use ensembldb functions to extract the information at the gene, transcript, or exon levels. We are interested in the gene-level annotations, so we can extract that information as follows:\n\n\nCode\n# Extract gene-level information\ngenes(human_ens, return.type = \"data.frame\") %&gt;% head()\n\n\nBut note that it is just as easy to get the transcript- or exon-level information:\n\n\nCode\n# Extract transcript-level information\ntranscripts(human_ens, return.type = \"data.frame\") %&gt;% head()\n\n# Extract exon-level information\nexons(human_ens, return.type = \"data.frame\") %&gt;% head()\n\n\nTo obtain an annotation data frame using AnnotationHub, we‚Äôll use the genes() function, but only keep selected columns and filter out rows to keep those corresponding to our gene identifiers in our results file:\n\n\nCode\n# Create a gene-level dataframe \nannotations_ahb &lt;- genes(human_ens, return.type = \"data.frame\")  %&gt;%\n  dplyr::select(gene_id, gene_name, entrezid, gene_biotype, description) %&gt;% \n  dplyr::filter(gene_id %in% res_tableCont_tb$gene)\n\n\nThis dataframe looks like it should be fine as it is, but we look a little closer we will notice that the column containing Entrez identifiers is a list, and in fact there are many Ensembl identifiers that map to more than one Entrez identifier!\n\n\nCode\n# Wait a second, we don't have one-to-one mappings!\nclass(annotations_ahb$entrezid)\nwhich(map(annotations_ahb$entrezid, length) &gt; 1)\n\n\nSo what do we do here? And why do we have this problem? An answer from the Ensembl Help Desk is that this occurs when we cannot choose a perfect match; ie when we have two good matches, but one does not appear to match with a better percentage than the other. In that case, we assign both matches. What we will do is choose to keep the first identifier for these multiple mapping cases.\n\n\nCode\nannotations_ahb$entrezid &lt;- map(annotations_ahb$entrezid,1) %&gt;%  unlist()\n\n\n!!! info\nNot all databases handle multiple mappings in the same way. For example, if we used the OrgDb instead of the EnsDb:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhuman_orgdb &lt;- query(ah, c(\"Homo sapiens\", \"OrgDb\"))\nhuman_orgdb &lt;- human_ens[[length(human_ens)]]\nannotations_orgdb &lt;- select(human_orgdb, res_tableCont_tb$gene, c(\"SYMBOL\", \"GENENAME\", \"ENTREZID\"), \"ENSEMBL\")\n```\n:::\n\n\nWe would find that multiple mapping entries would be automatically reduced to one-to-one. We would also find that more than half of the input genes do not return any annotations. This is because the OrgDb family of database are primarily based on mapping using Entrez Gene identifiers. Since our data is based on Ensembl mappings, using the OrgDb would result in a loss of information.\nLet‚Äôs take a look and see how many of our Ensembl identifiers have an associated gene symbol, and how many of them are unique:\n\n\nCode\nwhich(is.na(annotations_ahb$gene_name)) %&gt;% length()\n\nwhich(duplicated(annotations_ahb$gene_name)) %&gt;% length()\n\n\nLet‚Äôs identify the non-duplicated genes and only keep the ones that are not duplicated:\n\n\nCode\n# Determine the indices for the non-duplicated genes\nnon_duplicates_idx &lt;- which(duplicated(annotations_ahb$gene_name) == FALSE)\n\n# How many rows does annotations_ahb have?\nannotations_ahb %&gt;% nrow()\n\n# Return only the non-duplicated genes using indices\nannotations_ahb &lt;- annotations_ahb[non_duplicates_idx, ]\n\n# How many rows are we left with after removing?\nannotations_ahb %&gt;% nrow()\n\n\nFinally, it would be good to know what proportion of the Ensembl identifiers map to an Entrez identifier:\n\n\nCode\n# Determine how many of the Entrez column entries are NA\nwhich(is.na(annotations_ahb$entrezid)) %&gt;%  length()\n\n\nThat‚Äôs more than half of our genes! If we plan on using Entrez ID results for downstream analysis, we should definitely keep this in mind. If you look at some of the Ensembl IDs from our query that returned NA, these map to pseudogenes (i.e ENSG00000265439) or non-coding RNAs (i.e.¬†ENSG00000265425). The discrepancy (which we can expect to observe) between databases is due to the fact that each implements its own different computational approaches for generating the gene builds.\n\n\nTo create our tx2gene file, we would need to use a combination of the methods above and merge two dataframes together. For example:\n\n\nCode\n## DO NOT RUN THIS CODE\n\n# Create a transcript dataframe\n txdb &lt;- transcripts(human_ens, return.type = \"data.frame\") %&gt;%\n   dplyr::select(tx_id, gene_id)\n txdb &lt;- txdb[grep(\"ENST\", txdb$tx_id),]\n \n # Create a gene-level dataframe\n genedb &lt;- genes(human_ens, return.type = \"data.frame\")  %&gt;%\n   dplyr::select(gene_id, gene_name)\n \n # Merge the two dataframes together\n annotations &lt;- inner_join(txdb, genedb)\n\n\nIn this lesson our focus has been using annotation packages to extract information mainly just for gene ID conversion for the different tools that we use downstream. Many of the annotation packages we have presented have much more information than what we need for functional analysis and we have only just scratched the surface here. It‚Äôs good to know the capabilities of the tools we use, so we encourage you to spend some time exploring these packages to become more familiar with them."
  },
  {
    "objectID": "develop/rmd/08a_FA_genomic_annotation.html#annotables-package",
    "href": "develop/rmd/08a_FA_genomic_annotation.html#annotables-package",
    "title": "Genomic annotations for functional analyses",
    "section": "",
    "text": "The annotables package is a super easy annotation package to use. It is not updated frequently, so it‚Äôs not great for getting the most up-to-date information for the current builds and does not have information for other organisms than human and mouse, but is a quick way to get annotation information.\n\n\nCode\n# Install package\nBiocManager::install(\"annotables\")\n\n# Load library\nlibrary(annotables)\n\n# Access previous build of annotations\ngrch38\n\n\nWe can see that the grch38 object already contains all the information we want in a super easy way. Let‚Äôs annotate the results of our shrunken DEA for Control vs Vampirium:\n\n\nCode\n## Re-run this code if you are unsure that you have the right table\nres_tableCont &lt;- lfcShrink(dds, coef = \"condition_control_vs_vampirium\")\nres_tableCont_tb &lt;- res_tableCont %&gt;%\n    data.frame() %&gt;%\n    rownames_to_column(var=\"gene\") %&gt;% \n    as_tibble()\n\n\n\n\nCode\n## Return the IDs for the gene symbols in the DE results\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableCont))\n\n## Merge the IDs with the results \nres_ids &lt;- inner_join(res_tableCont_tb, ids, by=c(\"gene\"=\"ensgene\"))\n\nhead(res_ids)\n\n\nOur data is now ready to use for functional analysis! We have all the ids necessary to proceed.\n!!! question ‚ÄúExercise 1‚Äù\n- Create a new `res_ids` object using the `annotables` package with the human build grch37. **NOTE** call it `res_ids_grch37`!\n- What are the differences between the `res_id_ahb`object and the `res_ids_grch37`?\n??? question ‚ÄúSolution to Exercise 1‚Äù\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nids_grch37 &lt;- grch37 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableCont))\n\nres_ids_grch37 &lt;- inner_join(res_tableCont_tb, ids_grch38, by=c(\"gene\"=\"ensgene\"))\n  \nhead(res_ids_grch37)\n```\n:::\n\n\nLet's compare it to the `res_ids_ahb` object\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(res_ids_ahb)\n```\n:::\n\n\nWe can see that `res_id_ahb` contains less columns, but this is because we selected fewer columns in our previous steps. What about the the sizes of these tables?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(res_ids_ahb)\nnrow(res_ids_grch37)\nnrow(res_tableCont_tb)\n```\n:::\n\n\nWe see that there is a difference in the number of genes. So what is happening? The gene IDs that we have in our count data are from the genome version grch37, and we are trying to match it to annotations from the more updated version grch38. There will be genes that are missing just because of the version. Then we have also removed duplicated gene names in our `annotation_ahb` object, which may contain different gene IDs. So we may have deleted some gene IDs that are not matching anymore with our results table. In any case, we should always annotate our genes with the version of the reference genome we used for alignment and quantification!\n!!! question ‚ÄúExercise 2‚Äù\nAnnotate the results of your DEA for Garlicum vs Vampirium with grch38.\n??? question ‚ÄúSolution to Exercise 2‚Äù\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Return the IDs for the gene symbols in the DE results\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableGar))\n\n## Merge the IDs with the results \nres_ids_Gar &lt;- inner_join(res_tableGar_tb, ids, by=c(\"gene\"=\"ensgene\"))\n\nhead(res_ids_Gar)\n```\n:::\n\nThis lesson was originally developed by members of the teaching team (Mary Piper) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/modify_md.html",
    "href": "develop/rmd/modify_md.html",
    "title": "Bulk RNAseq data analysis",
    "section": "",
    "text": "Code\nimport os\nimport shutil\nimport re\n\n\n\n\nCode\nsource_folder = \"./develop/\"\nimages_folder = \"./img/\"\ndestination_folder = \"../\"\nfiles = os.listdir(source_folder)\n\nprint(files)\n\n\n['08b_FA_overrepresentation.md']\n\n\n\n\nCode\nfor i in files:\n    if i.endswith(\".md\"):\n        with open(source_folder + i) as f:\n            print(\"Processing:\", i, \"\\n\")\n            text = f.read()\n            text = re.sub(\"\\nknit((.|\\n)*)\\n=+\\n\", \"\\n---\\n\" ,text)\n            text = re.sub(\"(‚Äù|‚Äú)\",\"\\\"\",text)\n            text = re.sub(\"(‚Äò|‚Äô)\",\"\\'\",text)\n            text = re.sub(\"(‚Äò|‚Äô)\",\"\\'\",text)\n            text = re.sub(\"¬†\",\" \",text)\n            text = re.sub(\"\\\\&gt;\",\"&gt;\",text)\n            text = re.sub(\"\\\\&lt;\",\"&lt;\",text)\n        with open(source_folder + i, \"w\") as f:\n            f.write(text)\n\n\nProcessing: 08b_FA_overrepresentation.md \n\n\n\n\n\nCode\nfor file_name in os.listdir(source_folder):\n    # construct full file path\n    source = source_folder + file_name\n    destination = destination_folder + file_name\n    shutil.move(source, destination)\n    print('Moved:', file_name)\n\n\nMoved: 08b_FA_overrepresentation.md\n\n\n\n\nCode\n\ndef copy_directory(src_dir, dest_dir):\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n    for item in os.listdir(src_dir):\n        src_path = os.path.join(src_dir, item)\n        dest_path = os.path.join(dest_dir, item)\n        if os.path.isdir(src_path):\n            copy_directory(src_path, dest_path)\n        else:\n            shutil.copy2(src_path, dest_path)\n\ncopy_directory(images_folder, destination_folder+\"img/\")"
  },
  {
    "objectID": "develop/rmd/05b_count_matrix.html",
    "href": "develop/rmd/05b_count_matrix.html",
    "title": "The RNAseq count matrix",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 20 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1. Describe how to set up an RNA-seq project in R \n2. Describe RNA-seq data and the differential gene expression analysis workflow\n3. Load and create a count matrix from our preprocessing analysis using Salmon\n4. Explain why negative binomial distribution is used to model RNA-seq count data\nThe goal of RNA-seq is often to perform differential expression testing to determine which genes are expressed at different levels between conditions. These genes can offer biological insight into the processes affected by the condition(s) of interest.\nTo determine the expression levels of genes, our RNA-seq workflow followed the steps detailed in the image below.\n\n\n\n\n\n\n\n\n\nAll steps were performed using the nf-core RNAseq pipeline in our previous lesson. The differential expression analysis and any downstream functional analysis are generally performed in R using R packages specifically designed for the complex statistical analyses required to determine whether genes are differentially expressed.\nIn the next few lessons, we will walk you through an end-to-end gene-level RNA-seq differential expression workflow using various R packages. We will start with the count matrix, do some exploratory data analysis for quality assessment and explore the relationship between samples. Next, we will perform differential expression analysis, and visually explore the results prior to performing downstream functional analysis.\n\n\nBefore we get into the details of the analysis, let‚Äùs get started by opening up RStudio and setting up a new project for this analysis.\n\nGo to the File menu and select New Project.\nIn the New Project window, choose Existing Directory. Then, choose Intro_to_bulkRNAseq as your project working directory.\nThe new project should automatically open in RStudio.\n\nTo check whether or not you are in the correct working directory, use getwd(). The path /work/Intro_to_bulkRNAseq should be returned to you in the console. When finished your working directory should now look similar to this:\n\n\n\n\n\n\n\n\n\n\nInside the folder Notebooks you will find the scripts (in Rmd format) that we will follow during the sessions.\nIn the folder Results you will save the results of your scripts, analysis and tests.\n\nTo avoid copying the original dataset for each student (very inefficient) a backup of the preprocessing results is inside this folder /work/Intro_to_bulkRNAseq/Data/. You are also very welcome to use your own preprocessing results!\nNow you can open the first practical session: 05b_count_matrix.Rmd\n\n\nFor this analysis we will be using several R packages, some which have been installed from CRAN and others from Bioconductor. To use these packages (and the functions contained within them), we need to load the libraries. Add the following to your script and don‚Äùt forget to comment liberally!\n\n\nCode\nlibrary(tidyverse)\nlibrary(DESeq2)\nlibrary(tximport)\n\n# And with this last line of code, we set our working directory to the folder with this notebook.\n# This way, the relative paths will work without issues\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\n\nThe directories of output from the mapping/quantification step of the workflow (Salmon) is the data that we will be using. These transcript abundance estimates, often referred to as ‚Äúpseudocounts‚Äù, will be the starting point for our differential gene expression analysis. The main output of Salmon is a quant.sf file, and we have one of these for each individual sample in our dataset.\nFor the sake of reproducibility, we will be using the backup results from our preprocessing pipeline. You are welcome to use your own results!\n\n\nCode\n# Tabulated separated files can be opened using the read_table() function.\nread_table(\"/work/Intro_to_bulkRNAseq/Data/salmon/control_1/quant.sf\", ) %&gt;% head()\n\n\nFor each transcript that was assayed in the reference, we have:\n\nThe transcript identifier\nThe transcript length (in bp)\nThe effective length (described in detail below)\nTPM (transcripts per million), which is computed using the effective length\nThe estimated read count (‚Äúpseudocount‚Äù)\n\n!!! note ‚ÄúWhat exactly is the effective length?‚Äù\nThe sequence composition of a transcript affects how many reads are sampled from it. While two transcripts might be of identical actual length, depending on the sequence composition we are more likely to generate fragments from one versus the other. The transcript that has a higher likelihood of being sampled, will end up with the larger effective length. The effective length is transcript length which has been \"corrected\" to include factors due to sequence-specific and GC biases.\nWe will be using the R Bioconductor package tximport to prepare the quant.sf files for DESeq2. The first thing we need to do is create a variable that contains the paths to each of our quant.sf files. Then we will add names to our quant files which will allow us to easily distinguish between samples in the final output matrix.\nWe will use the samplesheet.csv file that we use to process our raw reads, since it already contains all the information we need to run our analysis.\n\n\nCode\n# Load metadata\nmeta &lt;- read_csv(\"/work/Intro_to_bulkRNAseq/Data/samplesheet.csv\")\n\n# View metadata\nmeta\n\n\nUsing the samples column, we can create all the paths needed:\n\n\nCode\n# Directory where salmon files are. You can change this path to the results of your own analysis\ndir &lt;- \"/work/Intro_to_bulkRNAseq/Data\"\n\n# List all directories containing quant.sf files using the samplename column of metadata\nfiles &lt;- file.path(dir,\"salmon\", meta$sample, \"quant.sf\")\n\n# Name the file list with the samplenames\nnames(files) &lt;- meta$sample\nfiles\n\n\nOur Salmon files were generated with transcript sequences listed by Ensembl IDs, but tximport needs to know which genes these transcripts came from. We will use annotation table the that was created in our workflow, called tx2gene.txt.\n\n\nCode\ntx2gene &lt;- read_table(\"/work/Intro_to_bulkRNAseq/Data/salmon/salmon_tx2gene.tsv\", col_names = c(\"transcript_ID\",\"gene_ID\",\"gene_symbol\"))\ntx2gene %&gt;% head()\n\n\ntx2gene is a three-column data frame linking transcript ID (column 1) to gene ID (column 2) to gene symbol (column 3). We will take the first two columns as input to tximport. The column names are not relevant, but the column order is (i.e transcript ID must be first).\nNow we are ready to run tximport. The tximport() function imports transcript-level estimates from various external software (e.g.¬†Salmon, Kallisto) and summarizes to the gene-level (default) or outputs transcript-level matrices. There are optional arguments to use the abundance estimates as they appear in the quant.sf files or to calculate alternative values.\nFor our analysis we need non-normalized or ‚Äúraw‚Äù count estimates at the gene-level for performing DESeq2 analysis.\nSince the gene-level count matrix is a default (txOut=FALSE) there is only one additional argument for us to modify to specify how to obtain our ‚Äúraw‚Äù count values. The options for countsFromAbundance are as follows:\n\nno (default): This will take the values in TPM (as our scaled values) and NumReads (as our ‚Äúraw‚Äù counts) columns, and collapse it down to the gene-level.\nscaledTPM: This is taking the TPM scaled up to library size as our ‚Äúraw‚Äù counts\nlengthScaledTPM: This is used to generate the ‚Äúraw‚Äù count table from the TPM (rather than summarizing the NumReads column). ‚ÄúRaw‚Äù count values are generated by using the TPM value x featureLength x library size. These represent quantities that are on the same scale as original counts, except no longer correlated with transcript length across samples. We will use this option for DESeq2 downstream analysis.\n\nAn additional argument for tximport: When performing your own analysis you may find that the reference transcriptome file you obtain from Ensembl will have version numbers included on your identifiers (i.e ENSG00000265439.2). This will cause a discrepancy with the tx2gene file since the annotation databases don‚Äùt usually contain version numbers (i.e ENSG00000265439). To get around this issue you can use the argument ignoreTxVersion  = TRUE. The logical value indicates whether to split the tx id on the ‚Äú.‚Äù character to remove version information, for easier matching.\n\n\nCode\ntxi &lt;- tximport(files, type=\"salmon\", tx2gene=tx2gene, countsFromAbundance = \"lengthScaledTPM\", ignoreTxVersion = TRUE)\n\n\n\n\n\nThe txi object is a simple list containing matrices of the abundance, counts, length. Another list element ‚ÄúcountsFromAbundance‚Äù carries through the character argument used in the tximport call. The length matrix contains the average transcript length for each gene which can be used as an offset for gene-level analysis.\n\n\nCode\nattributes(txi)\n\n\nWe will be using the txi object as is for input into DESeq2, but will save it until the next lesson. For now let‚Äùs take a look at the count matrix. You will notice that there are decimal values, so let‚Äùs round to the nearest whole number and convert it into a dataframe. We will save it to a variable called data that we can play with.\n\n\nCode\n# Look at the counts\ntxi$counts %&gt;% head()\n\n\n\n\nCode\n# Write the counts to an object\ndata &lt;- txi$counts %&gt;% \n  round() %&gt;% \n  data.frame()\n\n\nThere are a lot of rows with no gene expression at all.\n\n\nCode\nsum(rowSums(data) == 0)\n\n\nLet‚Äôs take them out!\n\n\nCode\nkeep &lt;- rowSums(data) &gt; 0\ndata &lt;- data[keep,]\n\n\n\n\n\n\nSo, what does this count data actually represent? The count data used for differential expression analysis represents the number of sequence reads that originated from a particular gene. The higher the number of counts, the more reads associated with that gene, and the assumption that there was a higher level of expression of that gene in the sample.\n\n\n\n\n\n\n\n\n\nWith differential expression analysis, we are looking for genes that change in expression between two or more groups (defined in the metadata) - case vs control - correlation of expression with some variable or clinical outcome\nWhy does it not work to identify differentially expressed gene by ranking the genes by how different they are between the two groups (based on fold change values)?\n\n\n\n\n\n\n\n\n\nGenes that vary in expression level between groups of samples may do so solely as a consequence of the biological variable(s) of interest. However, this difference is often also related to extraneous effects, in fact, sometimes these effects exclusively account for the observed variation. The goal of differential expression analysis to determine the relative role of these effects, hence separating the ‚Äúinteresting‚Äù variance from the ‚Äúuninteresting‚Äù variance.\n\n\n\n\n\n\n\n\n\nAlthough the mean expression levels between sample groups may appear to be quite different, it is possible that the difference is not actually significant. This is illustrated for ‚ÄúGeneA‚Äù expression between ‚Äúuntreated‚Äù and ‚Äútreated‚Äù groups in the figure below. The mean expression level of geneA for the ‚Äútreated‚Äù group is twice as large as for the ‚Äúuntreated‚Äù group, but the variation between replicates indicates that this may not be a significant difference. We need to take into account the variation in the data (and where it might be coming from) when determining whether genes are differentially expressed.\n\n\n\n\n\n\n\n\n\nDifferential expression analysis is used to determine, for each gene, whether the differences in expression (counts) between groups is significant given the amount of variation observed within groups (replicates). To test for significance, we need an appropriate statistical model that accurately performs normalization (to account for differences in sequencing depth, etc.) and variance modeling (to account for few numbers of replicates and large dynamic expression range).\n\n\n\nTo determine the appropriate statistical model, we need information about the distribution of counts. To get an idea about how RNA-seq counts are distributed, let‚Äùs plot the counts of all the samples:\n\n\nCode\n# Here we format the data into long format instead of wide format\npdata &lt;- data %&gt;% \n  gather(key = Sample, value = Count)\n\npdata\n\n\nAnd we plot our count distribution using all our samples:\n\n\nCode\nggplot(pdata) +\n  geom_density(aes(x = Count, color = Sample)) +\n  xlab(\"Raw expression counts\") +\n  ylab(\"Number of genes\")\n\n\nIf we zoom in close to zero, we can see a large number of genes with counts close to zero:\n\n\nCode\nggplot(pdata) +\n  geom_density(aes(x = Count, color = Sample)) +\n  xlim(-5, 500)  +\n  xlab(\"Raw expression counts\") +\n  ylab(\"Number of genes\")\n\n\nThese images illustrate some common features of RNA-seq count data, including a low number of counts associated with a large proportion of genes, and a long right tail due to the lack of any upper limit for expression. Unlike microarray data, which has a dynamic range maximum limited due to when the probes max out, there is no limit of maximum expression for RNA-seq data. Due to the differences in these technologies, the statistical models used to fit the data are different between the two methods.\n??? note ‚ÄúNote on microarray data distribution‚Äù\nThe log intensities of the microarray data approximate a normal distribution. However, due to the different properties of the of RNA-seq count data, such as integer counts instead of continuous measurements and non-normally distributed data, the normal distribution does not accurately model RNA-seq counts. [More info here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3541212/).\n\n\n\nRNAseq count data can be modeled using a Poisson distribution. this particular distribution is fitting for data where the number of cases is very large but the probability of an event occurring is very small. To give you an example, think of the lottery: many people buy lottery tickets (high number of cases), but only very few win (the probability of the event is small).\nWith RNA-Seq data, a very large number of RNAs are represented and the probability of pulling out a particular transcript is very small. Thus, it would be an appropriate situation to use the Poisson distribution. However, a unique property of this distribution is that the mean == variance. Realistically, with RNA-Seq data there is always some biological variation present across the replicates (within a sample class). Genes with larger average expression levels will tend to have larger observed variances across replicates.\nThe model that fits best, given this type of variability observed for replicates, is the Negative Binomial (NB) model. Essentially, the NB model is a good approximation for data where the mean &lt; variance, as is the case with RNA-Seq count data.\n\n\n\n\n\n\n\n\n\n!!! note ‚ÄúNote on technical replicates‚Äù\n-   **Biological replicates** represent multiple samples (i.e. RNA from different mice) representing the same sample class\n-   **Technical replicates** represent the same sample (i.e. RNA from the same mouse) but with technical steps replicated\n-   Usually biological variance is much greater than technical variance, so we do not need to account for technical variance to identify biological differences in expression\n-   **Don't spend money on technical replicates - biological replicates are much more useful**\n!!! note ‚ÄúNote on cell lines‚Äù\nIf you are using **cell lines** and are unsure whether or not you have prepared biological or technical replicates, take a look at [this link](https://web.archive.org/web/20170807192514/http://www.labstats.net:80/articles/cell_culture_n.html). This is a useful resource in helping you determine how best to set up your *in-vitro* experiment.\n??? note ‚ÄúHow do I know if my data should be modeled using the Poisson distribution or Negative Binomial distribution?‚Äù\nIf it's count data, it should fit the negative binomial, as discussed previously. However, it can be helpful to plot the *mean versus the variance* of your data. *Remember for the Poisson model, mean = variance, but for NB, mean &lt; variance.*\n\nHere we calculate the mean and the variance per gene for all columns and genes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf &lt;- data %&gt;% \nrowwise() %&gt;% \nsummarise(mean_counts = mean(c_across(everything())), \n                        variance_counts = var(c_across(everything())))\n```\n:::\n\n\nRun the following code to plot the *mean versus variance* of each gene for our data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_point(aes(x=mean_counts, y=variance_counts)) + \n  geom_abline(intercept = 0, slope = 1, color=\"red\") +\n  scale_y_log10() +\n  scale_x_log10()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/05b_count_matrix/deseq_mean_vs_variance.png){fig-align='center'}\n:::\n:::\n\n\nNote that in the above figure, the variance across replicates tends to be greater than the mean (red line), especially for genes with large mean expression levels. *This is a good indication that our data do not fit the Poisson distribution and we need to account for this increase in variance using the Negative Binomial model (i.e. Poisson will underestimate variability leading to an increase in false positive DE genes).*\n\n\n\nThe variance or scatter tends to reduce as we increase the number of biological replicates (the distribution will approach the Poisson distribution with increasing numbers of replicates), since standard deviations of averages are smaller than standard deviations of individual observations. The value of additional replicates is that as you add more data (replicates), you get increasingly precise estimates of group means, and ultimately greater confidence in the ability to distinguish differences between sample classes (i.e.¬†more DE genes).\nThe figure below illustrates the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified (from Liu et al.¬†(2013)):\n\n\n\n\n\n\n\n\n\nNote that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth. Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. Generally, the minimum sequencing depth recommended is 20-30 million reads per sample, but we have seen good RNA-seq experiments with 10 million reads if there are a good number of replicates.\n\n\n\n\n\n\n\n\n\n\n\n\nTo model counts appropriately when performing a differential expression analysis, there are a number of software packages that have been developed for differential expression analysis of RNA-seq data. Even as new methods are continuously being developed a few tools are generally recommended as best practice, like DESeq2, EdgeR and Limma-Voom.\nMany studies describing comparisons between these methods show that while there is some agreement, there is also much variability between tools. Additionally, there is no one method that performs optimally under all conditions (Soneson and Dleorenzi, 2013, Corchete et al, 2020).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be using DESeq2 for the DE analysis, and the analysis steps with DESeq2 are shown in the flowchart below in green. DESeq2 first normalizes the count data to account for differences in library sizes and RNA composition between samples. Then, we will use the normalized counts to make some plots for QC at the gene and sample level. The final step is to use the appropriate functions from the DESeq2 package to perform the differential expression analysis.\n\n\n\n\n\n\n\n\n\nWe will go in-depth into each of these steps in the following lessons, but additional details and helpful suggestions regarding DESeq2 can be found in the DESeq2 vignette. As you go through this workflow and questions arise, you can reference the vignette from within RStudio:\nvignette(\"DESeq2\")\nThis is very convenient, as it provides a wealth of information at your fingertips! Be sure to use this as you need during the workshop.\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/05b_count_matrix.html#setting-up",
    "href": "develop/rmd/05b_count_matrix.html#setting-up",
    "title": "The RNAseq count matrix",
    "section": "",
    "text": "Before we get into the details of the analysis, let‚Äùs get started by opening up RStudio and setting up a new project for this analysis.\n\nGo to the File menu and select New Project.\nIn the New Project window, choose Existing Directory. Then, choose Intro_to_bulkRNAseq as your project working directory.\nThe new project should automatically open in RStudio.\n\nTo check whether or not you are in the correct working directory, use getwd(). The path /work/Intro_to_bulkRNAseq should be returned to you in the console. When finished your working directory should now look similar to this:\n\n\n\n\n\n\n\n\n\n\nInside the folder Notebooks you will find the scripts (in Rmd format) that we will follow during the sessions.\nIn the folder Results you will save the results of your scripts, analysis and tests.\n\nTo avoid copying the original dataset for each student (very inefficient) a backup of the preprocessing results is inside this folder /work/Intro_to_bulkRNAseq/Data/. You are also very welcome to use your own preprocessing results!\nNow you can open the first practical session: 05b_count_matrix.Rmd\n\n\nFor this analysis we will be using several R packages, some which have been installed from CRAN and others from Bioconductor. To use these packages (and the functions contained within them), we need to load the libraries. Add the following to your script and don‚Äùt forget to comment liberally!\n\n\nCode\nlibrary(tidyverse)\nlibrary(DESeq2)\nlibrary(tximport)\n\n# And with this last line of code, we set our working directory to the folder with this notebook.\n# This way, the relative paths will work without issues\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\n\nThe directories of output from the mapping/quantification step of the workflow (Salmon) is the data that we will be using. These transcript abundance estimates, often referred to as ‚Äúpseudocounts‚Äù, will be the starting point for our differential gene expression analysis. The main output of Salmon is a quant.sf file, and we have one of these for each individual sample in our dataset.\nFor the sake of reproducibility, we will be using the backup results from our preprocessing pipeline. You are welcome to use your own results!\n\n\nCode\n# Tabulated separated files can be opened using the read_table() function.\nread_table(\"/work/Intro_to_bulkRNAseq/Data/salmon/control_1/quant.sf\", ) %&gt;% head()\n\n\nFor each transcript that was assayed in the reference, we have:\n\nThe transcript identifier\nThe transcript length (in bp)\nThe effective length (described in detail below)\nTPM (transcripts per million), which is computed using the effective length\nThe estimated read count (‚Äúpseudocount‚Äù)\n\n!!! note ‚ÄúWhat exactly is the effective length?‚Äù\nThe sequence composition of a transcript affects how many reads are sampled from it. While two transcripts might be of identical actual length, depending on the sequence composition we are more likely to generate fragments from one versus the other. The transcript that has a higher likelihood of being sampled, will end up with the larger effective length. The effective length is transcript length which has been \"corrected\" to include factors due to sequence-specific and GC biases.\nWe will be using the R Bioconductor package tximport to prepare the quant.sf files for DESeq2. The first thing we need to do is create a variable that contains the paths to each of our quant.sf files. Then we will add names to our quant files which will allow us to easily distinguish between samples in the final output matrix.\nWe will use the samplesheet.csv file that we use to process our raw reads, since it already contains all the information we need to run our analysis.\n\n\nCode\n# Load metadata\nmeta &lt;- read_csv(\"/work/Intro_to_bulkRNAseq/Data/samplesheet.csv\")\n\n# View metadata\nmeta\n\n\nUsing the samples column, we can create all the paths needed:\n\n\nCode\n# Directory where salmon files are. You can change this path to the results of your own analysis\ndir &lt;- \"/work/Intro_to_bulkRNAseq/Data\"\n\n# List all directories containing quant.sf files using the samplename column of metadata\nfiles &lt;- file.path(dir,\"salmon\", meta$sample, \"quant.sf\")\n\n# Name the file list with the samplenames\nnames(files) &lt;- meta$sample\nfiles\n\n\nOur Salmon files were generated with transcript sequences listed by Ensembl IDs, but tximport needs to know which genes these transcripts came from. We will use annotation table the that was created in our workflow, called tx2gene.txt.\n\n\nCode\ntx2gene &lt;- read_table(\"/work/Intro_to_bulkRNAseq/Data/salmon/salmon_tx2gene.tsv\", col_names = c(\"transcript_ID\",\"gene_ID\",\"gene_symbol\"))\ntx2gene %&gt;% head()\n\n\ntx2gene is a three-column data frame linking transcript ID (column 1) to gene ID (column 2) to gene symbol (column 3). We will take the first two columns as input to tximport. The column names are not relevant, but the column order is (i.e transcript ID must be first).\nNow we are ready to run tximport. The tximport() function imports transcript-level estimates from various external software (e.g.¬†Salmon, Kallisto) and summarizes to the gene-level (default) or outputs transcript-level matrices. There are optional arguments to use the abundance estimates as they appear in the quant.sf files or to calculate alternative values.\nFor our analysis we need non-normalized or ‚Äúraw‚Äù count estimates at the gene-level for performing DESeq2 analysis.\nSince the gene-level count matrix is a default (txOut=FALSE) there is only one additional argument for us to modify to specify how to obtain our ‚Äúraw‚Äù count values. The options for countsFromAbundance are as follows:\n\nno (default): This will take the values in TPM (as our scaled values) and NumReads (as our ‚Äúraw‚Äù counts) columns, and collapse it down to the gene-level.\nscaledTPM: This is taking the TPM scaled up to library size as our ‚Äúraw‚Äù counts\nlengthScaledTPM: This is used to generate the ‚Äúraw‚Äù count table from the TPM (rather than summarizing the NumReads column). ‚ÄúRaw‚Äù count values are generated by using the TPM value x featureLength x library size. These represent quantities that are on the same scale as original counts, except no longer correlated with transcript length across samples. We will use this option for DESeq2 downstream analysis.\n\nAn additional argument for tximport: When performing your own analysis you may find that the reference transcriptome file you obtain from Ensembl will have version numbers included on your identifiers (i.e ENSG00000265439.2). This will cause a discrepancy with the tx2gene file since the annotation databases don‚Äùt usually contain version numbers (i.e ENSG00000265439). To get around this issue you can use the argument ignoreTxVersion  = TRUE. The logical value indicates whether to split the tx id on the ‚Äú.‚Äù character to remove version information, for easier matching.\n\n\nCode\ntxi &lt;- tximport(files, type=\"salmon\", tx2gene=tx2gene, countsFromAbundance = \"lengthScaledTPM\", ignoreTxVersion = TRUE)\n\n\n\n\n\nThe txi object is a simple list containing matrices of the abundance, counts, length. Another list element ‚ÄúcountsFromAbundance‚Äù carries through the character argument used in the tximport call. The length matrix contains the average transcript length for each gene which can be used as an offset for gene-level analysis.\n\n\nCode\nattributes(txi)\n\n\nWe will be using the txi object as is for input into DESeq2, but will save it until the next lesson. For now let‚Äùs take a look at the count matrix. You will notice that there are decimal values, so let‚Äùs round to the nearest whole number and convert it into a dataframe. We will save it to a variable called data that we can play with.\n\n\nCode\n# Look at the counts\ntxi$counts %&gt;% head()\n\n\n\n\nCode\n# Write the counts to an object\ndata &lt;- txi$counts %&gt;% \n  round() %&gt;% \n  data.frame()\n\n\nThere are a lot of rows with no gene expression at all.\n\n\nCode\nsum(rowSums(data) == 0)\n\n\nLet‚Äôs take them out!\n\n\nCode\nkeep &lt;- rowSums(data) &gt; 0\ndata &lt;- data[keep,]"
  },
  {
    "objectID": "develop/rmd/05b_count_matrix.html#differential-gene-expression-analysis-overview",
    "href": "develop/rmd/05b_count_matrix.html#differential-gene-expression-analysis-overview",
    "title": "The RNAseq count matrix",
    "section": "",
    "text": "So, what does this count data actually represent? The count data used for differential expression analysis represents the number of sequence reads that originated from a particular gene. The higher the number of counts, the more reads associated with that gene, and the assumption that there was a higher level of expression of that gene in the sample.\n\n\n\n\n\n\n\n\n\nWith differential expression analysis, we are looking for genes that change in expression between two or more groups (defined in the metadata) - case vs control - correlation of expression with some variable or clinical outcome\nWhy does it not work to identify differentially expressed gene by ranking the genes by how different they are between the two groups (based on fold change values)?\n\n\n\n\n\n\n\n\n\nGenes that vary in expression level between groups of samples may do so solely as a consequence of the biological variable(s) of interest. However, this difference is often also related to extraneous effects, in fact, sometimes these effects exclusively account for the observed variation. The goal of differential expression analysis to determine the relative role of these effects, hence separating the ‚Äúinteresting‚Äù variance from the ‚Äúuninteresting‚Äù variance.\n\n\n\n\n\n\n\n\n\nAlthough the mean expression levels between sample groups may appear to be quite different, it is possible that the difference is not actually significant. This is illustrated for ‚ÄúGeneA‚Äù expression between ‚Äúuntreated‚Äù and ‚Äútreated‚Äù groups in the figure below. The mean expression level of geneA for the ‚Äútreated‚Äù group is twice as large as for the ‚Äúuntreated‚Äù group, but the variation between replicates indicates that this may not be a significant difference. We need to take into account the variation in the data (and where it might be coming from) when determining whether genes are differentially expressed.\n\n\n\n\n\n\n\n\n\nDifferential expression analysis is used to determine, for each gene, whether the differences in expression (counts) between groups is significant given the amount of variation observed within groups (replicates). To test for significance, we need an appropriate statistical model that accurately performs normalization (to account for differences in sequencing depth, etc.) and variance modeling (to account for few numbers of replicates and large dynamic expression range)."
  },
  {
    "objectID": "develop/rmd/05b_count_matrix.html#rna-seq-count-distribution",
    "href": "develop/rmd/05b_count_matrix.html#rna-seq-count-distribution",
    "title": "The RNAseq count matrix",
    "section": "",
    "text": "To determine the appropriate statistical model, we need information about the distribution of counts. To get an idea about how RNA-seq counts are distributed, let‚Äùs plot the counts of all the samples:\n\n\nCode\n# Here we format the data into long format instead of wide format\npdata &lt;- data %&gt;% \n  gather(key = Sample, value = Count)\n\npdata\n\n\nAnd we plot our count distribution using all our samples:\n\n\nCode\nggplot(pdata) +\n  geom_density(aes(x = Count, color = Sample)) +\n  xlab(\"Raw expression counts\") +\n  ylab(\"Number of genes\")\n\n\nIf we zoom in close to zero, we can see a large number of genes with counts close to zero:\n\n\nCode\nggplot(pdata) +\n  geom_density(aes(x = Count, color = Sample)) +\n  xlim(-5, 500)  +\n  xlab(\"Raw expression counts\") +\n  ylab(\"Number of genes\")\n\n\nThese images illustrate some common features of RNA-seq count data, including a low number of counts associated with a large proportion of genes, and a long right tail due to the lack of any upper limit for expression. Unlike microarray data, which has a dynamic range maximum limited due to when the probes max out, there is no limit of maximum expression for RNA-seq data. Due to the differences in these technologies, the statistical models used to fit the data are different between the two methods.\n??? note ‚ÄúNote on microarray data distribution‚Äù\nThe log intensities of the microarray data approximate a normal distribution. However, due to the different properties of the of RNA-seq count data, such as integer counts instead of continuous measurements and non-normally distributed data, the normal distribution does not accurately model RNA-seq counts. [More info here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3541212/)."
  },
  {
    "objectID": "develop/rmd/05b_count_matrix.html#modeling-count-data",
    "href": "develop/rmd/05b_count_matrix.html#modeling-count-data",
    "title": "The RNAseq count matrix",
    "section": "",
    "text": "RNAseq count data can be modeled using a Poisson distribution. this particular distribution is fitting for data where the number of cases is very large but the probability of an event occurring is very small. To give you an example, think of the lottery: many people buy lottery tickets (high number of cases), but only very few win (the probability of the event is small).\nWith RNA-Seq data, a very large number of RNAs are represented and the probability of pulling out a particular transcript is very small. Thus, it would be an appropriate situation to use the Poisson distribution. However, a unique property of this distribution is that the mean == variance. Realistically, with RNA-Seq data there is always some biological variation present across the replicates (within a sample class). Genes with larger average expression levels will tend to have larger observed variances across replicates.\nThe model that fits best, given this type of variability observed for replicates, is the Negative Binomial (NB) model. Essentially, the NB model is a good approximation for data where the mean &lt; variance, as is the case with RNA-Seq count data.\n\n\n\n\n\n\n\n\n\n!!! note ‚ÄúNote on technical replicates‚Äù\n-   **Biological replicates** represent multiple samples (i.e. RNA from different mice) representing the same sample class\n-   **Technical replicates** represent the same sample (i.e. RNA from the same mouse) but with technical steps replicated\n-   Usually biological variance is much greater than technical variance, so we do not need to account for technical variance to identify biological differences in expression\n-   **Don't spend money on technical replicates - biological replicates are much more useful**\n!!! note ‚ÄúNote on cell lines‚Äù\nIf you are using **cell lines** and are unsure whether or not you have prepared biological or technical replicates, take a look at [this link](https://web.archive.org/web/20170807192514/http://www.labstats.net:80/articles/cell_culture_n.html). This is a useful resource in helping you determine how best to set up your *in-vitro* experiment.\n??? note ‚ÄúHow do I know if my data should be modeled using the Poisson distribution or Negative Binomial distribution?‚Äù\nIf it's count data, it should fit the negative binomial, as discussed previously. However, it can be helpful to plot the *mean versus the variance* of your data. *Remember for the Poisson model, mean = variance, but for NB, mean &lt; variance.*\n\nHere we calculate the mean and the variance per gene for all columns and genes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf &lt;- data %&gt;% \nrowwise() %&gt;% \nsummarise(mean_counts = mean(c_across(everything())), \n                        variance_counts = var(c_across(everything())))\n```\n:::\n\n\nRun the following code to plot the *mean versus variance* of each gene for our data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_point(aes(x=mean_counts, y=variance_counts)) + \n  geom_abline(intercept = 0, slope = 1, color=\"red\") +\n  scale_y_log10() +\n  scale_x_log10()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/05b_count_matrix/deseq_mean_vs_variance.png){fig-align='center'}\n:::\n:::\n\n\nNote that in the above figure, the variance across replicates tends to be greater than the mean (red line), especially for genes with large mean expression levels. *This is a good indication that our data do not fit the Poisson distribution and we need to account for this increase in variance using the Negative Binomial model (i.e. Poisson will underestimate variability leading to an increase in false positive DE genes).*"
  },
  {
    "objectID": "develop/rmd/05b_count_matrix.html#improving-mean-estimates-i.e.-reducing-variance-with-biological-replicates",
    "href": "develop/rmd/05b_count_matrix.html#improving-mean-estimates-i.e.-reducing-variance-with-biological-replicates",
    "title": "The RNAseq count matrix",
    "section": "",
    "text": "The variance or scatter tends to reduce as we increase the number of biological replicates (the distribution will approach the Poisson distribution with increasing numbers of replicates), since standard deviations of averages are smaller than standard deviations of individual observations. The value of additional replicates is that as you add more data (replicates), you get increasingly precise estimates of group means, and ultimately greater confidence in the ability to distinguish differences between sample classes (i.e.¬†more DE genes).\nThe figure below illustrates the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified (from Liu et al.¬†(2013)):\n\n\n\n\n\n\n\n\n\nNote that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth. Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. Generally, the minimum sequencing depth recommended is 20-30 million reads per sample, but we have seen good RNA-seq experiments with 10 million reads if there are a good number of replicates."
  },
  {
    "objectID": "develop/rmd/05b_count_matrix.html#differential-expression-analysis-workflow",
    "href": "develop/rmd/05b_count_matrix.html#differential-expression-analysis-workflow",
    "title": "The RNAseq count matrix",
    "section": "",
    "text": "To model counts appropriately when performing a differential expression analysis, there are a number of software packages that have been developed for differential expression analysis of RNA-seq data. Even as new methods are continuously being developed a few tools are generally recommended as best practice, like DESeq2, EdgeR and Limma-Voom.\nMany studies describing comparisons between these methods show that while there is some agreement, there is also much variability between tools. Additionally, there is no one method that performs optimally under all conditions (Soneson and Dleorenzi, 2013, Corchete et al, 2020).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be using DESeq2 for the DE analysis, and the analysis steps with DESeq2 are shown in the flowchart below in green. DESeq2 first normalizes the count data to account for differences in library sizes and RNA composition between samples. Then, we will use the normalized counts to make some plots for QC at the gene and sample level. The final step is to use the appropriate functions from the DESeq2 package to perform the differential expression analysis.\n\n\n\n\n\n\n\n\n\nWe will go in-depth into each of these steps in the following lessons, but additional details and helpful suggestions regarding DESeq2 can be found in the DESeq2 vignette. As you go through this workflow and questions arise, you can reference the vignette from within RStudio:\nvignette(\"DESeq2\")\nThis is very convenient, as it provides a wealth of information at your fingertips! Be sure to use this as you need during the workshop.\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC)."
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html",
    "href": "develop/rmd/07b_hypothesis_testing.html",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 90 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1.  Demonstrate the use of the design formula with simple and complex designs\n2.  Construct R code to execute the differential expression analysis workflow with DESeq2\n3.  Describe the process of model fitting\n4.  Compare two methods for hypothesis testing (Wald test vs. LRT)\n5.  Discuss the steps required to generate a results table for pairwise comparisons (Wald test)\n6.  Recognize the importance of multiple test correction\n7.  Identify different methods for multiple test correction\n8.  Summarize the different levels of gene filtering\n9.  Evaluate the number of differentially expressed genes produced for each comparison\n10. Construct R objects containing significant genes from each comparison\nThe final step in the DESeq2 workflow is taking the counts for each gene and fitting it to the model and testing for differential expression.\n\n\n\n\n\n\n\n\n\n\n\nAs described earlier, the count data generated by RNA-seq exhibits overdispersion (variance &gt; mean) and the statistical distribution used to model the counts needs to account for this. As such, DESeq2 uses a negative binomial distribution to model the RNA-seq counts using the equation below:\n\n\n\n\n\n\n\n\n\nThe two parameters required are the size factor, and the dispersion estimate. Next, a generalized linear model (GLM) of the NB family is used to fit the data. Modeling is a mathematically formalized way to approximate how the data behaves given a set of parameters.\n??? quote ‚ÄúAbout GLMs‚Äù\n\"In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\" ([Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model)).\nAfter the model is fit, coefficients are estimated for each sample group along with their standard error. The coefficients are the estimates for the log2 fold changes, and will be used as input for hypothesis testing.\n\n\n\n\n\n\n\n\n\n\n\n\nThe first step in hypothesis testing is to set up a null hypothesis for each gene. In our case, the null hypothesis is that there is no differential expression across the two sample groups (LFC == 0). Notice that we can do this without observing any data, because it is based on a thought experiment. Second, we use a statistical test to determine if based on the observed data, the null hypothesis can be rejected.\n\n\nIn DESeq2, the Wald test is the default used for hypothesis testing when comparing two groups. The Wald test is a test usually performed on parameters that have been estimated by maximum likelihood. In our case we are testing each gene model coefficient (LFC) which was derived using parameters like dispersion, which were estimated using maximum likelihood.\nDESeq2 implements the Wald test by:\n\nTaking the LFC and dividing it by its standard error, resulting in a z-statistic.\nThe z-statistic is compared to a standard normal distribution, and a p-value is computed reporting the probability that a z-statistic at least as extreme as the observed value would be selected at random.\nIf the p-value is small we reject the null hypothesis and state that there is evidence against the null (i.e.¬†the gene is differentially expressed).\n\nThe model fit and Wald test were already run previously as part of the DESeq() function:\n\n\nCode\n## DO NOT RUN THIS CODE\n\n## Create DESeq2Dataset object\ndds &lt;- DESeqDataSetFromTximport(txi,\n                                   colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)\n## Run analysis\ndds &lt;- DESeq(dds)\n\n\n\n\n\nAn alternative to pair-wise comparisons is to analyze all levels of a factor at once. By default, the Wald test is used to generate the results table, but DESeq2 also offers the LRT which is used to identify any genes that show change in expression across the different levels. This type of test can be especially useful in analyzing time course experiments.\n!!! note ‚ÄúLRT vs Wald test‚Äù\nThe **Wald test** evaluates whether a gene's expression is up- or down-regulated in one group compared to another, while the LRT **identifies genes that are changing in expression in any combination across the different sample groups**. For example:\n\n-   Gene expression is equal between Control and Vampirium, but is less in Garlicum\n-   Gene expression of Vampirium is lower than Control and Control is lower than Garlicum\n-   Gene expression of Garlicum is equal to Control, but less than Vampirium.\n\nUsing LRT, we are evaluating the **null hypothesis that the full model fits just as well as the reduced model**. If we reject the null hypothesis, this suggests that there is a significant amount of variation explained by the full model (and our main factor of interest), therefore the gene is differentially expressed across the different levels. Generally, this test will result in a larger number of genes than the individual pair-wise comparisons. However, while the LRT is a test of significance for differences of any level of the factor, one should not expect it to be exactly equal to the union of sets of genes using Wald tests (although we do expect a majority overlap).\nTo use the LRT, we use the DESeq() function but this time adding two arguments:\n\nspecifying that we want to use the LRT test\nthe ‚Äúreduced‚Äù model\n\n\n\nCode\n# The full model was specified previously with the `design = ~ condition`:\n# dds &lt;- DESeqDataSetFromTximport(txi,\n                                   # colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              # design = ~ condition)\n\n# Likelihood ratio test\ndds_lrt &lt;- DESeq(dds, test=\"LRT\", reduced = ~ 1)\n\n\nSince our ‚Äúfull‚Äù model only has one factor (sampletype), the ‚Äúreduced‚Äù model (removing that factor) is just the intercept (~ 1). You will find that similar columns are reported for the LRT test. One thing to note is, even though there are fold changes present they are not directly associated with the actual hypothesis test.\n??? info ‚ÄúTime course analysis with LTR‚Äù\nThe LRT test can be especially helpful when performing time course analyses. We can use the LRT to explore whether there are any significant differences in treatment effect between any of the timepoints.\n\nFor have an experiment looking at the effect of treatment over time on mice of two different genotypes. We could use a design formula for our \"full model\" that would include the major sources of variation in our data: genotype, treatment, time, and our main condition of interest, which is the difference in the effect of treatment over time (treatment:time).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfull_model &lt;- ~ genotype + treatment + time + treatment:time\n```\n:::\n\n\nTo perform the LRT test, we can determine all genes that have significant differences in expression between treatments across time points by giving the \"reduced model\" without the `treatment:time` term:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreduced_model &lt;- ~ genotype + treatment + time\n```\n:::\n\n\nThen, we could run our test by using the following code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndds_lrt &lt;- DESeqDataSetFromMatrix(countData = data, colData = metadata, design = ~ genotype + treatment + time + treatment:time)\n\ndds_lrt_time &lt;- DESeq(dds_lrt, test=\"LRT\", reduced = ~ genotype + treatment + time)\n```\n:::\n\n\nThis analysis will not return genes where the treatment effect does not change over time, even though the genes may be differentially expressed between groups at a particular time point, as shown in the figure below:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/07b_hypothesis_testing/lrt_time_nodiff.png){fig-align='center'}\n:::\n:::\n\n\nThe significant DE genes will represent those genes that have differences in the effect of treatment over time, an example is displayed in the figure below:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/07b_hypothesis_testing/lrt_time_yesdiff.png){fig-align='center'}\n:::\n:::\n\n\n\n\nRegardless of whether we use the Wald test or the LRT, each gene that has been tested will be associated with a p-value. It is this result which we use to determine which genes are considered significantly differentially expressed. However, we cannot use the p-value directly.\n\n\nA gene with a significance cut-off of p &lt; 0.05, means there is a 5% chance it is a false positive. For example, if we test 20,000 genes for differential expression, at p &lt; 0.05 we would expect to find 1,000 DE genes by chance. If we found 3000 genes to be differentially expressed total, roughly one third of our genes are false positives! We would not want to sift through our ‚Äúsignificant‚Äù genes to identify which ones are true positives.\nSince each p-value is the result of a single test (single gene). The more genes we test, the more we inflate the false positive rate. This is the multiple testing problem.\n\n\n\nThere are a few common approaches for multiple test correction:\n\nBonferroni: The adjusted p-value is calculated by: \\(p-value*m\\) (\\(m\\) = total number of tests). This is a very conservative approach with a high probability of false negatives, so is generally not recommended.\nFDR/Benjamini-Hochberg: Benjamini and Hochberg (1995) defined the concept of False Discovery Rate (FDR) and created an algorithm to control the expected FDR below a specified level given a list of independent p-values. More info about BH.\nQ-value / Storey method: The minimum FDR that can be attained when calling that feature significant. For example, if gene X has a q-value of 0.013 it means that 1.3% of genes that show p-values at least as small as gene X are false positives.\n\nDESeq2 helps reduce the number of genes tested by removing those genes unlikely to be significantly DE prior to testing, such as those with low number of counts and outlier samples (see below). However, multiple test correction is also implemented to reduce the False Discovery Rate using an interpretation of the Benjamini-Hochberg procedure.\n!!! info ‚ÄúSo what does FDR &lt; 0.05 mean?‚Äù\nBy setting the FDR cutoff to &lt; 0.05, we're saying that the proportion of false positives we expect amongst our differentially expressed genes is 5%. For example, if you call 500 genes as differentially expressed with an FDR cutoff of 0.05, you expect 25 of them to be false positives.\n\n\n\n\nBy default DESeq2 uses the Wald test to identify genes that are differentially expressed between two sample groups. Given the factor(s) used in the design formula, and how many factor levels are present, we can extract results for a number of different comparisons. Here, we will walk you through how to obtain results from the dds object and provide some explanations on how to interpret them.\n!!! note ‚ÄúWald test on continuous variables‚Äù\nThe Wald test can also be used with **continuous variables**. If the variable of interest provided in the design formula is continuous-valued, then the reported `log2FoldChange` is per unit of change of that variable.\n\n\nIn our dataset, we have three sample groups so we can make three possible pairwise comparisons:\n\nControl vs.¬†Vampirium\nGarlicum vs.¬†Vampirium\nGarlicum vs.¬†Control\n\nWe are really only interested in 1 and 2 from above. When we initially created our dds object we had provided ~ condition as our design formula, indicating that sampletype is our main factor of interest.\nTo indicate which two sample groups we are interested in comparing, we need to specify contrasts. The contrasts are used as input to the DESeq2 results() function to extract the desired results.\nContrasts can be specified in three different ways:\n1.Contrasts can be supplied as a character vector with exactly three elements: the name of the factor (of interest) in the design formula, the name of the two factors levels to compare. The factor level given last is the base level for the comparison. The syntax is given below:\n\n\nCode\n# DO NOT RUN!\ncontrast &lt;- c(\"condition\", \"level_to_compare\", \"base_level\")\nresults(dds, contrast = contrast)\n\n\n2.Contrasts can be given as a list of 2 character vectors: the names of the fold changes for the level of interest, and the names of the fold changes for the base level. These names should match identically to the elements of resultsNames(object). This method can be useful for combining interaction terms and main effects.\n\n\nCode\n# DO NOT RUN!\nresultsNames(dds) # to see what names to use\ncontrast &lt;- list(resultsNames(dds)[1], resultsNames(dds)[2])\nresults(dds, contrast = contrast)\n\n\n3.One of the results from resultsNames(dds) and the name argument. This one is the simplest but it can also be very restrictive:\n\n\nCode\n# DO NOT RUN!\nresultsNames(dds) # to see what names to use\nresults(dds, name = resultsNames(dds)[2])\n\n\nAlternatively, if you only had two factor levels you could do nothing and not worry about specifying contrasts (i.e.¬†results(dds)). In this case, DESeq2 will choose what your base factor level based on alphabetical order of the levels.\nTo start, we want to evaluate expression changes between the control samples and the vampirium samples. As such we will use the first method for specifying contrasts and create a character vector:\n!!! question ‚ÄúExercise 1‚Äù\nDefine contrasts for Vampirium samples using one of the two methods above.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## your code here\n#contrast_cont &lt;- \n```\n:::\n??? question ‚ÄúSolution to Exercise 1‚Äù\nFirst let's check the metadata:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeta\n```\n:::\n\n\nSince we have only given the `condition` column in the design formula, the first element should be `condition`. The second element is the condition we are interested, `control` and our base level is `vampirium`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncontrast_cont &lt;- c(\"condition\", \"control\", \"vampirium\")\n```\n:::\n!!! warning ‚ÄúDoes it matter what I choose to be my base level?‚Äù\nYes, it does matter. **Deciding what level is the base level will determine how to interpret the fold change that is reported.** So for example, if we observe a log2 fold change of -2 this would mean the gene expression is lower in factor level of interest relative to the base level. Thus, if leaving it up to DESeq2 to decide on the contrasts be sure to check that the alphabetical order coincides with the fold change direction you are anticipating.\n\n\n\n\nNow that we have our contrast created, we can use it as input to the results() function. Let‚Äôs take a quick look at the help manual for the function:\n\n\nCode\n?results\n\n\nYou will see we have the option to provide a wide array of arguments and tweak things from the defaults as needed. As we go through the lesson we will keep coming back to the help documentation to discuss some arguments that are good to know about.\n\n\nCode\n## Extract results for Control vs Vampirium samples\nres_tableCont &lt;- results(dds, contrast=contrast_cont, alpha = 0.05)\n\n\n!!! warning\nFor our analysis, in addition to the `contrast` argument we will also provide a value of 0.05 for the `alpha` argument. We will describe this in more detail when we talk about gene-level filtering.\nThe results table that is returned to us is a DESeqResults object, which is a simple subclass of DataFrame. In many ways it can be treated like a dataframe (i.e when accessing/subsetting data), however it is important to recognize that there are differences for downstream steps like visualization.\n\n\nCode\n# Check what type of object is returned\nclass(res_tableCont)\n\n\nNow let‚Äôs take a look at what information is stored in the results:\n\n\nCode\n# What is stored in results?\nres_tableCont %&gt;% \n  data.frame() %&gt;% \n  head()\n\n\nWe have six columns of information reported for each gene (row). We can use the mcols() function to extract information on what the values stored in each column represent:\n\n\nCode\n# Get information on each column in results\nmcols(res_tableCont, use.names=T)\n\n\n\nbaseMean: mean of normalized counts for all samples\nlog2FoldChange: log2 fold change\nlfcSE: standard error\nstat: Wald statistic\npvalue: Wald test p-value\npadj: BH adjusted p-values\n\n\n\n\nThe p-value is a probability value used to determine whether there is evidence to reject the null hypothesis. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.\nThe padj column in the results table represents the p-value adjusted for multiple testing, and is the most important column of the results. Typically, a threshold such as padj &lt; 0.05 is a good starting point for identifying significant genes. The default method for multiple test correction in DESeq2 is the FDR. Other methods can be used with the pAdjustMethod argument in the results() function.\n\n\nLet‚Äôs take a closer look at our results table. As we scroll through it, you will notice that for selected genes there are NA values in the pvalue and padj columns. What does this mean?\n\n\n\n\n\n\n\n\n\nThe missing values represent genes that have undergone filtering as part of the DESeq() function. Prior to differential expression analysis it is beneficial to omit genes that have little or no chance of being detected as differentially expressed. This will increase the power to detect differentially expressed genes. DESeq2 does not physically remove any genes from the original counts matrix, and so all genes will be present in your results table. The genes omitted by DESeq2 meet one of the three filtering criteria outlined below:\n1. Genes with zero counts in all samples\nIf within a row, all samples have zero counts there is no expression information and therefore these genes are not tested. In our case, there are no genes that fulfill this criteria, since we have already filtered out these genes ourselves when we created our dds object.\n\n\nCode\n# Show genes with zero expression\nres_tableCont %&gt;%\n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(baseMean==0) %&gt;%\n  head()\n\n\n!!! info\nIf there would be any genes meeting this criteria, the baseMean column for these genes will be zero, and the log2 fold change estimates, p-value and adjusted p-value will all be set to NA.\n2. Genes with an extreme count outlier\nThe DESeq() function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook‚Äôs distance. Cook‚Äôs distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook‚Äôs distance is intended to indicate an outlier count. Genes which contain a Cook‚Äôs distance above a threshold are flagged, however at least 3 replicates are required for flagging, as it is difficult to judge which sample might be an outlier with only 2 replicates. We can turn off this filtering by using the cooksCutoff argument in the results() function.\n\n\nCode\n# Show genes that have an extreme outlier\nres_tableCont %&gt;% \n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(is.na(pvalue) & is.na(padj) & baseMean &gt; 0) %&gt;%\n  head()\n\n\nIt seems that we have some genes with outliers!\n!!! info\nIf a gene contains a sample with an extreme count outlier then the p-value and adjusted p-value will be set to NA.\n3. Genes with a low mean normalized counts\nDESeq2 defines a low mean threshold, that is empirically determined from your data, in which the fraction of significant genes can be increased by reducing the number of genes that are considered for multiple testing. This is based on the notion that genes with very low counts are not likely to see significant differences typically due to high dispersion.\n\n\n\n\n\n\n\n\n\nImage courtesy of slideshare presentation from Joachim Jacob, 2014.\nAt a user-specified value (alpha = 0.05), DESeq2 evaluates the change in the number of significant genes as it filters out increasingly bigger portions of genes based on their mean counts, as shown in the figure above. The point at which the number of significant genes reaches a peak is the low mean threshold that is used to filter genes that undergo multiple testing. There is also an argument to turn off the filtering off by setting independentFiltering = F.\n\n\nCode\n# Filter genes below the low mean threshold\nres_tableCont %&gt;% \n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(!is.na(pvalue) & is.na(padj) & baseMean &gt; 0) %&gt;%\n  head()\n\n\n!!! info\nIf a gene is filtered by independent filtering, then only the adjusted p-value will be set to NA.\n!!! warning\nDESeq2 will perform the filtering outlined above by default; however other DE tools, such as EdgeR will not. Filtering is a necessary step, even if you are using limma-voom and/or edgeR's quasi-likelihood methods. Be sure to follow pre-filtering steps when using other tools, as outlined in their user guides found on Bioconductor as they generally perform much better.*\n\n\n\n\nAnother important column in the results table, is the log2FoldChange. With large significant gene lists it can be hard to extract meaningful biological relevance. To help increase stringency, one can also add a fold change threshold. Keep in mind when setting that value that we are working with log2 fold changes, so a cutoff of log2FoldChange &lt; 1 would translate to an actual fold change of 2.\n??? info ‚ÄúAn alternative approach to add the fold change threshold‚Äù\nThe `results()` function has an option to add a fold change threshold using the `lfcThreshold` argument. This method is more statistically motivated, and is recommended when you want a more confident set of genes based on a certain fold-change. It actually performs a statistical test against the desired threshold, by performing a two-tailed test for log2 fold changes greater than the absolute value specified. The user can change the alternative hypothesis using `altHypothesis` and perform two one-tailed tests as well. **This is a more conservative approach, so expect to retrieve a much smaller set of genes!**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_tableCont_LFC1 &lt;- results(dds, contrast=contrast_cont, alpha = 0.05, lfcThreshold = 1)\n```\n:::\nThe fold changes reported in the results table are the coefficients calculated in the GLM mentioned in the previous section.\n\n\n\nTo summarize the results table, a handy function in DESeq2 is summary(). Confusingly it has the same name as the function used to inspect data frames. This function, when called with a DESeq results table as input, will summarize the results using a default threshold of padj &lt; 0.1. However, since we had set the alpha argument to 0.05 when creating our results table threshold: FDR &lt; 0.05 (padj/FDR is used even though the output says p-value &lt; 0.05). Let‚Äôs start with the OE vs control results:\n\n\nCode\n## Summarize results\nsummary(res_tableCont, alpha = 0.05)\n\n\nIn addition to the number of genes up- and down-regulated at the default threshold, the function also reports the number of genes that were tested (genes with non-zero total read count), and the number of genes not included in multiple test correction due to a low mean count.\n\n\n\nLet‚Äôs first create variables that contain our threshold criteria. We will only be using the adjusted p-values in our criteria:\n\n\nCode\n### Set thresholds\npadj.cutoff &lt;- 0.05\n\n\nWe can easily subset the results table to only include those that are significant using the dplyr::filter() function, but first we will convert the results table into a tibble:\n\n\nCode\n# Create a tibble of results and add gene IDs to new object\nres_tableCont_tb &lt;- res_tableCont %&gt;%\n  as_tibble(rownames = \"gene\") %&gt;%\n  relocate(gene, .before = baseMean)\n\nhead(res_tableCont_tb)\n\n\nNow we can subset that table to only keep the significant genes using our pre-defined thresholds:\n\n\nCode\n# Subset the tibble to keep only significant genes\nsigCont &lt;- res_tableCont_tb %&gt;%\n  dplyr::filter(padj &lt; padj.cutoff)\n\n\n\n\nCode\n# Take a quick look at this tibble\nsigCont\n\n\nNow that we have extracted the significant results, we are ready for visualization!\n!!! question ‚ÄúExercise 2‚Äù\n**Vampirium Differential Expression Analysis: Garlicum versus Vampirium**\n\nNow that we have results for the Control vs Vampirium results, do the same for the **Garlicum vs Vampirium samples**.\n\n1.  Create a contrast vector called `contrast_gar`.\n2.  Use contrast vector in the `results()` to extract a results table and store that to a variable called `res_tableGar`.\n3.  Using a p-adjusted threshold of 0.05 (`padj.cutoff &lt; 0.05`), subset `res_tableGar` to report the number of genes that are up- and down-regulated in garlicum compared to vampirium.\n4.  How many genes are differentially expressed in the Garlicum compared to Vampirium? How does this compare to the Control vs Vampirium significant gene list (in terms of numbers)?\n??? question ‚ÄúSolutions to Exercise 2‚Äù\n1. Contrast for Garlicum vs Vampirium\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncontrast_gar &lt;- c(\"condition\", \"garlicum\", \"vampirium\")\n```\n:::\n\n\n2. Extract results\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_tableGar &lt;- results(dds, contrast = contrast_gar)\n```\n:::\n\n\n3. Significant genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npadj.cutoff &lt;- 0.05\nsigGar &lt;- res_tableGar %&gt;% as_tibble(rownames = \"gene\") %&gt;%\n  relocate(gene, .before = baseMean) %&gt;% dplyr::filter(padj &lt; padj.cutoff)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigGar\n```\n:::\n\n\n4. Comparison against sigGar vs sigCont\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(sigGar) #number of significant genes in Garlicum vs Vampirium\nnrow(sigCont) #number of significant genes in Control vs Vampirium\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(sigCont) - nrow(sigGar)\n```\n:::\n\n\nsigCont has almost 2000 more genes that are differentially expressed!\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nSome materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website"
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#generalized-linear-model",
    "href": "develop/rmd/07b_hypothesis_testing.html#generalized-linear-model",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "As described earlier, the count data generated by RNA-seq exhibits overdispersion (variance &gt; mean) and the statistical distribution used to model the counts needs to account for this. As such, DESeq2 uses a negative binomial distribution to model the RNA-seq counts using the equation below:\n\n\n\n\n\n\n\n\n\nThe two parameters required are the size factor, and the dispersion estimate. Next, a generalized linear model (GLM) of the NB family is used to fit the data. Modeling is a mathematically formalized way to approximate how the data behaves given a set of parameters.\n??? quote ‚ÄúAbout GLMs‚Äù\n\"In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\" ([Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model)).\nAfter the model is fit, coefficients are estimated for each sample group along with their standard error. The coefficients are the estimates for the log2 fold changes, and will be used as input for hypothesis testing."
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#hypothesis-testing",
    "href": "develop/rmd/07b_hypothesis_testing.html#hypothesis-testing",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "The first step in hypothesis testing is to set up a null hypothesis for each gene. In our case, the null hypothesis is that there is no differential expression across the two sample groups (LFC == 0). Notice that we can do this without observing any data, because it is based on a thought experiment. Second, we use a statistical test to determine if based on the observed data, the null hypothesis can be rejected.\n\n\nIn DESeq2, the Wald test is the default used for hypothesis testing when comparing two groups. The Wald test is a test usually performed on parameters that have been estimated by maximum likelihood. In our case we are testing each gene model coefficient (LFC) which was derived using parameters like dispersion, which were estimated using maximum likelihood.\nDESeq2 implements the Wald test by:\n\nTaking the LFC and dividing it by its standard error, resulting in a z-statistic.\nThe z-statistic is compared to a standard normal distribution, and a p-value is computed reporting the probability that a z-statistic at least as extreme as the observed value would be selected at random.\nIf the p-value is small we reject the null hypothesis and state that there is evidence against the null (i.e.¬†the gene is differentially expressed).\n\nThe model fit and Wald test were already run previously as part of the DESeq() function:\n\n\nCode\n## DO NOT RUN THIS CODE\n\n## Create DESeq2Dataset object\ndds &lt;- DESeqDataSetFromTximport(txi,\n                                   colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)\n## Run analysis\ndds &lt;- DESeq(dds)\n\n\n\n\n\nAn alternative to pair-wise comparisons is to analyze all levels of a factor at once. By default, the Wald test is used to generate the results table, but DESeq2 also offers the LRT which is used to identify any genes that show change in expression across the different levels. This type of test can be especially useful in analyzing time course experiments.\n!!! note ‚ÄúLRT vs Wald test‚Äù\nThe **Wald test** evaluates whether a gene's expression is up- or down-regulated in one group compared to another, while the LRT **identifies genes that are changing in expression in any combination across the different sample groups**. For example:\n\n-   Gene expression is equal between Control and Vampirium, but is less in Garlicum\n-   Gene expression of Vampirium is lower than Control and Control is lower than Garlicum\n-   Gene expression of Garlicum is equal to Control, but less than Vampirium.\n\nUsing LRT, we are evaluating the **null hypothesis that the full model fits just as well as the reduced model**. If we reject the null hypothesis, this suggests that there is a significant amount of variation explained by the full model (and our main factor of interest), therefore the gene is differentially expressed across the different levels. Generally, this test will result in a larger number of genes than the individual pair-wise comparisons. However, while the LRT is a test of significance for differences of any level of the factor, one should not expect it to be exactly equal to the union of sets of genes using Wald tests (although we do expect a majority overlap).\nTo use the LRT, we use the DESeq() function but this time adding two arguments:\n\nspecifying that we want to use the LRT test\nthe ‚Äúreduced‚Äù model\n\n\n\nCode\n# The full model was specified previously with the `design = ~ condition`:\n# dds &lt;- DESeqDataSetFromTximport(txi,\n                                   # colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              # design = ~ condition)\n\n# Likelihood ratio test\ndds_lrt &lt;- DESeq(dds, test=\"LRT\", reduced = ~ 1)\n\n\nSince our ‚Äúfull‚Äù model only has one factor (sampletype), the ‚Äúreduced‚Äù model (removing that factor) is just the intercept (~ 1). You will find that similar columns are reported for the LRT test. One thing to note is, even though there are fold changes present they are not directly associated with the actual hypothesis test.\n??? info ‚ÄúTime course analysis with LTR‚Äù\nThe LRT test can be especially helpful when performing time course analyses. We can use the LRT to explore whether there are any significant differences in treatment effect between any of the timepoints.\n\nFor have an experiment looking at the effect of treatment over time on mice of two different genotypes. We could use a design formula for our \"full model\" that would include the major sources of variation in our data: genotype, treatment, time, and our main condition of interest, which is the difference in the effect of treatment over time (treatment:time).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfull_model &lt;- ~ genotype + treatment + time + treatment:time\n```\n:::\n\n\nTo perform the LRT test, we can determine all genes that have significant differences in expression between treatments across time points by giving the \"reduced model\" without the `treatment:time` term:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreduced_model &lt;- ~ genotype + treatment + time\n```\n:::\n\n\nThen, we could run our test by using the following code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndds_lrt &lt;- DESeqDataSetFromMatrix(countData = data, colData = metadata, design = ~ genotype + treatment + time + treatment:time)\n\ndds_lrt_time &lt;- DESeq(dds_lrt, test=\"LRT\", reduced = ~ genotype + treatment + time)\n```\n:::\n\n\nThis analysis will not return genes where the treatment effect does not change over time, even though the genes may be differentially expressed between groups at a particular time point, as shown in the figure below:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/07b_hypothesis_testing/lrt_time_nodiff.png){fig-align='center'}\n:::\n:::\n\n\nThe significant DE genes will represent those genes that have differences in the effect of treatment over time, an example is displayed in the figure below:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./img/07b_hypothesis_testing/lrt_time_yesdiff.png){fig-align='center'}\n:::\n:::"
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#multiple-test-correction",
    "href": "develop/rmd/07b_hypothesis_testing.html#multiple-test-correction",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "Regardless of whether we use the Wald test or the LRT, each gene that has been tested will be associated with a p-value. It is this result which we use to determine which genes are considered significantly differentially expressed. However, we cannot use the p-value directly.\n\n\nA gene with a significance cut-off of p &lt; 0.05, means there is a 5% chance it is a false positive. For example, if we test 20,000 genes for differential expression, at p &lt; 0.05 we would expect to find 1,000 DE genes by chance. If we found 3000 genes to be differentially expressed total, roughly one third of our genes are false positives! We would not want to sift through our ‚Äúsignificant‚Äù genes to identify which ones are true positives.\nSince each p-value is the result of a single test (single gene). The more genes we test, the more we inflate the false positive rate. This is the multiple testing problem.\n\n\n\nThere are a few common approaches for multiple test correction:\n\nBonferroni: The adjusted p-value is calculated by: \\(p-value*m\\) (\\(m\\) = total number of tests). This is a very conservative approach with a high probability of false negatives, so is generally not recommended.\nFDR/Benjamini-Hochberg: Benjamini and Hochberg (1995) defined the concept of False Discovery Rate (FDR) and created an algorithm to control the expected FDR below a specified level given a list of independent p-values. More info about BH.\nQ-value / Storey method: The minimum FDR that can be attained when calling that feature significant. For example, if gene X has a q-value of 0.013 it means that 1.3% of genes that show p-values at least as small as gene X are false positives.\n\nDESeq2 helps reduce the number of genes tested by removing those genes unlikely to be significantly DE prior to testing, such as those with low number of counts and outlier samples (see below). However, multiple test correction is also implemented to reduce the False Discovery Rate using an interpretation of the Benjamini-Hochberg procedure.\n!!! info ‚ÄúSo what does FDR &lt; 0.05 mean?‚Äù\nBy setting the FDR cutoff to &lt; 0.05, we're saying that the proportion of false positives we expect amongst our differentially expressed genes is 5%. For example, if you call 500 genes as differentially expressed with an FDR cutoff of 0.05, you expect 25 of them to be false positives."
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#exploring-results-wald-test",
    "href": "develop/rmd/07b_hypothesis_testing.html#exploring-results-wald-test",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "By default DESeq2 uses the Wald test to identify genes that are differentially expressed between two sample groups. Given the factor(s) used in the design formula, and how many factor levels are present, we can extract results for a number of different comparisons. Here, we will walk you through how to obtain results from the dds object and provide some explanations on how to interpret them.\n!!! note ‚ÄúWald test on continuous variables‚Äù\nThe Wald test can also be used with **continuous variables**. If the variable of interest provided in the design formula is continuous-valued, then the reported `log2FoldChange` is per unit of change of that variable.\n\n\nIn our dataset, we have three sample groups so we can make three possible pairwise comparisons:\n\nControl vs.¬†Vampirium\nGarlicum vs.¬†Vampirium\nGarlicum vs.¬†Control\n\nWe are really only interested in 1 and 2 from above. When we initially created our dds object we had provided ~ condition as our design formula, indicating that sampletype is our main factor of interest.\nTo indicate which two sample groups we are interested in comparing, we need to specify contrasts. The contrasts are used as input to the DESeq2 results() function to extract the desired results.\nContrasts can be specified in three different ways:\n1.Contrasts can be supplied as a character vector with exactly three elements: the name of the factor (of interest) in the design formula, the name of the two factors levels to compare. The factor level given last is the base level for the comparison. The syntax is given below:\n\n\nCode\n# DO NOT RUN!\ncontrast &lt;- c(\"condition\", \"level_to_compare\", \"base_level\")\nresults(dds, contrast = contrast)\n\n\n2.Contrasts can be given as a list of 2 character vectors: the names of the fold changes for the level of interest, and the names of the fold changes for the base level. These names should match identically to the elements of resultsNames(object). This method can be useful for combining interaction terms and main effects.\n\n\nCode\n# DO NOT RUN!\nresultsNames(dds) # to see what names to use\ncontrast &lt;- list(resultsNames(dds)[1], resultsNames(dds)[2])\nresults(dds, contrast = contrast)\n\n\n3.One of the results from resultsNames(dds) and the name argument. This one is the simplest but it can also be very restrictive:\n\n\nCode\n# DO NOT RUN!\nresultsNames(dds) # to see what names to use\nresults(dds, name = resultsNames(dds)[2])\n\n\nAlternatively, if you only had two factor levels you could do nothing and not worry about specifying contrasts (i.e.¬†results(dds)). In this case, DESeq2 will choose what your base factor level based on alphabetical order of the levels.\nTo start, we want to evaluate expression changes between the control samples and the vampirium samples. As such we will use the first method for specifying contrasts and create a character vector:\n!!! question ‚ÄúExercise 1‚Äù\nDefine contrasts for Vampirium samples using one of the two methods above.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## your code here\n#contrast_cont &lt;- \n```\n:::\n??? question ‚ÄúSolution to Exercise 1‚Äù\nFirst let's check the metadata:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeta\n```\n:::\n\n\nSince we have only given the `condition` column in the design formula, the first element should be `condition`. The second element is the condition we are interested, `control` and our base level is `vampirium`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncontrast_cont &lt;- c(\"condition\", \"control\", \"vampirium\")\n```\n:::\n!!! warning ‚ÄúDoes it matter what I choose to be my base level?‚Äù\nYes, it does matter. **Deciding what level is the base level will determine how to interpret the fold change that is reported.** So for example, if we observe a log2 fold change of -2 this would mean the gene expression is lower in factor level of interest relative to the base level. Thus, if leaving it up to DESeq2 to decide on the contrasts be sure to check that the alphabetical order coincides with the fold change direction you are anticipating."
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#the-results-table",
    "href": "develop/rmd/07b_hypothesis_testing.html#the-results-table",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "Now that we have our contrast created, we can use it as input to the results() function. Let‚Äôs take a quick look at the help manual for the function:\n\n\nCode\n?results\n\n\nYou will see we have the option to provide a wide array of arguments and tweak things from the defaults as needed. As we go through the lesson we will keep coming back to the help documentation to discuss some arguments that are good to know about.\n\n\nCode\n## Extract results for Control vs Vampirium samples\nres_tableCont &lt;- results(dds, contrast=contrast_cont, alpha = 0.05)\n\n\n!!! warning\nFor our analysis, in addition to the `contrast` argument we will also provide a value of 0.05 for the `alpha` argument. We will describe this in more detail when we talk about gene-level filtering.\nThe results table that is returned to us is a DESeqResults object, which is a simple subclass of DataFrame. In many ways it can be treated like a dataframe (i.e when accessing/subsetting data), however it is important to recognize that there are differences for downstream steps like visualization.\n\n\nCode\n# Check what type of object is returned\nclass(res_tableCont)\n\n\nNow let‚Äôs take a look at what information is stored in the results:\n\n\nCode\n# What is stored in results?\nres_tableCont %&gt;% \n  data.frame() %&gt;% \n  head()\n\n\nWe have six columns of information reported for each gene (row). We can use the mcols() function to extract information on what the values stored in each column represent:\n\n\nCode\n# Get information on each column in results\nmcols(res_tableCont, use.names=T)\n\n\n\nbaseMean: mean of normalized counts for all samples\nlog2FoldChange: log2 fold change\nlfcSE: standard error\nstat: Wald statistic\npvalue: Wald test p-value\npadj: BH adjusted p-values"
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#p-values",
    "href": "develop/rmd/07b_hypothesis_testing.html#p-values",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "The p-value is a probability value used to determine whether there is evidence to reject the null hypothesis. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.\nThe padj column in the results table represents the p-value adjusted for multiple testing, and is the most important column of the results. Typically, a threshold such as padj &lt; 0.05 is a good starting point for identifying significant genes. The default method for multiple test correction in DESeq2 is the FDR. Other methods can be used with the pAdjustMethod argument in the results() function.\n\n\nLet‚Äôs take a closer look at our results table. As we scroll through it, you will notice that for selected genes there are NA values in the pvalue and padj columns. What does this mean?\n\n\n\n\n\n\n\n\n\nThe missing values represent genes that have undergone filtering as part of the DESeq() function. Prior to differential expression analysis it is beneficial to omit genes that have little or no chance of being detected as differentially expressed. This will increase the power to detect differentially expressed genes. DESeq2 does not physically remove any genes from the original counts matrix, and so all genes will be present in your results table. The genes omitted by DESeq2 meet one of the three filtering criteria outlined below:\n1. Genes with zero counts in all samples\nIf within a row, all samples have zero counts there is no expression information and therefore these genes are not tested. In our case, there are no genes that fulfill this criteria, since we have already filtered out these genes ourselves when we created our dds object.\n\n\nCode\n# Show genes with zero expression\nres_tableCont %&gt;%\n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(baseMean==0) %&gt;%\n  head()\n\n\n!!! info\nIf there would be any genes meeting this criteria, the baseMean column for these genes will be zero, and the log2 fold change estimates, p-value and adjusted p-value will all be set to NA.\n2. Genes with an extreme count outlier\nThe DESeq() function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook‚Äôs distance. Cook‚Äôs distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook‚Äôs distance is intended to indicate an outlier count. Genes which contain a Cook‚Äôs distance above a threshold are flagged, however at least 3 replicates are required for flagging, as it is difficult to judge which sample might be an outlier with only 2 replicates. We can turn off this filtering by using the cooksCutoff argument in the results() function.\n\n\nCode\n# Show genes that have an extreme outlier\nres_tableCont %&gt;% \n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(is.na(pvalue) & is.na(padj) & baseMean &gt; 0) %&gt;%\n  head()\n\n\nIt seems that we have some genes with outliers!\n!!! info\nIf a gene contains a sample with an extreme count outlier then the p-value and adjusted p-value will be set to NA.\n3. Genes with a low mean normalized counts\nDESeq2 defines a low mean threshold, that is empirically determined from your data, in which the fraction of significant genes can be increased by reducing the number of genes that are considered for multiple testing. This is based on the notion that genes with very low counts are not likely to see significant differences typically due to high dispersion.\n\n\n\n\n\n\n\n\n\nImage courtesy of slideshare presentation from Joachim Jacob, 2014.\nAt a user-specified value (alpha = 0.05), DESeq2 evaluates the change in the number of significant genes as it filters out increasingly bigger portions of genes based on their mean counts, as shown in the figure above. The point at which the number of significant genes reaches a peak is the low mean threshold that is used to filter genes that undergo multiple testing. There is also an argument to turn off the filtering off by setting independentFiltering = F.\n\n\nCode\n# Filter genes below the low mean threshold\nres_tableCont %&gt;% \n  as_tibble(rownames = \"gene\") %&gt;% \n  dplyr::filter(!is.na(pvalue) & is.na(padj) & baseMean &gt; 0) %&gt;%\n  head()\n\n\n!!! info\nIf a gene is filtered by independent filtering, then only the adjusted p-value will be set to NA.\n!!! warning\nDESeq2 will perform the filtering outlined above by default; however other DE tools, such as EdgeR will not. Filtering is a necessary step, even if you are using limma-voom and/or edgeR's quasi-likelihood methods. Be sure to follow pre-filtering steps when using other tools, as outlined in their user guides found on Bioconductor as they generally perform much better.*"
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#fold-changes",
    "href": "develop/rmd/07b_hypothesis_testing.html#fold-changes",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "Another important column in the results table, is the log2FoldChange. With large significant gene lists it can be hard to extract meaningful biological relevance. To help increase stringency, one can also add a fold change threshold. Keep in mind when setting that value that we are working with log2 fold changes, so a cutoff of log2FoldChange &lt; 1 would translate to an actual fold change of 2.\n??? info ‚ÄúAn alternative approach to add the fold change threshold‚Äù\nThe `results()` function has an option to add a fold change threshold using the `lfcThreshold` argument. This method is more statistically motivated, and is recommended when you want a more confident set of genes based on a certain fold-change. It actually performs a statistical test against the desired threshold, by performing a two-tailed test for log2 fold changes greater than the absolute value specified. The user can change the alternative hypothesis using `altHypothesis` and perform two one-tailed tests as well. **This is a more conservative approach, so expect to retrieve a much smaller set of genes!**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_tableCont_LFC1 &lt;- results(dds, contrast=contrast_cont, alpha = 0.05, lfcThreshold = 1)\n```\n:::\nThe fold changes reported in the results table are the coefficients calculated in the GLM mentioned in the previous section."
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#summarizing-results",
    "href": "develop/rmd/07b_hypothesis_testing.html#summarizing-results",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "To summarize the results table, a handy function in DESeq2 is summary(). Confusingly it has the same name as the function used to inspect data frames. This function, when called with a DESeq results table as input, will summarize the results using a default threshold of padj &lt; 0.1. However, since we had set the alpha argument to 0.05 when creating our results table threshold: FDR &lt; 0.05 (padj/FDR is used even though the output says p-value &lt; 0.05). Let‚Äôs start with the OE vs control results:\n\n\nCode\n## Summarize results\nsummary(res_tableCont, alpha = 0.05)\n\n\nIn addition to the number of genes up- and down-regulated at the default threshold, the function also reports the number of genes that were tested (genes with non-zero total read count), and the number of genes not included in multiple test correction due to a low mean count."
  },
  {
    "objectID": "develop/rmd/07b_hypothesis_testing.html#extracting-significant-differentially-expressed-genes",
    "href": "develop/rmd/07b_hypothesis_testing.html#extracting-significant-differentially-expressed-genes",
    "title": "Hypothesis testing with DESeq2",
    "section": "",
    "text": "Let‚Äôs first create variables that contain our threshold criteria. We will only be using the adjusted p-values in our criteria:\n\n\nCode\n### Set thresholds\npadj.cutoff &lt;- 0.05\n\n\nWe can easily subset the results table to only include those that are significant using the dplyr::filter() function, but first we will convert the results table into a tibble:\n\n\nCode\n# Create a tibble of results and add gene IDs to new object\nres_tableCont_tb &lt;- res_tableCont %&gt;%\n  as_tibble(rownames = \"gene\") %&gt;%\n  relocate(gene, .before = baseMean)\n\nhead(res_tableCont_tb)\n\n\nNow we can subset that table to only keep the significant genes using our pre-defined thresholds:\n\n\nCode\n# Subset the tibble to keep only significant genes\nsigCont &lt;- res_tableCont_tb %&gt;%\n  dplyr::filter(padj &lt; padj.cutoff)\n\n\n\n\nCode\n# Take a quick look at this tibble\nsigCont\n\n\nNow that we have extracted the significant results, we are ready for visualization!\n!!! question ‚ÄúExercise 2‚Äù\n**Vampirium Differential Expression Analysis: Garlicum versus Vampirium**\n\nNow that we have results for the Control vs Vampirium results, do the same for the **Garlicum vs Vampirium samples**.\n\n1.  Create a contrast vector called `contrast_gar`.\n2.  Use contrast vector in the `results()` to extract a results table and store that to a variable called `res_tableGar`.\n3.  Using a p-adjusted threshold of 0.05 (`padj.cutoff &lt; 0.05`), subset `res_tableGar` to report the number of genes that are up- and down-regulated in garlicum compared to vampirium.\n4.  How many genes are differentially expressed in the Garlicum compared to Vampirium? How does this compare to the Control vs Vampirium significant gene list (in terms of numbers)?\n??? question ‚ÄúSolutions to Exercise 2‚Äù\n1. Contrast for Garlicum vs Vampirium\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncontrast_gar &lt;- c(\"condition\", \"garlicum\", \"vampirium\")\n```\n:::\n\n\n2. Extract results\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_tableGar &lt;- results(dds, contrast = contrast_gar)\n```\n:::\n\n\n3. Significant genes\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npadj.cutoff &lt;- 0.05\nsigGar &lt;- res_tableGar %&gt;% as_tibble(rownames = \"gene\") %&gt;%\n  relocate(gene, .before = baseMean) %&gt;% dplyr::filter(padj &lt; padj.cutoff)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigGar\n```\n:::\n\n\n4. Comparison against sigGar vs sigCont\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(sigGar) #number of significant genes in Garlicum vs Vampirium\nnrow(sigCont) #number of significant genes in Control vs Vampirium\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrow(sigCont) - nrow(sigGar)\n```\n:::\n\n\nsigCont has almost 2000 more genes that are differentially expressed!\n\nThis lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nSome materials and hands-on activities were adapted from RNA-seq workflow on the Bioconductor website"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html",
    "href": "develop/rmd/09_summarized_workflow.html",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\nWe have detailed the various steps in a differential expression analysis workflow, providing theory with example code. To provide a more succinct reference for the code needed to run a DGE analysis, we have summarized the steps in an analysis below:\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(DESeq2)\nlibrary(ggrepel)\nlibrary(pheatmap)\nlibrary(annotables)\nlibrary(clusterProfiler)\nlibrary(DOSE)\nlibrary(pathview)\nlibrary(org.Hs.eg.db)\nlibrary(tximport)\nlibrary(RColorBrewer)\n\n\n\n\n\n\n\nLoad data and metadata\n\n\nCode\ndata &lt;- read_table(\"../Data/Vampirium_counts_traditional.tsv\") \n\nmeta &lt;- read_table(\"../Data/samplesheet.csv\")\nmeta$condition &lt;- factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\n\n\nCheck that the row names of the metadata equal the column names of the raw counts data\n\n\nCode\nall(colnames(data)[-1] == meta$sample)\n\n\nCreate DESeq2Dataset object\n\n\nCode\ndds &lt;- DESeqDataSetFromMatrix(countData = data %&gt;% column_to_rownames(\"GeneSymbol\"), \n                              colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)\n\n\n\n\n\nLoad samplesheet with all our metadata from our pipeline\n\n\nCode\n# Load data, metadata and tx2gene and create a txi object\nmeta &lt;- read_csv(\"/work/Intro_bulkRNAseq/Data/samplesheet.csv\")\nmeta$condition &lt;- factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\n\n\nCreate a list of salmon results\n\n\nCode\ndir &lt;- \"/work/Intro_bulkRNAseq/Data/salmon\"\ntx2gene &lt;- read_table(file.path(dir,\"salmon_tx2gene.tsv\"), col_names = c(\"transcript_ID\",\"gene_ID\",\"gene_symbol\"))\n\n# Get all salmon results files\nfiles &lt;- file.path(dir, meta$sample, \"quant.sf\")\nnames(files) &lt;- meta$sample\n\n\nCreate txi object\n\n\nCode\ntxi &lt;- tximport(files, type=\"salmon\", tx2gene=tx2gene, countsFromAbundance = \"lengthScaledTPM\", ignoreTxVersion = TRUE)\n\n\nCreate dds object\n\n\nCode\ndds &lt;- DESeqDataSetFromTximport(txi,\n                                   colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)\n\n\n\n\n\n\nPrefiltering low count genes + PCA & hierarchical clustering - identifying outliers and sources of variation in the data:\n\n\n\n\nCode\nkeep &lt;- rowSums(counts(dds)) &gt; 0\ndds &lt;- dds[keep,]\n\n\n\n\n\n\n\nCode\n# Transform counts for data visualization\nrld &lt;- rlog(dds, \n            blind=TRUE)\n\n\n\n\n\nPlot PCA\n\n\nCode\nplotPCA(rld, \n        intgroup=\"condition\")\n\n\n\n\n\nExtract the rlog matrix from the object\n\n\nCode\nrld_mat &lt;- assay(rld)\nrld_cor &lt;- cor(rld_mat) # Pearson correlation betweeen samples\nrld_dist &lt;- as.matrix(dist(t(assay(rld)))) #distances are computed by rows, so we need to transponse the matrix\n\n\nPlot heatmap of correlations\n\n\nCode\npheatmap(rld_cor, \n         annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"))\n\n\nPlot heatmap of distances with a new color range\n\n\nCode\nheat.colors &lt;- brewer.pal(6, \"Blues\") # Colors from the RColorBrewer package (only 6)\nheat.colors &lt;- colorRampPalette(heat.colors)(100) # Interpolate 100 colors\n\npheatmap(rld_dist, \n         annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"),\n         color = heat.colors)\n\n\n\n\n\n\nOptional step - Re-create DESeq2 dataset if the design formula has changed after QC analysis in include other sources of variation using\n\n\nCode\n# dds &lt;- DESeqDataSetFromMatrix(data, colData = meta, design = ~ covariate + condition)\n\n\nRun DEseq2\n\n\nCode\n# Run DESeq2 differential expression analysis\ndds &lt;- DESeq(dds)\n\n\nOptional step - Output normalized counts to save as a file to access outside RStudio using\n\n\nCode\nnormalized_counts &lt;- counts(dds, normalized=TRUE)\n\n\n\n\n\nPlot dispersion estimates\n\n\nCode\nplotDispEsts(dds)\n\n\n\n\n\nFormal LFC calculation\n\n\nCode\n# Specify contrast for comparison of interest\ncontrast &lt;- c(\"condition\", \"control\", \"vampirium\")\n\n# Output results of Wald test for contrast\nres &lt;- results(dds, \n               contrast = contrast, \n               alpha = 0.05)\n\n\nShrinkage\n\n\nCode\n# Get name of the contrast you would like to use\nresultsNames(dds)\n\n# Shrink the log2 fold changes to be more accurate\nres &lt;- lfcShrink(dds, \n                 coef = \"condition_control_vs_vampirium\", \n                 type = \"apeglm\")\n\n\n\n\n\n\n\nCode\n# Set thresholds\npadj.cutoff &lt;- 0.05\n\n# Turn the results object into a tibble for use with tidyverse functions\nres_tbl &lt;- res %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") %&gt;% \n  as_tibble()\n\n# Subset the significant results\nsig_res &lt;- filter(res_tbl, \n                  padj &lt; padj.cutoff)\n\n\n\n\n\nFunction to get gene_IDs based on gene names. The function will take as input a vector of gene names of interest, the tx2gene dataframe and the dds object that you analyzed.\n\n\nCode\nlookup &lt;- function(gene_name, tx2gene, dds){\n  hits &lt;- tx2gene %&gt;% select(gene_symbol, gene_ID) %&gt;% distinct() %&gt;% \n    filter(gene_symbol %in% gene_name & gene_ID %in% rownames(dds))\n  return(hits)\n}\n\nlookup(gene_name = \"TSPAN7\", tx2gene = tx2gene, dds = dds)\n\n\nPlot expression for single gene\n\n\nCode\nplotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\")\n\n\n\n\n\n\nCode\nplotMA(res)\n\n\n\n\n\n\n\nCode\n## Obtain logical vector where TRUE values denote padj values &lt; 0.05 and fold change &gt; 1.5 in either direction\nres_tbl &lt;- res_tbl %&gt;% \nmutate(threshold = padj &lt; 0.05 & abs(log2FoldChange) &gt;= 0.58)\n\n\n\n\nCode\n## Create an empty column to indicate which genes to label\nres_tbl &lt;- res_tbl %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tbl &lt;- res_tbl %&gt;% arrange(padj)\n\n## Populate the genelabels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tbl$genelabels[1:10] &lt;- as.character(res_tbl$gene[1:10])\n\nhead(res_tbl)\n\n\n\n\nCode\nggplot(res_tbl, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Mov10 overexpression\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25))) \n\n\n\n\n\n\n\nCode\n# filter significant results from normalized counts\nnorm_sig &lt;- normalized_counts %&gt;% as_tibble(rownames = \"gene\") %&gt;%\n  dplyr::filter(gene %in% sig_res$gene) %&gt;% column_to_rownames(var=\"gene\")\n\n\n\n\nCode\npheatmap(norm_sig, \n         cluster_rows = T, #cluster by expression pattern\n         scale = \"row\", # scale by gene so expression pattern is visible\n         treeheight_row = 0, # dont show the row dendogram\n         show_rownames = F, # remove rownames so it is more clear\n         annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% dplyr::select(\"condition\")\n         )\n\n\n\n\n\n\n\n\n\n\nCode\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% res_tbl$gene) \nres_ids &lt;- inner_join(res_tbl, ids, by=c(\"gene\"=\"ensgene\"))\n\n\n\n\n\n\n\nCode\n# Create background dataset for hypergeometric testing using all genes tested for significance in the results\nall_genes &lt;- dplyr::filter(res_ids, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n# Extract significant results\nsig &lt;- dplyr::filter(res_ids, padj &lt; 0.05 & !is.na(gene))\n\nsig_genes &lt;- sig %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n\n\n\nCode\n# Perform enrichment analysis\nego &lt;- enrichGO(gene = sig_genes, \n                universe = all_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\nego &lt;- enrichplot::pairwise_termsim(ego)\n\n\n\n\n\n\n\nCode\ndotplot(ego, showCategory=50)\n\n\n\n\nCode\nemapplot(ego, showCategory = 50)\n\n\n\n\n\n\n\nCode\n## To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector\nsig_foldchanges &lt;- sig$log2FoldChange\n\nnames(sig_foldchanges) &lt;- sig$gene\n\n\n\n\nCode\n## Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj)\ncnetplot(ego, \n         categorySize=\"pvalue\", \n         showCategory = 5, \n         foldChange=sig_foldchanges, \n         vertex.label.font=6)\n\n\n\n\n\n\n\nCode\n# Extract entrez IDs. IDs should not be duplicated or NA\nres_entrez &lt;- dplyr::filter(res_ids, entrez != \"NA\" & entrez != \"NULL\" & duplicated(entrez)==F)\n\n## Extract the foldchanges\nfoldchanges &lt;- res_entrez$log2FoldChange\n\n## Name each fold change with the corresponding Entrez ID\nnames(foldchanges) &lt;- res_entrez$entrez\n\n## Sort fold changes in decreasing order\nfoldchanges &lt;- sort(foldchanges, decreasing = TRUE)\n\n\n\n\nCode\n# Run GSEA of KEGG\ngseaKEGG &lt;- gseKEGG(geneList = foldchanges, # ordered named vector of fold changes (Entrez IDs are the associated names)\n              organism = \"hsa\", # supported organisms listed below\n              pvalueCutoff = 0.05, # padj cutoff value\n              verbose = FALSE)\n\ngseaKEGG_results &lt;- gseaKEGG@result\nhead(gseaKEGG_results)\n\n\n\n\nCode\n## Plot the GSEA plot for a single enriched pathway:\ngseaplot(gseaKEGG, geneSetID = gseaKEGG_results$ID[1])\n\n\n\n\nCode\n## Output images for a single significant KEGG pathway\npathview(gene.data = foldchanges,\n              pathway.id = gseaKEGG_results$ID[1],\n              species = \"hsa\",\n              limit = list(gene = 2, # value gives the max/min limit for foldchanges\n              cpd = 1))\n\n\n\n\nCode\nknitr::include_graphics(paste0(\"./\",gseaKEGG_results$ID[1],\".png\"))\n\n\n\n\n\n\n\n\nCode\nsessionInfo()"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#libraries",
    "href": "develop/rmd/09_summarized_workflow.html#libraries",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(DESeq2)\nlibrary(ggrepel)\nlibrary(pheatmap)\nlibrary(annotables)\nlibrary(clusterProfiler)\nlibrary(DOSE)\nlibrary(pathview)\nlibrary(org.Hs.eg.db)\nlibrary(tximport)\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#obtaining-gene-level-counts-from-your-preprocessing-and-create-deseq-object",
    "href": "develop/rmd/09_summarized_workflow.html#obtaining-gene-level-counts-from-your-preprocessing-and-create-deseq-object",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Load data and metadata\n\n\nCode\ndata &lt;- read_table(\"../Data/Vampirium_counts_traditional.tsv\") \n\nmeta &lt;- read_table(\"../Data/samplesheet.csv\")\nmeta$condition &lt;- factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\n\n\nCheck that the row names of the metadata equal the column names of the raw counts data\n\n\nCode\nall(colnames(data)[-1] == meta$sample)\n\n\nCreate DESeq2Dataset object\n\n\nCode\ndds &lt;- DESeqDataSetFromMatrix(countData = data %&gt;% column_to_rownames(\"GeneSymbol\"), \n                              colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)\n\n\n\n\n\nLoad samplesheet with all our metadata from our pipeline\n\n\nCode\n# Load data, metadata and tx2gene and create a txi object\nmeta &lt;- read_csv(\"/work/Intro_bulkRNAseq/Data/samplesheet.csv\")\nmeta$condition &lt;- factor(meta$condition, levels = c(\"vampirium\", \"control\", \"garlicum\"))\n\n\nCreate a list of salmon results\n\n\nCode\ndir &lt;- \"/work/Intro_bulkRNAseq/Data/salmon\"\ntx2gene &lt;- read_table(file.path(dir,\"salmon_tx2gene.tsv\"), col_names = c(\"transcript_ID\",\"gene_ID\",\"gene_symbol\"))\n\n# Get all salmon results files\nfiles &lt;- file.path(dir, meta$sample, \"quant.sf\")\nnames(files) &lt;- meta$sample\n\n\nCreate txi object\n\n\nCode\ntxi &lt;- tximport(files, type=\"salmon\", tx2gene=tx2gene, countsFromAbundance = \"lengthScaledTPM\", ignoreTxVersion = TRUE)\n\n\nCreate dds object\n\n\nCode\ndds &lt;- DESeqDataSetFromTximport(txi,\n                                   colData = meta %&gt;% column_to_rownames(\"sample\"), \n                              design = ~ condition)"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#exploratory-data-analysis",
    "href": "develop/rmd/09_summarized_workflow.html#exploratory-data-analysis",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Prefiltering low count genes + PCA & hierarchical clustering - identifying outliers and sources of variation in the data:\n\n\n\n\nCode\nkeep &lt;- rowSums(counts(dds)) &gt; 0\ndds &lt;- dds[keep,]\n\n\n\n\n\n\n\nCode\n# Transform counts for data visualization\nrld &lt;- rlog(dds, \n            blind=TRUE)\n\n\n\n\n\nPlot PCA\n\n\nCode\nplotPCA(rld, \n        intgroup=\"condition\")\n\n\n\n\n\nExtract the rlog matrix from the object\n\n\nCode\nrld_mat &lt;- assay(rld)\nrld_cor &lt;- cor(rld_mat) # Pearson correlation betweeen samples\nrld_dist &lt;- as.matrix(dist(t(assay(rld)))) #distances are computed by rows, so we need to transponse the matrix\n\n\nPlot heatmap of correlations\n\n\nCode\npheatmap(rld_cor, \n         annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"))\n\n\nPlot heatmap of distances with a new color range\n\n\nCode\nheat.colors &lt;- brewer.pal(6, \"Blues\") # Colors from the RColorBrewer package (only 6)\nheat.colors &lt;- colorRampPalette(heat.colors)(100) # Interpolate 100 colors\n\npheatmap(rld_dist, \n         annotation = meta %&gt;% column_to_rownames(\"sample\") %&gt;% select(\"condition\"),\n         color = heat.colors)"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#run-deseq2",
    "href": "develop/rmd/09_summarized_workflow.html#run-deseq2",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Optional step - Re-create DESeq2 dataset if the design formula has changed after QC analysis in include other sources of variation using\n\n\nCode\n# dds &lt;- DESeqDataSetFromMatrix(data, colData = meta, design = ~ covariate + condition)\n\n\nRun DEseq2\n\n\nCode\n# Run DESeq2 differential expression analysis\ndds &lt;- DESeq(dds)\n\n\nOptional step - Output normalized counts to save as a file to access outside RStudio using\n\n\nCode\nnormalized_counts &lt;- counts(dds, normalized=TRUE)"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#check-the-fit-of-the-dispersion-estimates",
    "href": "develop/rmd/09_summarized_workflow.html#check-the-fit-of-the-dispersion-estimates",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Plot dispersion estimates\n\n\nCode\nplotDispEsts(dds)"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#create-contrasts-to-perform-wald-testing-or-the-shrunken-log2-fold-changes-between-specific-conditions",
    "href": "develop/rmd/09_summarized_workflow.html#create-contrasts-to-perform-wald-testing-or-the-shrunken-log2-fold-changes-between-specific-conditions",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Formal LFC calculation\n\n\nCode\n# Specify contrast for comparison of interest\ncontrast &lt;- c(\"condition\", \"control\", \"vampirium\")\n\n# Output results of Wald test for contrast\nres &lt;- results(dds, \n               contrast = contrast, \n               alpha = 0.05)\n\n\nShrinkage\n\n\nCode\n# Get name of the contrast you would like to use\nresultsNames(dds)\n\n# Shrink the log2 fold changes to be more accurate\nres &lt;- lfcShrink(dds, \n                 coef = \"condition_control_vs_vampirium\", \n                 type = \"apeglm\")"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#output-significant-results",
    "href": "develop/rmd/09_summarized_workflow.html#output-significant-results",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Code\n# Set thresholds\npadj.cutoff &lt;- 0.05\n\n# Turn the results object into a tibble for use with tidyverse functions\nres_tbl &lt;- res %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(var=\"gene\") %&gt;% \n  as_tibble()\n\n# Subset the significant results\nsig_res &lt;- filter(res_tbl, \n                  padj &lt; padj.cutoff)"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#visualize-results-volcano-plots-heatmaps-normalized-counts-plots-of-top-genes-etc.",
    "href": "develop/rmd/09_summarized_workflow.html#visualize-results-volcano-plots-heatmaps-normalized-counts-plots-of-top-genes-etc.",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Function to get gene_IDs based on gene names. The function will take as input a vector of gene names of interest, the tx2gene dataframe and the dds object that you analyzed.\n\n\nCode\nlookup &lt;- function(gene_name, tx2gene, dds){\n  hits &lt;- tx2gene %&gt;% select(gene_symbol, gene_ID) %&gt;% distinct() %&gt;% \n    filter(gene_symbol %in% gene_name & gene_ID %in% rownames(dds))\n  return(hits)\n}\n\nlookup(gene_name = \"TSPAN7\", tx2gene = tx2gene, dds = dds)\n\n\nPlot expression for single gene\n\n\nCode\nplotCounts(dds, gene=\"ENSG00000156298\", intgroup=\"condition\")\n\n\n\n\n\n\nCode\nplotMA(res)\n\n\n\n\n\n\n\nCode\n## Obtain logical vector where TRUE values denote padj values &lt; 0.05 and fold change &gt; 1.5 in either direction\nres_tbl &lt;- res_tbl %&gt;% \nmutate(threshold = padj &lt; 0.05 & abs(log2FoldChange) &gt;= 0.58)\n\n\n\n\nCode\n## Create an empty column to indicate which genes to label\nres_tbl &lt;- res_tbl %&gt;% mutate(genelabels = \"\")\n\n## Sort by padj values \nres_tbl &lt;- res_tbl %&gt;% arrange(padj)\n\n## Populate the genelabels column with contents of the gene symbols column for the first 10 rows, i.e. the top 10 most significantly expressed genes\nres_tbl$genelabels[1:10] &lt;- as.character(res_tbl$gene[1:10])\n\nhead(res_tbl)\n\n\n\n\nCode\nggplot(res_tbl, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(colour = threshold)) +\n  geom_text_repel(aes(label = genelabels)) +\n  ggtitle(\"Mov10 overexpression\") +\n  xlab(\"log2 fold change\") + \n  ylab(\"-log10 adjusted p-value\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25))) \n\n\n\n\n\n\n\nCode\n# filter significant results from normalized counts\nnorm_sig &lt;- normalized_counts %&gt;% as_tibble(rownames = \"gene\") %&gt;%\n  dplyr::filter(gene %in% sig_res$gene) %&gt;% column_to_rownames(var=\"gene\")\n\n\n\n\nCode\npheatmap(norm_sig, \n         cluster_rows = T, #cluster by expression pattern\n         scale = \"row\", # scale by gene so expression pattern is visible\n         treeheight_row = 0, # dont show the row dendogram\n         show_rownames = F, # remove rownames so it is more clear\n         annotation = meta %&gt;% column_to_rownames(var = \"sample\") %&gt;% dplyr::select(\"condition\")\n         )"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#perform-analysis-to-extract-functional-significance-of-results-go-or-kegg-enrichment-gsea-etc.",
    "href": "develop/rmd/09_summarized_workflow.html#perform-analysis-to-extract-functional-significance-of-results-go-or-kegg-enrichment-gsea-etc.",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Code\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% res_tbl$gene) \nres_ids &lt;- inner_join(res_tbl, ids, by=c(\"gene\"=\"ensgene\"))\n\n\n\n\n\n\n\nCode\n# Create background dataset for hypergeometric testing using all genes tested for significance in the results\nall_genes &lt;- dplyr::filter(res_ids, !is.na(gene)) %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n# Extract significant results\nsig &lt;- dplyr::filter(res_ids, padj &lt; 0.05 & !is.na(gene))\n\nsig_genes &lt;- sig %&gt;% \n  pull(gene) %&gt;% \n  as.character()\n\n\n\n\nCode\n# Perform enrichment analysis\nego &lt;- enrichGO(gene = sig_genes, \n                universe = all_genes,\n                keyType = \"ENSEMBL\",\n                OrgDb = org.Hs.eg.db, \n                ont = \"BP\", \n                pAdjustMethod = \"BH\", \n                qvalueCutoff = 0.05, \n                readable = TRUE)\nego &lt;- enrichplot::pairwise_termsim(ego)\n\n\n\n\n\n\n\nCode\ndotplot(ego, showCategory=50)\n\n\n\n\nCode\nemapplot(ego, showCategory = 50)\n\n\n\n\n\n\n\nCode\n## To color genes by log2 fold changes, we need to extract the log2 fold changes from our results table creating a named vector\nsig_foldchanges &lt;- sig$log2FoldChange\n\nnames(sig_foldchanges) &lt;- sig$gene\n\n\n\n\nCode\n## Cnetplot details the genes associated with one or more terms - by default gives the top 5 significant terms (by padj)\ncnetplot(ego, \n         categorySize=\"pvalue\", \n         showCategory = 5, \n         foldChange=sig_foldchanges, \n         vertex.label.font=6)\n\n\n\n\n\n\n\nCode\n# Extract entrez IDs. IDs should not be duplicated or NA\nres_entrez &lt;- dplyr::filter(res_ids, entrez != \"NA\" & entrez != \"NULL\" & duplicated(entrez)==F)\n\n## Extract the foldchanges\nfoldchanges &lt;- res_entrez$log2FoldChange\n\n## Name each fold change with the corresponding Entrez ID\nnames(foldchanges) &lt;- res_entrez$entrez\n\n## Sort fold changes in decreasing order\nfoldchanges &lt;- sort(foldchanges, decreasing = TRUE)\n\n\n\n\nCode\n# Run GSEA of KEGG\ngseaKEGG &lt;- gseKEGG(geneList = foldchanges, # ordered named vector of fold changes (Entrez IDs are the associated names)\n              organism = \"hsa\", # supported organisms listed below\n              pvalueCutoff = 0.05, # padj cutoff value\n              verbose = FALSE)\n\ngseaKEGG_results &lt;- gseaKEGG@result\nhead(gseaKEGG_results)\n\n\n\n\nCode\n## Plot the GSEA plot for a single enriched pathway:\ngseaplot(gseaKEGG, geneSetID = gseaKEGG_results$ID[1])\n\n\n\n\nCode\n## Output images for a single significant KEGG pathway\npathview(gene.data = foldchanges,\n              pathway.id = gseaKEGG_results$ID[1],\n              species = \"hsa\",\n              limit = list(gene = 2, # value gives the max/min limit for foldchanges\n              cpd = 1))\n\n\n\n\nCode\nknitr::include_graphics(paste0(\"./\",gseaKEGG_results$ID[1],\".png\"))"
  },
  {
    "objectID": "develop/rmd/09_summarized_workflow.html#make-sure-to-output-the-versions-of-all-tools-used-in-the-de-analysis",
    "href": "develop/rmd/09_summarized_workflow.html#make-sure-to-output-the-versions-of-all-tools-used-in-the-de-analysis",
    "title": "Summary of DGE workflow",
    "section": "",
    "text": "Code\nsessionInfo()"
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html",
    "href": "develop/rmd/07_extra_contrast_design.html",
    "title": "Contrast designs",
    "section": "",
    "text": "Last updated: {{ git_revision_date_localized }}\n!!! note ‚ÄúSection Overview‚Äù\n&#128368; **Time Estimation:** 40 minutes  \n\n&#128172; **Learning Objectives:**    \n\n1.  Demonstrate the use of the design formula with simple and complex designs\n2.  Construct R code to execute the differential expression analysis workflow with DESeq2\nThe final step in the differential expression analysis workflow is fitting the raw counts to the NB model and performing the statistical test for differentially expressed genes. In this step we essentially want to determine whether the mean expression levels of different sample groups are significantly different.\n\n\n\n\n\n\n\n\n\nImage credit: Paul Pavlidis, UBC\nThe DESeq2 paper was published in 2014, but the package is continually updated and available for use in R through Bioconductor. It builds on good ideas for dispersion estimation and use of Generalized Linear Models from the DSS and edgeR methods.\nDifferential expression analysis with DESeq2 involves multiple steps as displayed in the flowchart below in blue. Briefly, DESeq2 will model the raw counts, using normalization factors (size factors) to account for differences in library depth. Then, it will estimate the gene-wise dispersions and shrink these estimates to generate more accurate estimates of dispersion to model the counts. Finally, DESeq2 will fit the negative binomial model and perform hypothesis testing using the Wald test or Likelihood Ratio Test.\n\n\n\n\n\n\n\n\n\n!!! tip\nDESeq2 is actively maintained by the developers and continuously being updated. As such, it is important that you note the version you are working with. Recently, there have been some rather **big changes implemented** that impact the output. To find out more detail about the specific **modifications made to methods described in the original 2014 paper**, take a look at [this section in the DESeq2 vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#methods-changes-since-the-2014-deseq2-paper).\n\nAdditional details on the statistical concepts underlying DESeq2 are elucidated nicely in Rafael Irizarry's [materials](https://rafalab.github.io/pages/harvardx.html) for the EdX course, \"Data Analysis for the Life Sciences Series\".\nPrior to performing the differential expression analysis, it is a good idea to know what sources of variation are present in your data, either by exploration during the QC and/or prior knowledge. Once you know the major sources of variation, you can remove them prior to analysis or control for them in the statistical model by including them in your design formula.\n\n\nA design formula tells the statistical software the known sources of variation to control for, as well as, the factor of interest to test for during differential expression testing. For example, if you know that sex is a significant source of variation in your data, then sex should be included in your model. The design formula should have all of the factors in your metadata that account for major sources of variation in your data. The last factor entered in the formula should be the condition of interest.\nFor example, suppose you have the following metadata:\n\n\n\n\n\n\n\n\n\nIf you want to examine the expression differences between condition, and you know that major sources of variation include bloodtype and patient, then your design formula would be:\ndesign = ~ bloodtype + patient + condition\nThe tilde (~) should always precede your factors and tells DESeq2 to model the counts using the following formula. Note the factors included in the design formula need to match the column names in the metadata.\nIn this tutorial we show a general and flexible way to define contrasts, and is often useful for more complex contrasts or when the design of the experiment is imbalanced (e.g.¬†different number of replicates in each group). Although we focus on DESeq2, the approach can also be used with the other popular package edgeR.\nEach section below covers a particular experimental design, from simpler to more complex ones. The first chunk of code in each section is to simulate data, which has no particular meaning and is only done in order to have a DESeqDataSet object with the right kind of variables for each example. In practice, users can ignore this step as they should have created a DESeqDataSet object from their own data following the instructions in the vignette.\n\n\n\n\n\nCode\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 6, betaSD = 2)\ndds$condition &lt;- factor(rep(c(\"control\", \"treat\"), each = 3))\n\n\nFirst we can look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 6 rows and 1 column\n##         condition\n##          &lt;factor&gt;\n## sample1   control\n## sample2   control\n## sample3   control\n## sample4   treat  \n## sample5   treat  \n## sample6   treat\nOur factor of interest is condition and so we define our design and run the DESeq model fitting routine:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + condition # or just `~ condition`\ndds &lt;- DESeq(dds) # equivalent to edgeR::glmFit()\n\n\nThen check what coefficients DESeq estimated:\n\n\nCode\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"condition_treat_vs_control\"\nWe can see that we have a coefficient for our intercept and coefficient for the effect of treat (i.e.¬†differences between treat versus control).\nUsing the more standard syntax, we can obtain the results for the effect of treat as such:\n\n\nCode\nres1 &lt;- results(dds, contrast = list(\"condition_treat_vs_control\"))\nres1\n\n\n## log2 fold change (MLE): condition_treat_vs_control effect \n## Wald test p-value: condition_treat_vs_control effect \n## DataFrame with 1000 rows and 6 columns\n##           baseMean log2FoldChange     lfcSE         stat      pvalue\n##          &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;   &lt;numeric&gt;\n## gene1     40.90941    1.267525859  0.574144  2.207679752   0.0272666\n## gene2     12.21876   -0.269917301  1.111127 -0.242922069   0.8080658\n## gene3      1.91439   -3.538133611  2.564464 -1.379677442   0.1676860\n## gene4     10.24472    0.954811627  1.166408  0.818591708   0.4130194\n## gene5     13.16824    0.000656519  0.868780  0.000755679   0.9993971\n## ...            ...            ...       ...          ...         ...\n## gene996   40.43827     -1.0291276  0.554587    -1.855664 0.063501471\n## gene997   52.88360      0.0622133  0.561981     0.110704 0.911851377\n## gene998   73.06582      1.3271896  0.576695     2.301373 0.021370581\n## gene999    8.87701     -5.8385374  1.549471    -3.768084 0.000164506\n## gene1000  37.06533      1.2669314  0.602010     2.104501 0.035334764\n##                 padj\n##            &lt;numeric&gt;\n## gene1      0.0712378\n## gene2      0.8779871\n## gene3      0.2943125\n## gene4      0.5692485\n## gene5      0.9996728\n## ...              ...\n## gene996  0.138827354\n## gene997  0.948279388\n## gene998  0.059599481\n## gene999  0.000914882\n## gene1000 0.087737235\nThe above is a simple way to obtain the results of interest. But it is worth understanding how DESeq is getting to these results by looking at the model‚Äôs matrix. DESeq defines the model matrix using base R functionality:\n\n\nCode\nmodel.matrix(design(dds), colData(dds))\n\n\n##         (Intercept) conditiontreat\n## sample1           1              0\n## sample2           1              0\n## sample3           1              0\n## sample4           1              1\n## sample5           1              1\n## sample6           1              1\n## attr(,\"assign\")\n## [1] 0 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\nWe can see that R coded condition as a dummy variable, with an intercept (common to all samples) and a ‚Äúconditiontreat‚Äù variable, which adds the effect of treat to samples 4-6.\nWe can actually set our contrasts in DESeq2::results() using a numeric vector. The way it works is to define a vector of ‚Äúweights‚Äù for the coefficient(s) we want to test for. In this case, we have (Intercept) and conditiontreat as our coefficients (see model matrix above), and we want to test for the effect of treat, so our contrast vector would be c(0, 1). In other words, we don‚Äôt care about the value of (Intercept) (so it has a weight of 0), and we‚Äôre only interested in the effect of treat (so we give it a weight of 1).\nIn this case the design is very simple, so we could define our contrast vector ‚Äúmanually‚Äù. But for complex designs this can get more difficult to do, so it‚Äôs worth mentioning the general way in which we can define this. For any contrast of interest, we can follow three steps:\n\nGet the model matrix\nSubset the matrix for each group of interest and calculate its column means - this results in a vector of coefficients for each group\nSubtract the group vectors from each other according to the comparison we‚Äôre interested in\n\nLet‚Äôs see this example in action:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##         (Intercept) conditiontreat\n## sample1           1              0\n## sample2           1              0\n## sample3           1              0\n## sample4           1              1\n## sample5           1              1\n## sample6           1              1\n## attr(,\"assign\")\n## [1] 0 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n\n\nCode\n# calculate the vector of coefficient weights in the treat\ntreat &lt;- colMeans(mod_mat[dds$condition == \"treat\", ])\ntreat\n\n\n##    (Intercept) conditiontreat \n##              1              1\n\n\nCode\n# calculate the vector of coefficient weights in the control\ncontrol &lt;- colMeans(mod_mat[dds$condition == \"control\", ])\ncontrol\n\n\n##    (Intercept) conditiontreat \n##              1              0\n\n\nCode\n# The contrast we are interested in is the difference between treat and control\ntreat - control\n\n\n##    (Intercept) conditiontreat \n##              0              1\nThat last step is where we define our contrast vector, and we can give this directly to the results function:\n\n\nCode\n# get the results for this contrast\nres2 &lt;- results(dds, contrast = treat - control)\n\n\nThis gives us exactly the same results as before, which we can check for example by plotting the log-fold-changes between the first and second approach:\n\n\nCode\nplot(res1$log2FoldChange, res2$log2FoldChange)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOften, we can use different model matrices that essentially correspond to the same design. For example, we could recode our design above by removing the intercept:\n\n\nCode\ndesign(dds) &lt;- ~ 0 + condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"conditioncontrol\" \"conditiontreat\"\nIn this case we get a coefficient corresponding to the average expression in control and the average expression in the treat (rather than the difference between treat and control).\nIf we use the same contrast trick as before (using the model matrix), we can see the result is the same:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##         conditioncontrol conditiontreat\n## sample1                1              0\n## sample2                1              0\n## sample3                1              0\n## sample4                0              1\n## sample5                0              1\n## sample6                0              1\n## attr(,\"assign\")\n## [1] 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n\n\nCode\n# calculate weights for coefficients in each condition\ntreat &lt;- colMeans(mod_mat[which(dds$condition == \"treat\"), ])\ncontrol &lt;- colMeans(mod_mat[which(dds$condition == \"control\"), ])\n# get the results for our contrast\nres3 &lt;- results(dds, contrast = treat - control)\n\n\nAgain, the results are essentially the same:\n\n\nCode\nplot(res1$log2FoldChange, res3$log2FoldChange)\n\n\n\n\n\n\n\n\n\n\n\nIn theory there‚Äôs no difference between these two ways of defining our design. The design with an intercept is more common, but for the purposes of understanding what‚Äôs going on, it‚Äôs sometimes easier to look at models without intercept.\n\n\n\n\n\nCode\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 9, betaSD = 2)\ndds$condition &lt;- NULL\ndds$bloodtype &lt;- factor(rep(c(\"bloodA\", \"bloodB\", \"bloodO\"), each = 3))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\n\n\nFirst we can look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 9 rows and 1 column\n##         bloodtype\n##          &lt;factor&gt;\n## sample1    bloodA\n## sample2    bloodA\n## sample3    bloodA\n## sample4    bloodB\n## sample5    bloodB\n## sample6    bloodB\n## sample7    bloodO\n## sample8    bloodO\n## sample9    bloodO\nAs in the previous example, we only have one factor of interest, bloodtype, and so we define our design and run the DESeq as before:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + bloodtype\ndds &lt;- DESeq(dds)\n# check the coefficients estimated by DEseq\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"bloodtype_bloodA_vs_bloodO\"\n## [3] \"bloodtype_bloodB_vs_bloodO\"\nWe see that now we have 3 coefficients:\n\n‚ÄúIntercept‚Äù corresponds to bloodO bloodtype (our reference level)\n‚Äúbloodtype_bloodA_vs_bloodO‚Äù corresponds to the difference between the reference level and bloodA\n‚Äúbloodtype_bloodB_vs_bloodO‚Äù corresponds to the difference between the reference level and bloodB\n\nWe could obtain the difference between bloodO and any of the two bloodtypes easily:\n\n\nCode\nres1_bloodA_bloodO &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\"))\nres1_bloodB_bloodO &lt;- results(dds, contrast = list(\"bloodtype_bloodB_vs_bloodO\"))\n\n\nFor comparing bloodA vs bloodB, however, we need to compare two coefficients with each other to check whether they are themselves different (check the slide to see the illustration). This is how the standard DESeq syntax would be:\n\n\nCode\nres1_bloodA_bloodB &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\", \n                                                 \"bloodtype_bloodB_vs_bloodO\"))\n\n\nHowever, following our three steps detailed in the first section, we can define our comparisons from the design matrix:\n\n\nCode\n# define the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##         (Intercept) bloodtypebloodA bloodtypebloodB\n## sample1           1               1               0\n## sample2           1               1               0\n## sample3           1               1               0\n## sample4           1               0               1\n## sample5           1               0               1\n## sample6           1               0               1\n## sample7           1               0               0\n## sample8           1               0               0\n## sample9           1               0               0\n## attr(,\"assign\")\n## [1] 0 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$bloodtype\n## [1] \"contr.treatment\"\n\n\nCode\n# calculate coefficient vectors for each group\nbloodA &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\", ])\nbloodB &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodB\", ])\nbloodO &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\", ])\n\n\nAnd we can now define any contrasts we want:\n\n\nCode\n# obtain results for each pairwise contrast\nres2_bloodA_bloodO &lt;- results(dds, contrast = bloodA - bloodO)\nres2_bloodB_bloodO &lt;- results(dds, contrast = bloodB - bloodO)\nres2_bloodA_bloodB &lt;- results(dds, contrast = bloodA - bloodB)\n# plot the results from the two approaches to check that they are identical\nplot(res1_bloodA_bloodO$log2FoldChange, res2_bloodA_bloodO$log2FoldChange)\nplot(res1_bloodB_bloodO$log2FoldChange, res2_bloodB_bloodO$log2FoldChange)\nplot(res1_bloodA_bloodB$log2FoldChange, res2_bloodA_bloodB$log2FoldChange)\n\n\n\n\nWith this approach, we could even define a more unusual contrast, for example to find genes that differ between A and B against and O samples:\n\n\nCode\n# define vector of coefficients for A_B samples\nA_B &lt;- colMeans(mod_mat[dds$bloodtype %in% c(\"bloodA\", \"bloodB\"),])\n# Our contrast of interest is\nA_B - bloodO\n\n\n##     (Intercept) bloodtypebloodA bloodtypebloodB \n##             0.0             0.5             0.5\nNotice the contrast vector in this case assigns a ‚Äúweight‚Äù of 0.5 to each of bloodtypebloodA and bloodtypebloodB. This is equivalent to saying that we want to consider the average of bloodA and bloodB expression. In fact, we could have also defined our contrast vector like this:\n\n\nCode\n# average of bloodA and bloodB minus bloodO\n(bloodA + bloodB)/2 - bloodO\n\n\n##     (Intercept) bloodtypebloodA bloodtypebloodB \n##             0.0             0.5             0.5\nTo obtain our results, we use the results() function as before:\n\n\nCode\n# get the results between A_B and bloodA\nres2_AB &lt;- results(dds, contrast = A_B - bloodO)\n\n\n\n\n\nFor this last example (A_B vs bloodO), we may have considered creating a new variable in our column data:\n\n\nCode\ndds$A_B &lt;- factor(dds$bloodtype %in% c(\"bloodA\", \"bloodB\"))\ncolData(dds)\n\n\n## DataFrame with 9 rows and 3 columns\n##         bloodtype sizeFactor      A_B\n##          &lt;factor&gt;  &lt;numeric&gt; &lt;factor&gt;\n## sample1    bloodA   0.972928    TRUE \n## sample2    bloodA   0.985088    TRUE \n## sample3    bloodA   0.960749    TRUE \n## sample4    bloodB   0.916582    TRUE \n## sample5    bloodB   0.936918    TRUE \n## sample6    bloodB   1.137368    TRUE \n## sample7    bloodO   1.071972    FALSE\n## sample8    bloodO   1.141490    FALSE\n## sample9    bloodO   1.140135    FALSE\nand then re-run DESeq with a new design:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + A_B\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"         \"A_B_TRUE_vs_FALSE\"\n\n\nCode\nres1_A_B &lt;- results(dds, contrast = list(\"A_B_TRUE_vs_FALSE\"))\n\n\nHowever, in this model the gene dispersion is estimated together for bloodA and bloodB samples as if they were replicates of each other, which may result in inflated/deflated estimates. Instead, our approach above estimates the error within each of those groups.\nTo check the difference one could compare the two approaches visually:\n\n\nCode\n# compare the log-fold-changes between the two approaches\nplot(res1_A_B$log2FoldChange, res2_AB$log2FoldChange)\nabline(0, 1, col = \"brown\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# compare the errors between the two approaches\nplot(res1_A_B$lfcSE, res2_AB$lfcSE)\nabline(0, 1, col = \"brown\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 12, betaSD = 2)\ndds$bloodtype &lt;- factor(rep(c(\"bloodO\", \"bloodA\"), each = 6))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\ndds$condition &lt;- factor(rep(c(\"treat\", \"control\"), 6))\ndds &lt;- dds[, order(dds$bloodtype, dds$condition)]\ncolnames(dds) &lt;- paste0(\"sample\", 1:ncol(dds))\n\n\nFirst let‚Äôs look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 12 rows and 2 columns\n##          condition bloodtype\n##           &lt;factor&gt;  &lt;factor&gt;\n## sample1    control    bloodO\n## sample2    control    bloodO\n## sample3    control    bloodO\n## sample4    treat      bloodO\n## sample5    treat      bloodO\n## ...            ...       ...\n## sample8    control    bloodA\n## sample9    control    bloodA\n## sample10   treat      bloodA\n## sample11   treat      bloodA\n## sample12   treat      bloodA\nThis time we have two factors of interest, and we want to model both with an interaction (i.e.¬†we assume that bloodA and bloodO samples may respond differently to treat/control). We define our design accordingly and fit the model:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + bloodtype + condition + bloodtype:condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                      \"bloodtype_bloodA_vs_bloodO\"    \n## [3] \"condition_treat_vs_control\"     \"bloodtypebloodA.conditiontreat\"\nBecause we have two factors and an interaction, the number of comparisons we can do is larger. Using our three-step approach from the model matrix, we do things exactly as we‚Äôve been doing so far:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\n# Define coefficient vectors for each condition\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\n\n\nWe are now ready to define any contrast of interest from these vectors (for completeness we show the equivalent syntax using the coefficient‚Äôs names from DESeq).\nbloodA vs bloodO (in the control):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodA_control - bloodO_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\"))\n\n\nbloodA vs bloodO (in the treatment):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodO_treat - bloodA_treat)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"bloodtype_bloodA_vs_bloodO\",\n                                       \"bloodtypebloodA.conditiontreat\")))\n\n\ntreat vs control (for bloodtypes O):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodO_treat - bloodO_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"condition_treat_vs_control\")))\n\n\ntreat vs control (for bloodtypes A):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodA_treat - bloodA_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"condition_treat_vs_control\", \n                                       \"bloodtypebloodA.conditiontreat\")))\n\n\nInteraction between bloodtype and condition\nI.e. do bloodAs and bloodOs respond differently to the treatment?\n\n\nCode\nres1 &lt;- results(dds, \n                contrast = (bloodA_treat - bloodA_control) - (bloodO_treat - bloodO_control))\n# or equivalently\nres2 &lt;- results(dds, contrast = list(\"bloodtypebloodA.conditiontreat\"))\n\n\nIn conclusion, although we can define these contrasts using DESeq coefficient names, it is somewhat more explicit (and perhaps intuitive?) what it is we‚Äôre comparing using matrix-based contrasts.\n\n\n\n\n\nCode\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 24, betaSD = 2)\ndds$bloodtype &lt;- factor(rep(c(\"bloodA\", \"bloodO\"), each = 12))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\ndds$patient &lt;- factor(rep(LETTERS[1:4], each = 6))\ndds$condition &lt;- factor(rep(c(\"treat\", \"control\"), 12))\ndds &lt;- dds[, order(dds$bloodtype, dds$patient, dds$condition)]\ncolnames(dds) &lt;- paste0(\"sample\", 1:ncol(dds))\n\n\nFirst let‚Äôs look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 24 rows and 3 columns\n##          condition bloodtype  patient\n##           &lt;factor&gt;  &lt;factor&gt; &lt;factor&gt;\n## sample1    control    bloodO        C\n## sample2    control    bloodO        C\n## sample3    control    bloodO        C\n## sample4    treat      bloodO        C\n## sample5    treat      bloodO        C\n## ...            ...       ...      ...\n## sample20   control    bloodA        B\n## sample21   control    bloodA        B\n## sample22   treat      bloodA        B\n## sample23   treat      bloodA        B\n## sample24   treat      bloodA        B\nNow we have three factors, but patient is nested within bloodtype (i.e.¬†a patient is either bloodA or bloodO, it cannot be both). Therefore, bloodtype is a linear combination with patient (or, another way to think about it is that bloodtype is redundant with patient). Because of this, we will define our design without including ‚Äúbloodtype‚Äù, although later we can compare groups of patient of the same bloodtype with each other.\n\n\nCode\ndesign(dds) &lt;- ~ 1 + patient + condition + patient:condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"patient_B_vs_A\"            \n## [3] \"patient_C_vs_A\"             \"patient_D_vs_A\"            \n## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\"   \n## [7] \"patientC.conditiontreat\"    \"patientD.conditiontreat\"\nNow it‚Äôs harder to define contrasts between groups of patient of the same bloodtype using DESeq‚Äôs coefficient names (although still possible). But using the model matrix approach, we do it in exactly the same way we have done so far!\nAgain, let‚Äôs define our groups from the model matrix:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\n# define coefficient vectors for each group\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\n\n\nIt‚Äôs worth looking at some of these vectors, to see that they are composed of weighted coefficients from different patient. For example, for ‚ÄúbloodO‚Äù patient, we have equal contribution from ‚ÄúpatientC‚Äù and ‚ÄúpatientD‚Äù:\n\n\nCode\nbloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     1.0                     0.0                     0.5 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.5                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\nAnd so, when we define our contrasts, each patient will be correctly weighted:\n\n\nCode\nbloodO_treat - bloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.0                     0.0 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.0                     1.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.5                     0.5\nWe can set our contrasts in exactly the same way as we did in the previous section (for completeness, we also give the contrasts using DESeq‚Äôs named coefficients).\nbloodA vs bloodO (in the control):\n\n\nCode\nres1_bloodA_bloodO_control &lt;- results(dds, contrast = bloodA_control - bloodO_control)\n# or equivalently\nres2_bloodA_bloodO_control &lt;- results(dds, \n                                 contrast = list(c(\"patient_B_vs_A\"), # Blood type A\n                                                 c(\"patient_C_vs_A\", # Blood type O\n                                                   \"patient_D_vs_A\"))) # Blood type O\n\n\nbloodA vs bloodO (in the treat):\n\n\nCode\nres1_bloodO_bloodA_treat &lt;- results(dds, contrast = bloodO_treat - bloodA_treat)\n# or equivalently\nres2_bloodO_bloodA_treat &lt;- results(dds, \n                           contrast = list(c(\"patient_B_vs_A\", # Blood type A\n                                             \"patientB.conditiontreat\"), # Interaction of patient B with treatment\n                                           c(\"patient_C_vs_A\", # Blood type O\n                                             \"patient_D_vs_A\", # Blood type O\n                                             \"patientC.conditiontreat\", # Interaction of patient C with treatment\n                                             \"patientD.conditiontreat\"))) # Interaction of patient B with treatment\n\n\nAnd so on, for other contrasts of interest‚Ä¶\n\n\n\n\nLet‚Äôs take our previous example, but drop one of the samples from the data, so that we only have 2 replicates for it.\n\n\nCode\ndds &lt;- dds[, -1] # drop one of the patient C samples\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"patient_B_vs_A\"            \n## [3] \"patient_C_vs_A\"             \"patient_D_vs_A\"            \n## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\"   \n## [7] \"patientC.conditiontreat\"    \"patientD.conditiontreat\"\nDefine our model matrix and coefficient vectors:\n\n\nCode\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##          (Intercept) patientB patientC patientD conditiontreat\n## sample2            1        0        1        0              0\n## sample3            1        0        1        0              0\n## sample4            1        0        1        0              1\n## sample5            1        0        1        0              1\n## sample6            1        0        1        0              1\n## sample7            1        0        0        1              0\n## sample8            1        0        0        1              0\n## sample9            1        0        0        1              0\n## sample10           1        0        0        1              1\n## sample11           1        0        0        1              1\n## sample12           1        0        0        1              1\n## sample13           1        0        0        0              0\n## sample14           1        0        0        0              0\n## sample15           1        0        0        0              0\n## sample16           1        0        0        0              1\n## sample17           1        0        0        0              1\n## sample18           1        0        0        0              1\n## sample19           1        1        0        0              0\n## sample20           1        1        0        0              0\n## sample21           1        1        0        0              0\n## sample22           1        1        0        0              1\n## sample23           1        1        0        0              1\n## sample24           1        1        0        0              1\n##          patientB:conditiontreat patientC:conditiontreat\n## sample2                        0                       0\n## sample3                        0                       0\n## sample4                        0                       1\n## sample5                        0                       1\n## sample6                        0                       1\n## sample7                        0                       0\n## sample8                        0                       0\n## sample9                        0                       0\n## sample10                       0                       0\n## sample11                       0                       0\n## sample12                       0                       0\n## sample13                       0                       0\n## sample14                       0                       0\n## sample15                       0                       0\n## sample16                       0                       0\n## sample17                       0                       0\n## sample18                       0                       0\n## sample19                       0                       0\n## sample20                       0                       0\n## sample21                       0                       0\n## sample22                       1                       0\n## sample23                       1                       0\n## sample24                       1                       0\n##          patientD:conditiontreat\n## sample2                        0\n## sample3                        0\n## sample4                        0\n## sample5                        0\n## sample6                        0\n## sample7                        0\n## sample8                        0\n## sample9                        0\n## sample10                       1\n## sample11                       1\n## sample12                       1\n## sample13                       0\n## sample14                       0\n## sample15                       0\n## sample16                       0\n## sample17                       0\n## sample18                       0\n## sample19                       0\n## sample20                       0\n## sample21                       0\n## sample22                       0\n## sample23                       0\n## sample24                       0\n## attr(,\"assign\")\n## [1] 0 1 1 1 2 3 3 3\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$patient\n## [1] \"contr.treatment\"\n## \n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n\n\nCode\n# define coefficient vectors for each group\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\n\n\nNow let‚Äôs check what happens to the bloodO_control group:\n\n\nCode\nbloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     1.0                     0.0                     0.4 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.6                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\nNotice that whereas before ‚ÄúpatientC‚Äù and ‚ÄúpatientD‚Äù had each a weight of 0.5, now they have different weights. That‚Äôs because for patientC there‚Äôs only 2 replicates. So, we have a total of 5 bloodtype O individuals in the control (2 from patient C and 3 from D). Therefore, when we calculate the average coefficients for bloodOs, we need to do it as 0.4 x patientC + 0.6 x patientD.\nThe nice thing about this approach is that we do not need to worry about any of this, the weights come from our colMeans() call automatically. And now, any contrasts that we make will take these weights into account:\n\n\nCode\n# bloodA vs bloodO (in the control)\nbloodA_control - bloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.5                    -0.4 \n##                patientD          conditiontreat patientB:conditiontreat \n##                    -0.6                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\n\n\nCode\n# interaction\n(bloodA_treat - bloodA_control) - (bloodO_treat - bloodO_control)\n\n\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.0                    -0.1 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.1                     0.0                     0.5 \n## patientC:conditiontreat patientD:conditiontreat \n##                    -0.5                    -0.5\n\nPart of this lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nIn addition, we would like to thank Hugo Tavares from the Bioinformatics Training Facility of the University of Cambridge."
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html#design-formula",
    "href": "develop/rmd/07_extra_contrast_design.html#design-formula",
    "title": "Contrast designs",
    "section": "",
    "text": "A design formula tells the statistical software the known sources of variation to control for, as well as, the factor of interest to test for during differential expression testing. For example, if you know that sex is a significant source of variation in your data, then sex should be included in your model. The design formula should have all of the factors in your metadata that account for major sources of variation in your data. The last factor entered in the formula should be the condition of interest.\nFor example, suppose you have the following metadata:\n\n\n\n\n\n\n\n\n\nIf you want to examine the expression differences between condition, and you know that major sources of variation include bloodtype and patient, then your design formula would be:\ndesign = ~ bloodtype + patient + condition\nThe tilde (~) should always precede your factors and tells DESeq2 to model the counts using the following formula. Note the factors included in the design formula need to match the column names in the metadata.\nIn this tutorial we show a general and flexible way to define contrasts, and is often useful for more complex contrasts or when the design of the experiment is imbalanced (e.g.¬†different number of replicates in each group). Although we focus on DESeq2, the approach can also be used with the other popular package edgeR.\nEach section below covers a particular experimental design, from simpler to more complex ones. The first chunk of code in each section is to simulate data, which has no particular meaning and is only done in order to have a DESeqDataSet object with the right kind of variables for each example. In practice, users can ignore this step as they should have created a DESeqDataSet object from their own data following the instructions in the vignette."
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html#one-factor-two-levels",
    "href": "develop/rmd/07_extra_contrast_design.html#one-factor-two-levels",
    "title": "Contrast designs",
    "section": "",
    "text": "Code\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 6, betaSD = 2)\ndds$condition &lt;- factor(rep(c(\"control\", \"treat\"), each = 3))\n\n\nFirst we can look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 6 rows and 1 column\n##         condition\n##          &lt;factor&gt;\n## sample1   control\n## sample2   control\n## sample3   control\n## sample4   treat  \n## sample5   treat  \n## sample6   treat\nOur factor of interest is condition and so we define our design and run the DESeq model fitting routine:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + condition # or just `~ condition`\ndds &lt;- DESeq(dds) # equivalent to edgeR::glmFit()\n\n\nThen check what coefficients DESeq estimated:\n\n\nCode\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"condition_treat_vs_control\"\nWe can see that we have a coefficient for our intercept and coefficient for the effect of treat (i.e.¬†differences between treat versus control).\nUsing the more standard syntax, we can obtain the results for the effect of treat as such:\n\n\nCode\nres1 &lt;- results(dds, contrast = list(\"condition_treat_vs_control\"))\nres1\n\n\n## log2 fold change (MLE): condition_treat_vs_control effect \n## Wald test p-value: condition_treat_vs_control effect \n## DataFrame with 1000 rows and 6 columns\n##           baseMean log2FoldChange     lfcSE         stat      pvalue\n##          &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;   &lt;numeric&gt;\n## gene1     40.90941    1.267525859  0.574144  2.207679752   0.0272666\n## gene2     12.21876   -0.269917301  1.111127 -0.242922069   0.8080658\n## gene3      1.91439   -3.538133611  2.564464 -1.379677442   0.1676860\n## gene4     10.24472    0.954811627  1.166408  0.818591708   0.4130194\n## gene5     13.16824    0.000656519  0.868780  0.000755679   0.9993971\n## ...            ...            ...       ...          ...         ...\n## gene996   40.43827     -1.0291276  0.554587    -1.855664 0.063501471\n## gene997   52.88360      0.0622133  0.561981     0.110704 0.911851377\n## gene998   73.06582      1.3271896  0.576695     2.301373 0.021370581\n## gene999    8.87701     -5.8385374  1.549471    -3.768084 0.000164506\n## gene1000  37.06533      1.2669314  0.602010     2.104501 0.035334764\n##                 padj\n##            &lt;numeric&gt;\n## gene1      0.0712378\n## gene2      0.8779871\n## gene3      0.2943125\n## gene4      0.5692485\n## gene5      0.9996728\n## ...              ...\n## gene996  0.138827354\n## gene997  0.948279388\n## gene998  0.059599481\n## gene999  0.000914882\n## gene1000 0.087737235\nThe above is a simple way to obtain the results of interest. But it is worth understanding how DESeq is getting to these results by looking at the model‚Äôs matrix. DESeq defines the model matrix using base R functionality:\n\n\nCode\nmodel.matrix(design(dds), colData(dds))\n\n\n##         (Intercept) conditiontreat\n## sample1           1              0\n## sample2           1              0\n## sample3           1              0\n## sample4           1              1\n## sample5           1              1\n## sample6           1              1\n## attr(,\"assign\")\n## [1] 0 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\nWe can see that R coded condition as a dummy variable, with an intercept (common to all samples) and a ‚Äúconditiontreat‚Äù variable, which adds the effect of treat to samples 4-6.\nWe can actually set our contrasts in DESeq2::results() using a numeric vector. The way it works is to define a vector of ‚Äúweights‚Äù for the coefficient(s) we want to test for. In this case, we have (Intercept) and conditiontreat as our coefficients (see model matrix above), and we want to test for the effect of treat, so our contrast vector would be c(0, 1). In other words, we don‚Äôt care about the value of (Intercept) (so it has a weight of 0), and we‚Äôre only interested in the effect of treat (so we give it a weight of 1).\nIn this case the design is very simple, so we could define our contrast vector ‚Äúmanually‚Äù. But for complex designs this can get more difficult to do, so it‚Äôs worth mentioning the general way in which we can define this. For any contrast of interest, we can follow three steps:\n\nGet the model matrix\nSubset the matrix for each group of interest and calculate its column means - this results in a vector of coefficients for each group\nSubtract the group vectors from each other according to the comparison we‚Äôre interested in\n\nLet‚Äôs see this example in action:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##         (Intercept) conditiontreat\n## sample1           1              0\n## sample2           1              0\n## sample3           1              0\n## sample4           1              1\n## sample5           1              1\n## sample6           1              1\n## attr(,\"assign\")\n## [1] 0 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n\n\nCode\n# calculate the vector of coefficient weights in the treat\ntreat &lt;- colMeans(mod_mat[dds$condition == \"treat\", ])\ntreat\n\n\n##    (Intercept) conditiontreat \n##              1              1\n\n\nCode\n# calculate the vector of coefficient weights in the control\ncontrol &lt;- colMeans(mod_mat[dds$condition == \"control\", ])\ncontrol\n\n\n##    (Intercept) conditiontreat \n##              1              0\n\n\nCode\n# The contrast we are interested in is the difference between treat and control\ntreat - control\n\n\n##    (Intercept) conditiontreat \n##              0              1\nThat last step is where we define our contrast vector, and we can give this directly to the results function:\n\n\nCode\n# get the results for this contrast\nres2 &lt;- results(dds, contrast = treat - control)\n\n\nThis gives us exactly the same results as before, which we can check for example by plotting the log-fold-changes between the first and second approach:\n\n\nCode\nplot(res1$log2FoldChange, res2$log2FoldChange)"
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html#recoding-the-design",
    "href": "develop/rmd/07_extra_contrast_design.html#recoding-the-design",
    "title": "Contrast designs",
    "section": "",
    "text": "Often, we can use different model matrices that essentially correspond to the same design. For example, we could recode our design above by removing the intercept:\n\n\nCode\ndesign(dds) &lt;- ~ 0 + condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"conditioncontrol\" \"conditiontreat\"\nIn this case we get a coefficient corresponding to the average expression in control and the average expression in the treat (rather than the difference between treat and control).\nIf we use the same contrast trick as before (using the model matrix), we can see the result is the same:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##         conditioncontrol conditiontreat\n## sample1                1              0\n## sample2                1              0\n## sample3                1              0\n## sample4                0              1\n## sample5                0              1\n## sample6                0              1\n## attr(,\"assign\")\n## [1] 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n\n\nCode\n# calculate weights for coefficients in each condition\ntreat &lt;- colMeans(mod_mat[which(dds$condition == \"treat\"), ])\ncontrol &lt;- colMeans(mod_mat[which(dds$condition == \"control\"), ])\n# get the results for our contrast\nres3 &lt;- results(dds, contrast = treat - control)\n\n\nAgain, the results are essentially the same:\n\n\nCode\nplot(res1$log2FoldChange, res3$log2FoldChange)\n\n\n\n\n\n\n\n\n\n\n\nIn theory there‚Äôs no difference between these two ways of defining our design. The design with an intercept is more common, but for the purposes of understanding what‚Äôs going on, it‚Äôs sometimes easier to look at models without intercept."
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html#one-factor-three-levels",
    "href": "develop/rmd/07_extra_contrast_design.html#one-factor-three-levels",
    "title": "Contrast designs",
    "section": "",
    "text": "Code\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 9, betaSD = 2)\ndds$condition &lt;- NULL\ndds$bloodtype &lt;- factor(rep(c(\"bloodA\", \"bloodB\", \"bloodO\"), each = 3))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\n\n\nFirst we can look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 9 rows and 1 column\n##         bloodtype\n##          &lt;factor&gt;\n## sample1    bloodA\n## sample2    bloodA\n## sample3    bloodA\n## sample4    bloodB\n## sample5    bloodB\n## sample6    bloodB\n## sample7    bloodO\n## sample8    bloodO\n## sample9    bloodO\nAs in the previous example, we only have one factor of interest, bloodtype, and so we define our design and run the DESeq as before:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + bloodtype\ndds &lt;- DESeq(dds)\n# check the coefficients estimated by DEseq\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"bloodtype_bloodA_vs_bloodO\"\n## [3] \"bloodtype_bloodB_vs_bloodO\"\nWe see that now we have 3 coefficients:\n\n‚ÄúIntercept‚Äù corresponds to bloodO bloodtype (our reference level)\n‚Äúbloodtype_bloodA_vs_bloodO‚Äù corresponds to the difference between the reference level and bloodA\n‚Äúbloodtype_bloodB_vs_bloodO‚Äù corresponds to the difference between the reference level and bloodB\n\nWe could obtain the difference between bloodO and any of the two bloodtypes easily:\n\n\nCode\nres1_bloodA_bloodO &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\"))\nres1_bloodB_bloodO &lt;- results(dds, contrast = list(\"bloodtype_bloodB_vs_bloodO\"))\n\n\nFor comparing bloodA vs bloodB, however, we need to compare two coefficients with each other to check whether they are themselves different (check the slide to see the illustration). This is how the standard DESeq syntax would be:\n\n\nCode\nres1_bloodA_bloodB &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\", \n                                                 \"bloodtype_bloodB_vs_bloodO\"))\n\n\nHowever, following our three steps detailed in the first section, we can define our comparisons from the design matrix:\n\n\nCode\n# define the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##         (Intercept) bloodtypebloodA bloodtypebloodB\n## sample1           1               1               0\n## sample2           1               1               0\n## sample3           1               1               0\n## sample4           1               0               1\n## sample5           1               0               1\n## sample6           1               0               1\n## sample7           1               0               0\n## sample8           1               0               0\n## sample9           1               0               0\n## attr(,\"assign\")\n## [1] 0 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$bloodtype\n## [1] \"contr.treatment\"\n\n\nCode\n# calculate coefficient vectors for each group\nbloodA &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\", ])\nbloodB &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodB\", ])\nbloodO &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\", ])\n\n\nAnd we can now define any contrasts we want:\n\n\nCode\n# obtain results for each pairwise contrast\nres2_bloodA_bloodO &lt;- results(dds, contrast = bloodA - bloodO)\nres2_bloodB_bloodO &lt;- results(dds, contrast = bloodB - bloodO)\nres2_bloodA_bloodB &lt;- results(dds, contrast = bloodA - bloodB)\n# plot the results from the two approaches to check that they are identical\nplot(res1_bloodA_bloodO$log2FoldChange, res2_bloodA_bloodO$log2FoldChange)\nplot(res1_bloodB_bloodO$log2FoldChange, res2_bloodB_bloodO$log2FoldChange)\nplot(res1_bloodA_bloodB$log2FoldChange, res2_bloodA_bloodB$log2FoldChange)\n\n\n\n\nWith this approach, we could even define a more unusual contrast, for example to find genes that differ between A and B against and O samples:\n\n\nCode\n# define vector of coefficients for A_B samples\nA_B &lt;- colMeans(mod_mat[dds$bloodtype %in% c(\"bloodA\", \"bloodB\"),])\n# Our contrast of interest is\nA_B - bloodO\n\n\n##     (Intercept) bloodtypebloodA bloodtypebloodB \n##             0.0             0.5             0.5\nNotice the contrast vector in this case assigns a ‚Äúweight‚Äù of 0.5 to each of bloodtypebloodA and bloodtypebloodB. This is equivalent to saying that we want to consider the average of bloodA and bloodB expression. In fact, we could have also defined our contrast vector like this:\n\n\nCode\n# average of bloodA and bloodB minus bloodO\n(bloodA + bloodB)/2 - bloodO\n\n\n##     (Intercept) bloodtypebloodA bloodtypebloodB \n##             0.0             0.5             0.5\nTo obtain our results, we use the results() function as before:\n\n\nCode\n# get the results between A_B and bloodA\nres2_AB &lt;- results(dds, contrast = A_B - bloodO)\n\n\n\n\n\nFor this last example (A_B vs bloodO), we may have considered creating a new variable in our column data:\n\n\nCode\ndds$A_B &lt;- factor(dds$bloodtype %in% c(\"bloodA\", \"bloodB\"))\ncolData(dds)\n\n\n## DataFrame with 9 rows and 3 columns\n##         bloodtype sizeFactor      A_B\n##          &lt;factor&gt;  &lt;numeric&gt; &lt;factor&gt;\n## sample1    bloodA   0.972928    TRUE \n## sample2    bloodA   0.985088    TRUE \n## sample3    bloodA   0.960749    TRUE \n## sample4    bloodB   0.916582    TRUE \n## sample5    bloodB   0.936918    TRUE \n## sample6    bloodB   1.137368    TRUE \n## sample7    bloodO   1.071972    FALSE\n## sample8    bloodO   1.141490    FALSE\n## sample9    bloodO   1.140135    FALSE\nand then re-run DESeq with a new design:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + A_B\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"         \"A_B_TRUE_vs_FALSE\"\n\n\nCode\nres1_A_B &lt;- results(dds, contrast = list(\"A_B_TRUE_vs_FALSE\"))\n\n\nHowever, in this model the gene dispersion is estimated together for bloodA and bloodB samples as if they were replicates of each other, which may result in inflated/deflated estimates. Instead, our approach above estimates the error within each of those groups.\nTo check the difference one could compare the two approaches visually:\n\n\nCode\n# compare the log-fold-changes between the two approaches\nplot(res1_A_B$log2FoldChange, res2_AB$log2FoldChange)\nabline(0, 1, col = \"brown\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# compare the errors between the two approaches\nplot(res1_A_B$lfcSE, res2_AB$lfcSE)\nabline(0, 1, col = \"brown\", lwd = 2)"
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html#two-factors-with-interaction",
    "href": "develop/rmd/07_extra_contrast_design.html#two-factors-with-interaction",
    "title": "Contrast designs",
    "section": "",
    "text": "Code\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 12, betaSD = 2)\ndds$bloodtype &lt;- factor(rep(c(\"bloodO\", \"bloodA\"), each = 6))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\ndds$condition &lt;- factor(rep(c(\"treat\", \"control\"), 6))\ndds &lt;- dds[, order(dds$bloodtype, dds$condition)]\ncolnames(dds) &lt;- paste0(\"sample\", 1:ncol(dds))\n\n\nFirst let‚Äôs look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 12 rows and 2 columns\n##          condition bloodtype\n##           &lt;factor&gt;  &lt;factor&gt;\n## sample1    control    bloodO\n## sample2    control    bloodO\n## sample3    control    bloodO\n## sample4    treat      bloodO\n## sample5    treat      bloodO\n## ...            ...       ...\n## sample8    control    bloodA\n## sample9    control    bloodA\n## sample10   treat      bloodA\n## sample11   treat      bloodA\n## sample12   treat      bloodA\nThis time we have two factors of interest, and we want to model both with an interaction (i.e.¬†we assume that bloodA and bloodO samples may respond differently to treat/control). We define our design accordingly and fit the model:\n\n\nCode\ndesign(dds) &lt;- ~ 1 + bloodtype + condition + bloodtype:condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                      \"bloodtype_bloodA_vs_bloodO\"    \n## [3] \"condition_treat_vs_control\"     \"bloodtypebloodA.conditiontreat\"\nBecause we have two factors and an interaction, the number of comparisons we can do is larger. Using our three-step approach from the model matrix, we do things exactly as we‚Äôve been doing so far:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\n# Define coefficient vectors for each condition\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\n\n\nWe are now ready to define any contrast of interest from these vectors (for completeness we show the equivalent syntax using the coefficient‚Äôs names from DESeq).\nbloodA vs bloodO (in the control):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodA_control - bloodO_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\"))\n\n\nbloodA vs bloodO (in the treatment):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodO_treat - bloodA_treat)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"bloodtype_bloodA_vs_bloodO\",\n                                       \"bloodtypebloodA.conditiontreat\")))\n\n\ntreat vs control (for bloodtypes O):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodO_treat - bloodO_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"condition_treat_vs_control\")))\n\n\ntreat vs control (for bloodtypes A):\n\n\nCode\nres1 &lt;- results(dds, contrast = bloodA_treat - bloodA_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"condition_treat_vs_control\", \n                                       \"bloodtypebloodA.conditiontreat\")))\n\n\nInteraction between bloodtype and condition\nI.e. do bloodAs and bloodOs respond differently to the treatment?\n\n\nCode\nres1 &lt;- results(dds, \n                contrast = (bloodA_treat - bloodA_control) - (bloodO_treat - bloodO_control))\n# or equivalently\nres2 &lt;- results(dds, contrast = list(\"bloodtypebloodA.conditiontreat\"))\n\n\nIn conclusion, although we can define these contrasts using DESeq coefficient names, it is somewhat more explicit (and perhaps intuitive?) what it is we‚Äôre comparing using matrix-based contrasts."
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html#three-factors-with-nesting",
    "href": "develop/rmd/07_extra_contrast_design.html#three-factors-with-nesting",
    "title": "Contrast designs",
    "section": "",
    "text": "Code\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 24, betaSD = 2)\ndds$bloodtype &lt;- factor(rep(c(\"bloodA\", \"bloodO\"), each = 12))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\ndds$patient &lt;- factor(rep(LETTERS[1:4], each = 6))\ndds$condition &lt;- factor(rep(c(\"treat\", \"control\"), 12))\ndds &lt;- dds[, order(dds$bloodtype, dds$patient, dds$condition)]\ncolnames(dds) &lt;- paste0(\"sample\", 1:ncol(dds))\n\n\nFirst let‚Äôs look at our sample information:\n\n\nCode\ncolData(dds)\n\n\n## DataFrame with 24 rows and 3 columns\n##          condition bloodtype  patient\n##           &lt;factor&gt;  &lt;factor&gt; &lt;factor&gt;\n## sample1    control    bloodO        C\n## sample2    control    bloodO        C\n## sample3    control    bloodO        C\n## sample4    treat      bloodO        C\n## sample5    treat      bloodO        C\n## ...            ...       ...      ...\n## sample20   control    bloodA        B\n## sample21   control    bloodA        B\n## sample22   treat      bloodA        B\n## sample23   treat      bloodA        B\n## sample24   treat      bloodA        B\nNow we have three factors, but patient is nested within bloodtype (i.e.¬†a patient is either bloodA or bloodO, it cannot be both). Therefore, bloodtype is a linear combination with patient (or, another way to think about it is that bloodtype is redundant with patient). Because of this, we will define our design without including ‚Äúbloodtype‚Äù, although later we can compare groups of patient of the same bloodtype with each other.\n\n\nCode\ndesign(dds) &lt;- ~ 1 + patient + condition + patient:condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"patient_B_vs_A\"            \n## [3] \"patient_C_vs_A\"             \"patient_D_vs_A\"            \n## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\"   \n## [7] \"patientC.conditiontreat\"    \"patientD.conditiontreat\"\nNow it‚Äôs harder to define contrasts between groups of patient of the same bloodtype using DESeq‚Äôs coefficient names (although still possible). But using the model matrix approach, we do it in exactly the same way we have done so far!\nAgain, let‚Äôs define our groups from the model matrix:\n\n\nCode\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\n# define coefficient vectors for each group\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\n\n\nIt‚Äôs worth looking at some of these vectors, to see that they are composed of weighted coefficients from different patient. For example, for ‚ÄúbloodO‚Äù patient, we have equal contribution from ‚ÄúpatientC‚Äù and ‚ÄúpatientD‚Äù:\n\n\nCode\nbloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     1.0                     0.0                     0.5 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.5                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\nAnd so, when we define our contrasts, each patient will be correctly weighted:\n\n\nCode\nbloodO_treat - bloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.0                     0.0 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.0                     1.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.5                     0.5\nWe can set our contrasts in exactly the same way as we did in the previous section (for completeness, we also give the contrasts using DESeq‚Äôs named coefficients).\nbloodA vs bloodO (in the control):\n\n\nCode\nres1_bloodA_bloodO_control &lt;- results(dds, contrast = bloodA_control - bloodO_control)\n# or equivalently\nres2_bloodA_bloodO_control &lt;- results(dds, \n                                 contrast = list(c(\"patient_B_vs_A\"), # Blood type A\n                                                 c(\"patient_C_vs_A\", # Blood type O\n                                                   \"patient_D_vs_A\"))) # Blood type O\n\n\nbloodA vs bloodO (in the treat):\n\n\nCode\nres1_bloodO_bloodA_treat &lt;- results(dds, contrast = bloodO_treat - bloodA_treat)\n# or equivalently\nres2_bloodO_bloodA_treat &lt;- results(dds, \n                           contrast = list(c(\"patient_B_vs_A\", # Blood type A\n                                             \"patientB.conditiontreat\"), # Interaction of patient B with treatment\n                                           c(\"patient_C_vs_A\", # Blood type O\n                                             \"patient_D_vs_A\", # Blood type O\n                                             \"patientC.conditiontreat\", # Interaction of patient C with treatment\n                                             \"patientD.conditiontreat\"))) # Interaction of patient B with treatment\n\n\nAnd so on, for other contrasts of interest‚Ä¶"
  },
  {
    "objectID": "develop/rmd/07_extra_contrast_design.html#extra-imbalanced-design",
    "href": "develop/rmd/07_extra_contrast_design.html#extra-imbalanced-design",
    "title": "Contrast designs",
    "section": "",
    "text": "Let‚Äôs take our previous example, but drop one of the samples from the data, so that we only have 2 replicates for it.\n\n\nCode\ndds &lt;- dds[, -1] # drop one of the patient C samples\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n\n\n## [1] \"Intercept\"                  \"patient_B_vs_A\"            \n## [3] \"patient_C_vs_A\"             \"patient_D_vs_A\"            \n## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\"   \n## [7] \"patientC.conditiontreat\"    \"patientD.conditiontreat\"\nDefine our model matrix and coefficient vectors:\n\n\nCode\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n\n\n##          (Intercept) patientB patientC patientD conditiontreat\n## sample2            1        0        1        0              0\n## sample3            1        0        1        0              0\n## sample4            1        0        1        0              1\n## sample5            1        0        1        0              1\n## sample6            1        0        1        0              1\n## sample7            1        0        0        1              0\n## sample8            1        0        0        1              0\n## sample9            1        0        0        1              0\n## sample10           1        0        0        1              1\n## sample11           1        0        0        1              1\n## sample12           1        0        0        1              1\n## sample13           1        0        0        0              0\n## sample14           1        0        0        0              0\n## sample15           1        0        0        0              0\n## sample16           1        0        0        0              1\n## sample17           1        0        0        0              1\n## sample18           1        0        0        0              1\n## sample19           1        1        0        0              0\n## sample20           1        1        0        0              0\n## sample21           1        1        0        0              0\n## sample22           1        1        0        0              1\n## sample23           1        1        0        0              1\n## sample24           1        1        0        0              1\n##          patientB:conditiontreat patientC:conditiontreat\n## sample2                        0                       0\n## sample3                        0                       0\n## sample4                        0                       1\n## sample5                        0                       1\n## sample6                        0                       1\n## sample7                        0                       0\n## sample8                        0                       0\n## sample9                        0                       0\n## sample10                       0                       0\n## sample11                       0                       0\n## sample12                       0                       0\n## sample13                       0                       0\n## sample14                       0                       0\n## sample15                       0                       0\n## sample16                       0                       0\n## sample17                       0                       0\n## sample18                       0                       0\n## sample19                       0                       0\n## sample20                       0                       0\n## sample21                       0                       0\n## sample22                       1                       0\n## sample23                       1                       0\n## sample24                       1                       0\n##          patientD:conditiontreat\n## sample2                        0\n## sample3                        0\n## sample4                        0\n## sample5                        0\n## sample6                        0\n## sample7                        0\n## sample8                        0\n## sample9                        0\n## sample10                       1\n## sample11                       1\n## sample12                       1\n## sample13                       0\n## sample14                       0\n## sample15                       0\n## sample16                       0\n## sample17                       0\n## sample18                       0\n## sample19                       0\n## sample20                       0\n## sample21                       0\n## sample22                       0\n## sample23                       0\n## sample24                       0\n## attr(,\"assign\")\n## [1] 0 1 1 1 2 3 3 3\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$patient\n## [1] \"contr.treatment\"\n## \n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n\n\nCode\n# define coefficient vectors for each group\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\n\n\nNow let‚Äôs check what happens to the bloodO_control group:\n\n\nCode\nbloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     1.0                     0.0                     0.4 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.6                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\nNotice that whereas before ‚ÄúpatientC‚Äù and ‚ÄúpatientD‚Äù had each a weight of 0.5, now they have different weights. That‚Äôs because for patientC there‚Äôs only 2 replicates. So, we have a total of 5 bloodtype O individuals in the control (2 from patient C and 3 from D). Therefore, when we calculate the average coefficients for bloodOs, we need to do it as 0.4 x patientC + 0.6 x patientD.\nThe nice thing about this approach is that we do not need to worry about any of this, the weights come from our colMeans() call automatically. And now, any contrasts that we make will take these weights into account:\n\n\nCode\n# bloodA vs bloodO (in the control)\nbloodA_control - bloodO_control\n\n\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.5                    -0.4 \n##                patientD          conditiontreat patientB:conditiontreat \n##                    -0.6                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\n\n\nCode\n# interaction\n(bloodA_treat - bloodA_control) - (bloodO_treat - bloodO_control)\n\n\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.0                    -0.1 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.1                     0.0                     0.5 \n## patientC:conditiontreat patientD:conditiontreat \n##                    -0.5                    -0.5\n\nPart of this lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nIn addition, we would like to thank Hugo Tavares from the Bioinformatics Training Facility of the University of Cambridge."
  },
  {
    "objectID": "develop/index.html",
    "href": "develop/index.html",
    "title": "Welcome to the bulk RNA-seq analysis workshop",
    "section": "",
    "text": "Welcome to the bulk RNA-seq analysis workshop\n\n\nThis workshop material includes a tutorial on how to approach RNAseq data, starting from your sequencing reads (fastq files). Thus, the workshop only briefly touches upon laboratory protocols, library preparation, and experimental design of RNA sequencing experiments, mainly for the purpose of outlining considerations in the downstream bioinformatic analysis. This workshop is based on the materials developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC), a collection of modified tutorials from the DESeq2, R language vignettes and the nf-core rnaseq pipeline.\n\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\n\nCourse introduction\n\nExperimental planning\n\nData explanation\n\nRead reprocessing and preprocessing pipelines\n\nAnalysing RNAseq data\n\nRNAseq counts\n\nExploratory analysis\n\nDifferential Expression Analysis\n\nFunctional analysis\n\nSummarized workflow\n\n\n‚è∞ Total Time Estimation: 8 hours\nüìÅ Supporting Materials: Workshop slides with theory on bulk RNAseq can be found in this zenodo repository.\nüë®‚Äçüíª Target Audience: Ph.D., MSc, etc.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nKnowledge of R, Rstudio and Rmarkdown. It is recommended that you have at least followed our workshop R basics\nBasic knowledge of RNAseq technology\nBasic knowledge of data science and statistics such as PCA, clustering and statistical testing\n\n\n\nThis workshop material includes a tutorial on how to approach RNAseq data, starting from your sequencing reads (fastq files). Thus, the workshop only briefly touches upon laboratory protocols, library preparation, and experimental design of RNA sequencing experiments, mainly for the purpose of outlining considerations in the downstream bioinformatic analysis. This workshop is based on the materials developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC), a collection of modified tutorials from the DESeq2, R language vignettes and the nf-core rnaseq pipeline.\nThe aim of this repository is to run a comprehensive but introductory workshop on bulk-RNAseq bioinformatic analyses. Each of the modules of this workshop is accompanied by a powerpoint slideshow explaining the steps and the theory behind a typical bioinformatics analysis (ideally with a teacher). Many of the slides are annotated with extra information and/or point to original sources for extra reading material.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to analyse your own bulk RNAseq data:\n\nPreprocess your reads into a count matrix.\nNormalize your data.\nExplore your samples with PCAs and heatmaps.\nPerform Differential Expression Analysis.\nAnnotate your results.\n\n\n\n\nAcknowledgements\nWe recognize the substantial contribution of Jos√© Alejandro Romero Herrera, a former team member, in developing the course material. Other members that have contributed to the development of this course:\n\n\n\n\n\n\n\n\n\nMember\nRole\nInstitution\nPI\n\n\n\n\nJennifer Bartell\nProject Manager, Data Scientist\nCenter for Health Data Science, KU\nAnders Krogh\n\n\nDiana Andrejeva\nData Scientist\nCenter for Health Data Science, KU\nAnders Krogh\n\n\nSamuele Soraggi\nData Scientist\nBioinformatics Research Centre, AU\nMikkel Schierup\n\n\n\nWe would also like to extend our gratitude to:\n\nCenter for Health Data Science, University of Copenhagen.\nHugo Tavares, Bioinformatics Training Facility, University of Cambridge.\nSilvia Raineri, Center for Stem Cell Medicine (reNew), University of Copenhagen.\nHarvard Chan Bioinformatics Core (HBC), check out their github repo\nnf-core community\n\n\n\nCourse Instructors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdrija Kalvisa\n\n\nData Scientist\n\n\n\n\n\n\n\n\n\n\n\nAlba Refoyo Martinez\n\n\nData Scientist\n\n\n\n\n\n\n\n\n\n\n\nHenrike Zschach\n\n\nData Scientist\n\n\n\n\n\n\n\n\n\n\n\nThilde Terkelsen\n\n\nData Scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "develop/04b_pipelines.html",
    "href": "develop/04b_pipelines.html",
    "title": "Nextflow & nf-core pipelines",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 40 minutes\nüí¨ Learning Objectives:\n\nUnderstand what is a pipeline/workflow and the Nextflow language.\nLearn about existing automated workflows from the bioinformatics community.\nLearn how to use the nf-core pipeline for bulk RNAseq analysis.\nAutomating your workflow: Nextflow and nf-core pipelines\nA pipeline consists of a chain of processing elements (processes, threads, coroutines, functions, etc.), arranged so that the output of each element is the input of the next; the name is by analogy to a physical pipeline. Narrowly speaking, a pipeline is linear and one-directional, though sometimes the term is applied to more general flows. For example, a primarily one-directional pipeline may have some communication in the other direction, known as a return channel or backchannel, or a pipeline may be fully bi-directional. Flows with one-directional tree and directed acyclic graph topologies behave similarly to (linear) pipelines‚Äì the lack of cycles makes them simple ‚Äì and thus may be loosely referred to as ‚Äúpipelines‚Äù.\nIn our case, a ‚Äúpreprocessing‚Äù pipeline consists on concatenating all the processes explained in the previous lesson so that we have one continuous workflow from raw sequencing reads to a count matrix. For example, the RNASeq pipeline developed by the nf-core community (see below).\nAs you can see in the image above, each process, such as read trimming, QC, alignment, etc, are connected to each other. A workflow created in this way is ideal to reproduce analysis and makes the task of analysing RNAseq data much much easier.",
    "crumbs": [
      "Data processing",
      "Nextflow & nf-core pipelines"
    ]
  },
  {
    "objectID": "develop/04b_pipelines.html#nextflow-pipelines",
    "href": "develop/04b_pipelines.html#nextflow-pipelines",
    "title": "Nextflow & nf-core pipelines",
    "section": "Nextflow pipelines",
    "text": "Nextflow pipelines\n\nNextflow is a bioinformatics workflow manager that enables the development of portable and reproducible workflows. It supports deploying workflows on a variety of execution platforms including local, HPC schedulers, AWS Batch, Google Cloud Life Sciences, and Kubernetes. Additionally, it provides support for manage your workflow dependencies through built-in support for Conda, Spack, Docker, Podman, Singularity, Modules, and more.\nWith the rise of big data, techniques to analyse and run experiments on large datasets are increasingly necessary. Parallelization and distributed computing are the best ways to tackle this problem, but the tools commonly available to the bioinformatics community often lack good support for these techniques, or provide a model that fits badly with the specific requirements in the bioinformatics domain and, most of the time, require the knowledge of complex tools or low-level APIs.\nNextflow framework is based on the dataflow programming model, which greatly simplifies writing parallel and distributed pipelines without adding unnecessary complexity and letting you concentrate on the flow of data, i.e.¬†the functional logic of the application/algorithm.\nIt doesn‚Äôt aim to be another pipeline scripting language yet, but it is built around the idea that the Linux platform is the lingua franca of data science, since it provides many simple command line and scripting tools, which by themselves are powerful, but when chained together facilitate complex data manipulations.\nIn practice, this means that a Nextflow script is defined by composing many different processes. Each process can execute a given bioinformatics tool or scripting language, to which is added the ability to coordinate and synchronize the processes execution by simply specifying their inputs and outputs.\n\nFeatures\n\nFast prototyping: Nextflow allows you to write a computational pipeline by making it simpler to put together many different tasks. You may reuse your existing scripts and tools and you don‚Äôt need to learn a new language or API to start using it.\nReproducibility: Nextflow supports Docker and Singularity containers technology. This, along with the integration of the GitHub code sharing platform, allows you to write self-contained pipelines, manage versions and to rapidly reproduce any former configuration.\nPortable: Nextflow provides an abstraction layer between your pipeline‚Äôs logic and the execution layer, so that it can be executed on multiple platforms without it changing. It provides out of the box executors for GridEngine, SLURM, LSF, PBS, Moab and HTCondor batch schedulers and for Kubernetes, Amazon AWS, Google Cloud and Microsoft Azure platforms.\nUnified parallelism: Nextflow is based on the dataflow programming model which greatly simplifies writing complex distributed pipelines. Parallelisation is implicitly defined by the processes input and output declarations. The resulting applications are inherently parallel and can scale-up or scale-out, transparently, without having to adapt to a specific platform architecture.\nContinuous checkpoints: All the intermediate results produced during the pipeline execution are automatically tracked. This allows you to resume its execution, from the last successfully executed step, no matter what the reason was for it stopping.",
    "crumbs": [
      "Data processing",
      "Nextflow & nf-core pipelines"
    ]
  },
  {
    "objectID": "develop/04b_pipelines.html#the-nf-core-project",
    "href": "develop/04b_pipelines.html#the-nf-core-project",
    "title": "Nextflow & nf-core pipelines",
    "section": "The nf-core project",
    "text": "The nf-core project\n\nThe nf-core project is a community effort to collect a curated set of analysis pipelines built using Nextflow, an incredibly powerful and flexible workflow language. This means that all the tools and steps used in your RNAseq workflow can be automated and easily reproduced by other researchers if necessary. In addition, if you use any of the nf-core pipelines, you will be sure that all the necessary tools are available to you in any computer platform (Cloud computing, HPC or your personal computer).\n\n\n\nnf-core/rnaseq pipeline metro schematic.\n\n\nThe RNAseq pipeline enables using many different tools, such as STAR, RSEM, HISAT2 or Salmon, and allows quantification of gene/isoform counts and provides extensive quality control checks at each step of the workflow. We encourage your to take a look at the pipeline and its documentation if you need to preprocess your RNAseq reads from stratch, as well as checkout this introductory video.",
    "crumbs": [
      "Data processing",
      "Nextflow & nf-core pipelines"
    ]
  },
  {
    "objectID": "develop/04b_pipelines.html#nf-corernaseq-usage-for-version-3.11.2",
    "href": "develop/04b_pipelines.html#nf-corernaseq-usage-for-version-3.11.2",
    "title": "Nextflow & nf-core pipelines",
    "section": "nf-core/rnaseq: Usage for version 3.11.2",
    "text": "nf-core/rnaseq: Usage for version 3.11.2\nIn this section, we will see some of the most important arguments that the pipeline uses to run a RNAseq preprocessing workflow. There are many more options for advanced users and we really encourage you to check them out at the official webpage. Below you will find the arguments we will use for our own data.\n\nRunning the pipeline\nThe typical command for running the pipeline is as follows:\nnextflow run nf-core/rnaseq --input samplesheet.csv --outdir &lt;OUTDIR&gt; --genome GRCh37 -profile docker\nThis will launch the pipeline with the docker configuration profile. See below for more information about profiles.\nNote that the pipeline will create the following files in your working directory:\nwork            # Directory containing the nextflow working files\nresults         # Finished results (configurable, see below)\n.nextflow_log   # Log file from Nextflow\n\n# Other nextflow hidden files, eg. history of pipeline runs and old logs.\n\n\n\n\n\n\nNote\n\n\n\n\nOptions with a single hyphen are part of Nextflo, i.e.¬†-profile.\nPipeline specific parameters use a double-hyphen, i.e.¬†--input.\n\n\n\n\n\nCore Nextflow arguments\n\n-work-dir\nUse this parameter to choose a path where the work folder, which containing all the intermediary files from the pipeline, should be saved.\n\n\n-profile\nUse this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.\nSeveral generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Conda) - see below. When using Biocontainers, most of these software packaging methods pull Docker containers from quay.io e.g FastQC except for Singularity which directly downloads Singularity images via https hosted by the Galaxy project and Conda which downloads and installs software locally from Bioconda.\n\n\n\n\n\n\nTip\n\n\n\nIt is highly recommended to use Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.\n\n\nThe pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to see if your system is available in these configs please see the nf-core/configs documentation.\nNote that multiple profiles can be loaded, for example: -profile test,docker - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.\nIf -profile is not specified, the pipeline will run locally and expect all software to be installed and available on the PATH. This is not recommended.\n\ndocker\n\nA generic configuration profile to be used with Docker\n\nsingularity\n\nA generic configuration profile to be used with Singularity\n\npodman\n\nA generic configuration profile to be used with Podman\n\nshifter\n\nA generic configuration profile to be used with Shifter\n\ncharliecloud\n\nA generic configuration profile to be used with Charliecloud\n\nconda\n\nA generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e.¬†when it‚Äôs not possible to run the pipeline with Docker, Singularity, Podman, Shifter or Charliecloud.\n\ntest\n\nA profile with a complete configuration for automated testing\nIncludes links to test data so needs no other parameters\n\n\n\n\n-name\nUse this parameter to give a unique name to the run. This name cannot be used again for another run in the same folder. This is very useful to track different runs since otherwise Nextflow will assign a random unique name to the run.\n\n\n-resume\nSpecify this when restarting a pipeline. Nextflow will used cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. You can also supply a run name to resume a specific run: -resume [run-name]. Use the nextflow log command to show previous run names.\n\n\n-r\nIt is a good idea to specify a pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you‚Äôll be running the same version of the pipeline, even if there have been changes to the code since.\nFirst, go to the nf-core/rnaseq releases page and find the latest version number (numeric only), e.g., 1.3.1. Then specify this when running the pipeline with -r (one hyphen), e.g., -r 1.3.1.\nThis version number will be logged in reports when you run the pipeline, so that you‚Äôll know what you used when you look back in the future.\n\n\n\nRNAseq pipeline arguments\n\nSamplesheet input\nYou will need to create a samplesheet with information about the samples you would like to analyse before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 4 columns, and a header row as shown in the examples below.\n--input '[path to samplesheet file]'\nMultiple runs of the same sample\nThe sample identifiers have to be the same when you have re-sequenced the same sample more than once e.g.¬†to increase sequencing depth. The pipeline will concatenate the raw reads before performing any downstream analysis. Below is an example for the same sample sequenced across 3 lanes:\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,unstranded\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,unstranded\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,unstranded\nFull samplesheet\nThe pipeline will auto-detect whether a sample is single- or paired-end using the information provided in the samplesheet. The samplesheet can have as many columns as you desire, however, there is a strict requirement for the first 4 columns to match those defined in the table below.\nA final samplesheet file consisting of both single- and paired-end data may look something like the one below. This is for 6 samples, where TREATMENT_REP3 has been sequenced twice.\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,forward\nCONTROL_REP2,AEG588A2_S2_L002_R1_001.fastq.gz,AEG588A2_S2_L002_R2_001.fastq.gz,forward\nCONTROL_REP3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz,forward\nTREATMENT_REP1,AEG588A4_S4_L003_R1_001.fastq.gz,,reverse\nTREATMENT_REP2,AEG588A5_S5_L003_R1_001.fastq.gz,,reverse\nTREATMENT_REP3,AEG588A6_S6_L003_R1_001.fastq.gz,,reverse\nTREATMENT_REP3,AEG588A6_S6_L004_R1_001.fastq.gz,,reverse\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nsample\nCustom sample name. This entry will be identical for multiple sequencing libraries/runs from the same sample. Spaces in sample names are automatically converted to underscores (_).\n\n\nfastq_1\nFull path to FastQ file for Illumina short reads 1. File has to be gzipped and have the extension ‚Äú.fastq.gz‚Äù or ‚Äú.fq.gz‚Äù.\n\n\nfastq_2\nFull path to FastQ file for Illumina short reads 2. File has to be gzipped and have the extension ‚Äú.fastq.gz‚Äù or ‚Äú.fq.gz‚Äù.\n\n\nstrandedness\nSample strand-specificity. Must be one of unstranded, forward or reverse.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe group and replicate columns were replaced with a single sample column as of v3.1 of the pipeline. The sample column is essentially a concatenation of the group and replicate columns, however it now also offers more flexibility in instances where replicate information is not required e.g.¬†when sequencing clinical samples. If all values of sample have the same number of underscores, fields defined by these underscore-separated names may be used in the PCA plots produced by the pipeline, to regain the ability to represent different groupings.\n\n\n\n\nResults folder\nThe output directory where the results of the pipeline will be saved.\n--outdir '[path to output]'\n\n\nAlignment options\nBy default, the pipeline uses STAR (i.e.¬†--aligner star_salmon) to map the raw FastQ reads to the reference genome, project the alignments onto the transcriptome and to perform the downstream BAM-level quantification with Salmon. STAR is fast but requires a lot of memory to run, typically around 38GB for the Human GRCh37 reference genome. Since the RSEM (i.e.¬†--aligner star_rsem) workflow in the pipeline also uses STAR you should use the HISAT2 aligner (i.e.¬†--aligner hisat2) if you have memory limitations.\nYou also have the option to pseudo-align and quantify your data with Salmon by providing the --pseudo_aligner salmon parameter. Salmon will then be run in addition to the standard alignment workflow defined by --aligner, mainly because it allows you to obtain QC metrics with respect to the genomic alignments. However, you can provide the --skip_alignment parameter if you would like to run Salmon in isolation.\nTwo additional parameters --extra_star_align_args and --extra_salmon_quant_args were added in v3.10 of the pipeline that allow you to append any custom parameters to the STAR align and Salmon quant commands, respectively. Note, the --seqBias and --gcBias are not provided to Salmon quant by default so you can provide these via --extra_salmon_quant_args '--seqBias --gcBias' if required.\n\n\nReference genome files\nThe minimum reference genome requirements are a FASTA and GTF file, all other files required to run the pipeline can be generated from these files. However, it is more storage and compute friendly if you are able to re-use reference genome files as efficiently as possible. It is recommended to use the --save_reference parameter if you are using the pipeline to build new indices (e.g.¬†those unavailable on AWS iGenomes) so that you can save them somewhere locally. The index building step can be quite a time-consuming process and it permits their reuse for future runs of the pipeline to save disk space. You can then either provide the appropriate reference genome files on the command-line via the appropriate parameters (e.g.¬†--star_index '/path/to/STAR/index/') or via a custom config file.\n\nIf --genome is provided then the FASTA and GTF files (and existing indices) will be automatically obtained from AWS-iGenomes unless these have already been downloaded locally in the path specified by --igenomes_base.\nIf --gff is provided as input then this will be converted to a GTF file, or the latter will be used if both are provided.\nIf --gene_bed is not provided then it will be generated from the GTF file.\nIf --additional_fasta is provided then the features in this file (e.g.¬†ERCC spike-ins) will be automatically concatenated onto both the reference FASTA file as well as the GTF annotation before building the appropriate indices.\n\nWhen using --aligner star_rsem, both the STAR and RSEM indices should be present in the path specified by --rsem_index (see #568)\n\n\n\n\n\n\nNote\n\n\n\nCompressed reference files are also supported by the pipeline i.e.¬†standard files with the .gz extension and indices folders with the tar.gz extension.\n\n\n\n\nProcess skipping options\nThere are several options to skip various steps within the workflow.\n\n--skip_bigwig: Skip bigWig file creation\n--skip_stringtie: Skip StringTie.\n--skip_fastqc: Skip FastQC.\n--skip_preseq: Skip Preseq.\n--skip_dupradar: Skip dupRadar.\n--skip_qualimap: Skip Qualimap.\n--skip_rseqc: Skip RSeQC.\n--skip_biotype_qc: Skip additional featureCounts process for biotype QC.\n--skip_deseq2_qc: Skip DESeq2 PCA and heatmap plotting.\n--skip_multiqc: Skip MultiQC.\n--skip_qc: Skip all QC steps except for MultiQC.\n\n\n\n\nUnderstanding the results folder\nThe pipeline will save everything inside the --outdir folder. Inside it you will find different results. In this section we will go through the most relevant results for this workshop. If you are interested in the full documentation, visit the nf-core rnaseq output docs.\n\n1. pipeline_info\nFirst, we will check the pipeline_info folder. Nextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.\npipeline_info/\n\nReports generated by Nextflow: execution_report.html, execution_timeline.html, execution_trace.txt and pipeline_dag.dot/pipeline_dag.svg.\nReports generated by the pipeline: pipeline_report.html, pipeline_report.txt and software_versions.yml. The pipeline_report* files will only be present if the --email / --email_on_fail parameter‚Äôs are used when running the pipeline.\nReformatted samplesheet files used as input to the pipeline: samplesheet.valid.csv.\n\n\n\n2. genome\nA number of genome-specific files are generated by the pipeline because they are required for the downstream processing of the results. If the --save_reference parameter is provided then these will be saved in the genome/ directory. It is recommended to use the --save_reference parameter if you are using the pipeline to build new indices so that you can save them somewhere locally. The index building step can be quite a time-consuming process and it permits their reuse for future runs of the pipeline to save disk space.\n\n\n\n\n\n\nWarning\n\n\n\nIf you have not selected --save_reference, you will instead get a README.txt file containing information about the reference that was used for the run.\n\n\ngenome/\n\n*.fa, *.gtf, *.gff, *.bed, .tsv: If the --save_reference parameter is provided then all of the genome reference files will be placed in this directory\n\ngenome/index/\n\nstar/: Directory containing STAR indices.\nhisat2/: Directory containing HISAT2 indices.\nrsem/: Directory containing STAR and RSEM indices.\nsalmon/: Directory containing Salmon indices.\n\n\n\n3. multiqc\nResults generated by MultiQC collate pipeline QC from supported tools i.e.¬†FastQC, Cutadapt, SortMeRNA, STAR, RSEM, HISAT2, Salmon, SAMtools, Picard, RSeQC, Qualimap, Preseq and featureCounts. Additionally, various custom content has been added to the report to assess the output of dupRadar, DESeq2 and featureCounts biotypes, and to highlight samples failing a mimimum mapping threshold or those that failed to match the strand-specificity provided in the input samplesheet. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability.\nmultiqc/\n\nmultiqc_report.html: a standalone HTML file that can be viewed in your web browser.\nmultiqc_data/: directory containing parsed statistics from the different tools used in the pipeline.\nmultiqc_plots/: directory containing individual plots from the different tools used in the pipeline.\n\n\n\n4. fastqc\nThe FastQC plots in this directory are generated relative to the raw, input reads. They may contain adapter sequence and regions of low quality.\nfastqc/\n\n*_fastqc.html: FastQC report containing quality metrics.\n*_fastqc.zip: Zip archive containing the FastQC report, tab-delimited data file and plot images.\n\n\n\n5. trimgalore\nIn this folder you will find your trimmed and filtered fastq files from TrimGalore, including its fastqc results!\ntrimgalore/\n\n*.fq.gz: If --save_trimmed is specified, FastQ files after adapter trimming will be placed in this directory.\n*_trimming_report.txt: Log file generated by Trim Galore!.\n\ntrimgalore/fastqc/\n\n*_fastqc.html: FastQC report containing quality metrics for read 1 (and read2 if paired-end) after adapter trimming.\n*_fastqc.zip: Zip archive containing the FastQC report, tab-delimited data file and plot images.\n\n\n\n6. aligner\nDepending on the aligner that you use, this folder may change its contents and name. Generally, here you can find the aligned reads in .bam format, samtool stats, duplicated stats and different QC tools related to mapped reads.\nAlso, depending on the --aligner option you can either find the quantification results from either salmon (pseudoquantification) or rsem (traditional quantification, i.e., a count matrix)\nFrom star_salmon\nstar_salmon/\n\n*.Aligned.out.bam: If --save_align_intermeds is specified the original BAM file containing read alignments to the reference genome will be placed in this directory.\n\nstar_salmon/log/\n\n*.SJ.out.tab: File containing filtered splice junctions detected after mapping the reads.\n*.Log.final.out: STAR alignment report containing the mapping results summary.\n*.Log.out and *.Log.progress.out: STAR log files containing detailed information about the run. Typically only useful for debugging purposes.\n\nstar_salmon/salmon/\n\nsalmon.merged.gene_counts.tsv: Matrix of gene-level raw counts across all samples.\nsalmon.merged.gene_tpm.tsv: Matrix of gene-level TPM values across all samples.\nsalmon.merged.gene_counts.rds: RDS object that can be loaded in R that contains a SummarizedExperiment container with the TPM (abundance), estimated counts (counts) and transcript length (length) in the assays slot for genes.\nsalmon.merged.gene_counts_scaled.tsv: Matrix of gene-level scaled counts across all samples.\nsalmon.merged.gene_counts_scaled.rds: RDS object that can be loaded in R that contains a SummarizedExperiment container with the TPM (abundance), estimated counts (counts) and transcript length (length) in the assays slot for genes.\nsalmon.merged.gene_counts_length_scaled.tsv: Matrix of gene-level length-scaled counts across all samples.\nsalmon.merged.gene_counts_length_scaled.rds: RDS object that can be loaded in R that contains a SummarizedExperiment container with the TPM (abundance), estimated counts (counts) and transcript length (length) in the assays slot for genes.\nsalmon.merged.transcript_counts.tsv: Matrix of isoform-level raw counts across all samples.\nsalmon.merged.transcript_tpm.tsv: Matrix of isoform-level TPM values across all samples.\nsalmon.merged.transcript_counts.rds: RDS object that can be loaded in R that contains a SummarizedExperiment container with the TPM (abundance), estimated counts (counts) and transcript length (length) in the assays slot for transcripts.\nsalmon_tx2gene.tsv: Tab-delimited file containing gene to transcripts ids mappings.\n\nstar_salmon/salmon/&lt;SAMPLE&gt;/\n\nlogs/: Contains the file salmon_quant.log giving a record of Salmon‚Äôs quantification.\nquant.genes.sf: Salmon gene-level quantification of the sample, including feature length, effective length, TPM, and number of reads.\nquant.sf: Salmon transcript-level quantification of the sample, including feature length, effective length, TPM, and number of reads.\n\nFrom star_rsem\nstar_rsem/\n\nrsem.merged.gene_counts.tsv: Matrix of gene-level raw counts across all samples.\nrsem.merged.gene_tpm.tsv: Matrix of gene-level TPM values across all samples.\nrsem.merged.transcript_counts.tsv: Matrix of isoform-level raw counts across all samples.\nrsem.merged.transcript_tpm.tsv: Matrix of isoform-level TPM values across all samples.\n*.genes.results: RSEM gene-level quantification results for each sample.\n*.isoforms.results: RSEM isoform-level quantification results for each sample.\n\nstar_rsem/&lt;SAMPLE&gt;.stat/\n\n*.cnt, *.model, *.theta: RSEM counts and statistics for each sample.\n\nstar_rsem/log/\n\n*.log: STAR alignment report containing the mapping results summary.\n\n\n\n7. pseudoaligner\nThis is the same output as the star_salmon/salmon folder.\n\nParts of this lesson have been taken from Wikipedia, the Nextflow webpage and the nf-core project webpage.",
    "crumbs": [
      "Data processing",
      "Nextflow & nf-core pipelines"
    ]
  },
  {
    "objectID": "develop/cards/extraCards/SamueleSoraggi.html",
    "href": "develop/cards/extraCards/SamueleSoraggi.html",
    "title": "Samuele Soraggi",
    "section": "",
    "text": "Samuele is a Sandbox data scientist based at the university of Aarhus. During his academic activity he has gained experience in population genomics, transcriptomics, single cell multiomics and spans his knowledge across various themes of advanced computational statistics."
  },
  {
    "objectID": "develop/cards/AdrijaKalvisa.html",
    "href": "develop/cards/AdrijaKalvisa.html",
    "title": "Adrija Kalvisa",
    "section": "",
    "text": "Adrija is a Special consultant at the NNF CPR/reNEW Genomics platform. Find out more about the genomics platform here."
  },
  {
    "objectID": "develop/summary.html",
    "href": "develop/summary.html",
    "title": "Workshop summary",
    "section": "",
    "text": "Below you will find a summary of all the steps used in the workshop, as well as annotations for each of the steps."
  },
  {
    "objectID": "develop/summary.html#rnaseq-experiment",
    "href": "develop/summary.html#rnaseq-experiment",
    "title": "Workshop summary",
    "section": "RNAseq experiment",
    "text": "RNAseq experiment\n\nThe goal of a bulk RNAseq analysis is to compare gene expression between two or more conditions.\nA bulk RNAseq experiment requires isolation of mRNA from samples with different conditions.\nmRNA molecules are sequenced (reads), so we know their nucleotide sequence.\nWe still cannot do any sort of comparison between genes.\n\nSequences lack gene information, cannot perform any statistical analysis.\n\nReads need to be preprocessed to obtain a matrix of counts per gene and sample (‚Äúcount matrix‚Äù).\nTo preprocess fastq files we use a plethora of programs for each step.\n\nQC, trimming, alignment and quantification.\n\nAll these steps are compiled into a pipeline: nf-core rnaseq.\n\nRead more about how to use the nf-core rnaseq pipeline here.\n\nThen, we can proceed with data analysis and finding differences in expression.\n\nAfter the count matrix is created, we use R to analyse our data.\nBulk RNAseq analysis in R uses also several R packages.\n\nSo our workflow can be divided in two: preprocessing and data analysis."
  },
  {
    "objectID": "develop/summary.html#ucloud",
    "href": "develop/summary.html#ucloud",
    "title": "Workshop summary",
    "section": "UCloud",
    "text": "UCloud\n\nUCloud is a danish High Performance Computing environment.\n\nLots of storage, lots of CPUs and RAM (computing power).\n\nDanish institutions have access to it.\n\nYou personally have 1000 DKK in computing resources.\n\nUCloud works in apps, giving you access to different programs.\n\nAll apps have documentation on how to use them!\n\nThis means everyone is using the same versions of software.\n\nMakes teaching much much easier, results are reproducible."
  },
  {
    "objectID": "develop/summary.html#step-1-preprocessing",
    "href": "develop/summary.html#step-1-preprocessing",
    "title": "Workshop summary",
    "section": "Step 1 Preprocessing",
    "text": "Step 1 Preprocessing\n\nTraditional preprocessing\n\n\nExperimental design. Before you start a bulk RNAseq experiment, take into account: 1. Proper controls. 2. Replicates. 3. Confounding factors. 4. Batch effects. 5. Talk to a bioinformatician!\nWet lab protocol The RNAseq lab protocol ends in the sequencer\nSequencer Sequencer returns images of the sequence-by-synthesis process in bcl format\nBcl to fastq files translation Software bcl2fastq transforms sequencer images into fastq format. This is formally known as ‚Äúreads‚Äù. Depending on your RNAseq protocol, you will have either single end or paired end reads.\nRaw fastq files They contain read information, such as the nucleotide sequence and the quality of each of the nucleotides. They are called ‚Äúraw‚Äù cause they are unfiltered and may contain primers and adaptors. We need to check read quality and probably clean them\nReads Quality Control Using the software fastQC we can check the quality of our reads before and after cleaning. fastQC will calculate metrics such as: GC-content, overall read quality, per base quality, overrepresented sequences, etc.\nRaw fastQC report We check the report in html, which can be visualized in an internet browser. We check the fastQC metrics mentioned in 6.\nTrimming and cleaning Raw fastq files may contain bad quality reads and adaptors which will pose a problem when trying to align the reads. TrimGalore! is a tool that will clean, filter and trim reads. It will also create a fastQC report of the clean reads.\n\nRemove adapters\nTrim bad quality bases from either end\nFilter out bad quality reads\n\n\nTrimming & cleaning: quality control\n\nClean fastq files These are reads that have been cleaned and ready to aligned.\nClean fastQC report We check again that we have solved the issues we saw in 7.\nAlignment Alignment is the process of mapping the origin of our reads to the reference genome12. There are many aligners tools, such as STAR, BWA, or HISAT2. They will use the indexed genome13 to map the reads.\nReference genome In order to know from which genes our reads are coming from, we need to align them to a reference genome. Make sure you are using the latest stable version of the genome for your organism of interest! Depending on the aligner you are using, you will have to index your genome15 using that same aligner\nfasta file A fasta file is a text file that contains a nucleotide or amino acid sequence. It always starts with ‚Äú&gt;‚Äù as a header. The header contains information about the sequence, such as ‚ÄúChromosome 1 of the mouse genome‚Äù.\nAnnotations in gtf or gff file While the fasta file contains the actual sequence, the annotation file provides information about regions of the sequence, such as genes, promoters, enhancers, etc. The annotation file will have coordinates of where this region falls in the fasta sequence, as well as what type of region it is. Thus it is important that the coordinates of the file matches the version of the reference genome you are using.\nReference indexing In order to align reads to a reference in an efficient way, the fasta file must be structured. This process is called ‚Äúindexing‚Äù, which allows fast search and mapping of reads to the reference. Indexing is performed by the same aligner that you will be using. It will return files with different formats, depending on the aligner you used.\nAligned reads Reads that have been mapped to the reference genome. They are in BAM format17.\nBAM files BAM files are binary files containing information about the read alignment, such as: - Where in the reference do they fall. - Quality of the mapping. - Quality of the read.\n\nDuplicates evaluation\n\nCheck duplicates Due to the nature of mRNAs and the protocol to sequence them, duplications will occur (blue reads in 16). They can be artifacts of PCR amplification or natural duplicates of highly expressed genes. The tool dupRadar will create these plots for you to check them out19.\ndupRadar results dupRadar will plot the ratio of duplications (y axis) as a function of the gene expression (x axis).\nAs long as you find a low ratio of duplicates for lowly expressed genes and high duplication rates of highly expressed genes, there is nothing to worry about (left image).\nIf your ratio of duplicates is proportional (right plot) you probably have artificial duplicates. You may consider removing them in 20.\nRemove duplicates This is an optional step if your duplicate ratios are not what you expected from the dupRadar results19. This can be done with the tool MarkDuplicates and will return you BAM files17 without duplicated reads.\nQuantification Using the annotations from the gtf or gff file14, we can finally count how many reads belong to each gene from each sample and create a count matrix.\nThere are several tools that can do this, such as STAR, HISAT2, bedtools, Rsubread.\nCount matrix Matrix of rows and columns where: columns are samples and rows are genes. Usually in tab separated values (tsv) format.\nMultiQC report All metrics and QC checks can be compiled into a single html report that summarizes the entire preprocessing workflow:\n\nRaw reads fastQC\nClean and trimmed reads QC\nAlignment metrics\nDuplication rates\ndupRadar results\nQuantification results \n\n\nAll these steps are compiled into a pipeline: nf-core rnaseq\n\n\nPseudo preprocessing\n\nPseudoalignment and quantification preprocessing differs from traditional mapping. With pseudoalignment we do not know the exact coordinates of our reads, but it makes the alignment and quantification process much faster. However, we will lose many QC steps on the way. Other than that, the workflow starts very similar.\nNOTE: you can do traditional alignment and also pseudoquantification. This way you can still get all your QC metrics.\n\nExperimental design. Before you start a bulk RNAseq experiment, take into account: 1. Proper controls. 2. Replicates. 3. Confounding factors. 4. Batch effects. 5. Talk to a bioinformatician!\nWet lab protocol The RNAseq lab protocol ends in the sequencer\nSequencer Sequencer returns images of the sequence-by-synthesis process in bcl format\nBcl to fastq files translation Software bcl2fastq transforms sequencer images into fastq format. This is formally known as ‚Äúreads‚Äù. Depending on your RNAseq protocol, you will have either single end or paired end reads.\nRaw fastq files They contain read information, such as the nucleotide sequence and the quality of each of the nucleotides. They are called ‚Äúraw‚Äù cause they are unfiltered and may contain primers and adaptors. We need to check read quality and probably clean them\nReads Quality Control Using the software fastQC we can check the quality of our reads before and after cleaning. fastQC will calculate metrics such as: GC-content, overall read quality, per base quality, overrepresented sequences, etc.\nRaw fastQC report We check the report in html, which can be visualized in an internet browser. We check the fastQC metrics mentioned in 6.\nTrimming and cleaning Raw fastq files may contain bad quality reads and adaptors which will pose a problem when trying to align the reads. TrimGalore! is a tool that will clean, filter and trim reads. It will also create a fastQC report of the clean reads.\n\nRemove adapters\nTrim bad quality bases from either end\nFilter out bad quality reads\n\n\nTrimming & cleaning: quality control\n\nClean fastq files These are reads that have been cleaned and ready to aligned.\nClean fastQC report We check again that we have solved the issues we saw in 7.\n\nAlignment\nA. Pseudoalignment Alignment is the process of mapping the origin of our reads to the reference genome12. There are several pseudoaligners tools, such as salmon or kallisto. They will use the genome graph/index15 to align the reads. NOTE: Pseudoalignment and quantification can be done in one step/calculation, there are no aligned files.\n\nReference genome In order to know from which genes our reads are coming from, we need to align them to a reference genome. Make sure you are using the latest stable version of the genome for your organism of interest! Depending on the aligner you are using, you will have to index your genome14 using that same aligner\nfasta file A fasta file is a text file that contains a nucleotide or amino acid sequence. It always starts with ‚Äú&gt;‚Äù as a header. The header contains information about the sequence, such as ‚ÄúChromosome 1 of the mouse genome‚Äù.\nAnnotations in gtf or gff file While the fasta file contains the actual sequence, the annotation file provides information about regions of the sequence, such as genes, promoters, enhancers, etc. The annotation file will have coordinates of where this region falls in the fasta sequence, as well as what type of region it is. Thus it is important that the coordinates of the file matches the version of the reference genome you are using.\nReference indexing In order to pseudoalign reads to a reference in an efficient way, the fasta file must be structured: 1. First the reference, which includes different trasncripts, will be transfomed into a graph. 2. Each node is a k-mer consisting of ‚Äúk‚Äù nucleotides. 3. Each noche is compatible with X transcripts 4. The nodes and their compatibilities are indexed 5. Redundant nodes are removed\nIndexing is performed by the same pseudoaligner that you will be using. It will return files with different formats, depending on the pseudoaligner you used.\n\nResults quantification\n\nPseudoquantification Using the annotations from the gtf or gff file14, we can estimate count how many reads belong to each gene from each sample. Salmon and kallisto will create a quantification file for each of your samples.\nNOTE: Pseudoalignment and quantification can be done in one step/calculation, there are no aligned files.\nQuantification results Pseudoaligners will create individual results for each of the samples, called ‚Äúquant.sf‚Äù files. The quant.sf file is matrix where each row is a transcript. There are several columns: - Name: This is the name of the target transcript provided in the input transcript database (FASTA file). - Length: This is the length of the target transcript in nucleotides. - EffectiveLength: This is the computed effective length of the target transcript. - TPM: This is salmon‚Äôs estimate of the relative abundance of this transcript in units of Transcripts Per Million (TPM). - NumReads: This is salmon‚Äôs estimate of the number of reads mapping to each transcript that was quantified.\nThe pseudoaligner will also create a tx2gene.txt file that contains translations between transcripts IDs, gene IDs and gene names.\nImport and merge quant.sf files Using the tximport R package, we can import all our quant.sf files and txt2gene.txt file into a proper count matrix that will be fed into the DESeq2 package for differential expression analysis.\nCount matrix Matrix of rows and columns where: columns are samples and rows are genes. Usually in tab separated values (tsv) format.\nMultiQC report All metrics and QC checks can be compiled into a single html report that summarizes the entire preprocessing workflow: - Raw reads fastQC - Clean and trimmed reads QC - Alignment metrics - Duplication rates - dupRadar results - Quantification results\n\nAll these steps are compiled into a pipeline: nf-core rnaseq"
  },
  {
    "objectID": "develop/summary.html#data-analysis",
    "href": "develop/summary.html#data-analysis",
    "title": "Workshop summary",
    "section": "2. Data analysis",
    "text": "2. Data analysis\n\nWe have now our count matrix, either from the salmon results or from our traditional count matrix. It is time to get into Rstudio and do our data analysis!\n\nCount matrix Matrix of rows and columns where: columns are samples and rows are genes. Usually in tab separated values (tsv) format.\nMetadata Matrix of rows and columns that contain information about your samples. This could be your samplesheet.csv used for the nf-core pipeline. Each row is a sample and each column is information about that sample, including our variables of interest (condition) and possible confounding variables.\n\n\nCreate DESeq object Create a DESeq object with the count matrix and metadata, either from the salmon results (from tximport) or a traditional count matrix (traditional aligner and quantification, like STAR). We need to already specify our formula for statistical testing. This can be changed later after checking for sources of variation.\nNOTE: make sure that the order of the samples in columns in the count matrix is the same as the order of the samples in the rows of your metadata! - DESeqDataSetFromTximport(counts, colData = metadata, design = ~ condition) -&gt; For salmon results - DESeqDataSetFromMatrix(counts, colData = metadata, design = ~ condition) -&gt; For traditional count matrix\n\n\nDESeq object DESeq object will contain all the info necessary to run a differential expression analysis, including the design, size factors, dispersions and statistical tests.\nDesign Formula used for modeling and statistical testing. In here you should also put your variables that contribute to differences between samples (variance). - A basic experiment, e.g., treated vs untreated cells will have this information in the metadata variable ‚Äúcondition‚Äù. Thus the design formula should be: ~ condition. - A experiment containing two variables, such as ‚Äútreatment‚Äù (treated and control) vs sex (males and females) would have a design formula like this: ~ sex + condition. Your variable of interest should be the last one! - If you expect a combined effect between sex and treatment, add the combined factor to the design: ~ sex + condition + sex:condition.\n\n\nModel counts Running the DESeq() function, you will automatically model your counts for differential expression analysis. The DESeq() function will run three steps in a row:\n\nestimateSizeFactors(dds): calculate size factors for each sample and normalize count matrix by the median of rations method\nestimateDispersions(dds): estimate gene dispersions, that is, how is the variance of a gene compared to its mean across all samples.\nnbinomWaldTest(dds): model your counts using the size factors and dispersions as well as running Wald tests for statistical significance.\n\n\n\nModeled DESeq object DESeq object that has run the DESeq() function on it. It contains all the information calculated in 6.\nNormalized counts You can extract your normalized counts using the median of ratios methods with the function: counts(dds, normalized = T). This normalized counts can be useful for downstream analysis using other types of tools as well as visualizations.\n\n\nExploratory analysis Now we are able to check if our sampels are behaving the way we expect, that is, if our replicates are grouped together, our genes have the dispersion we expect and check for sources of variation.\n\nDispersion plot After calculating gene dispersions, we can check if our dispersions match what we expect from a negative binomial distribution typical of bulk RNAseq. If the fitted line is not in the middle of the cloud of dots, or the is a big ‚Äúrainfall‚Äù of dots coming from the clouds it might indicate problems with your data, like an outlier, contamination or you are not using a raw count matrix.\nHeatmaps and clustering You can use sample distances or Pearson correlations to group or clusters your samples together. Replicates of the same type should cluster together and be apart from other types of replicates.\nPCA plot The PCA can be used also to check that your replicates are group together. It can also be used to check for different variables that are driving the clustering11. Ideally you will find that PC1 and PC2 separate your samples by the variable of interest. Otherwise, you might want to adjust your design formula11.\n\n\n\nCheck sources of variation If variables (confounding variables) other than your variables of interest are separating your samples in your PCA plot, you should probably go back to create a DESeqObject3 step and include your confounding variables into the design formula.\n\n\nDEA We can extract now our results of Differential Expression analysis, that is, our log2 Fold Changes and statistical significance (p-value). Remember that there are different statistical filters (genes with 0 counts, outliers, etc) which will return NA as a p-value. In order to get your results you will have to provide a comparison, a.k.a., a ‚Äúcontrast‚Äù. There are different ways of specifying this contrast. You can use either:\n\nNormal results12: results(dds, contrast = &lt;comparison&gt;)\nLog2FC shrunken results13: lfcShrink(dds, coef = &lt;comparison&gt;)\n\n\n\nResults table Normal, unshrunken results for DEA of the specified comparison (contrast). You can obtain this table using the function: results(dds, contrast = comparison). The table will contain several columns. The most important ones are: - baseMean: mean of your normalized counts - log2FoldChange: ratio of expression between two conditions in log2 scale - p-value: wald statistical test pvalue - padj: adjusted pvalues for multiple testing\nYou can further filter this table using a cutoff for your adjusted pvalue and thresholds for your LFCs (up regulated genes or down regulated genes).\nLogFoldShrink results table Normal results will have very noise LFC for low counts, as well as having very low statistical significance. Using this technique, you can reduce the noisy LFC from insignificant genes. These are very useful for visualization15 or ranking of genes based on their LFC in a GSEA18. This function returns the same columns as before13.\n\n\nVisualize results Instead of looking at tables, you can check your differentially expressed genes using different visualizations.\n\nMAplot: each dot is a gene; x axis is the mean of the normalized counts; y axis is the shrunken LFC14. Colored dots are significant genes. Use the plotMA(res) function.\nVolcano plot: each dot is a gene; x axis is the shrunken LFC14; y axis is the adjusted p-value in -log10 scale. Colored dots are significant genes. You need to create custom ggplot().\nHeatmap plots. Each row is a gene and each column is a sample. You can plot here differentially expressed genes using the normalized counts8 or LFC between different comparisons.\n\nAnnotate genes We will annotate our genes with different information so that we can perform functional analysis using different databases. We do this using the R package annotationHub. We extract information regarding our genes of interest, such as entrez ID, gene names, genomic regions, etc.\nNOTE: make sure we use the same genome version as the one we used to align our reads. Different versions of the genome will have gene Ids for the same gene.\n\n\nAnnotated results table Results table annotated with more information15, such as database IDs, ready for functional analysis17.\n\n\nFunctional analysis We can annotate the results of our DEA to reveal the function or biological relevance of our genes of interest. We use the R package clusterProfiler to do functional analysis. We can consult different databases, such as Gene Ontologies (GO), KEGG pathways or Disease Ontologies (DO), using different methods:\n\nOverrepresentation analysis: enrichXXX(). Performs a statistical test on a contingency table for enrichment on GO or DO terms, as well as KEGG pathways. XXX depends on the enrichment you want to perform.\nGSEA: gseXXX(). Performs Gene Set Enrichment Analysis (GSA) on a ranked list of genes (by p-value or shrunken LFC13). XXXdepends on the enrichment you want to perform.\nPathway perturbance: pathway(). You visualize how genes in a path are being up or down regulated based on LFC. You should select a path that was a significant hit.\nNetworks: you can compile information of similar enriched terms into a network for easier understanding of your functional analysis results."
  },
  {
    "objectID": "develop/08a_FA_genomic_annotation.html",
    "href": "develop/08a_FA_genomic_annotation.html",
    "title": "Gene annotation",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 30 minutes\nüí¨ Learning Objectives:\n\nDiscuss the available genomic annotation databases and the different types of information stored\nCompare and contrast the tools available for accessing genomic annotation databases\nApply various R packages for retrieval of genomic annotations\nGenomic annotations for functional analyses\nThe analysis of next-generation sequencing results requires associating genes, transcripts, proteins, etc. with functional or regulatory information. To perform functional analysis on gene lists, we often need to obtain gene identifiers that are compatible with the tools we wish to use and this is not always trivial. Here, we discuss ways in which you can obtain gene annotation information and some of the advantages and disadvantages of each method.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Gene annotation"
    ]
  },
  {
    "objectID": "develop/08a_FA_genomic_annotation.html#databases",
    "href": "develop/08a_FA_genomic_annotation.html#databases",
    "title": "Gene annotation",
    "section": "Databases",
    "text": "Databases\nWe retrieve information on the processes, pathways, etc. (for which a gene is involved in) from the necessary database where the information is stored. The database you choose will be dependent on what type of information you are trying to obtain. Examples of databases that are often queried, include:\nGeneral databases\nOffer comprehensive information on genome features, feature coordinates, homology, variant information, phenotypes, protein domain/family information, associated biological processes/pathways, associated microRNAs, etc.:\n\nEnsembl (use Ensembl gene IDs)\nNCBI (use Entrez gene IDs)\nUCSC\nEMBL-EBI\n\nAnnotation-specific databases\nProvide annotations related to a specific topic:\n\nGene Ontology (GO): database of gene ontology biological processes, cellular components and molecular functions - based on Ensembl or Entrez gene IDs or official gene symbols\nKEGG: database of biological pathways - based on Entrez gene IDs\nMSigDB: database of gene sets\nReactome: database of biological pathways\nHuman Phenotype Ontology: database of genes associated with human disease\nCORUM: database of protein complexes for human, mouse, rat\n‚Ä¶\n\nThis is by no means an exhaustive list, there are many other databases available that are not listed here.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Gene annotation"
    ]
  },
  {
    "objectID": "develop/08a_FA_genomic_annotation.html#genome-builds",
    "href": "develop/08a_FA_genomic_annotation.html#genome-builds",
    "title": "Gene annotation",
    "section": "Genome builds",
    "text": "Genome builds\nBefore you begin your search through any of these databases, you should know which build of the genome was used to generate your gene list and make sure you use the same build for the annotations during functional analysis. When a new genome build is acquired, the names and/or coordinate location of genomic features (gene, transcript, exon, etc.) may change. Therefore, the annotations regarding genome features (gene, transcript, exon, etc.) is genome-build specific and we need to make sure that our annotations are obtained from the appropriate resource.\nFor example, we have used the GRCh38 build of the human genome to quantify gene expression for differential expression analysis, so we should use the same GRCh38 build of the genome to convert between gene IDs and to identify annotations for each of the genes.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Gene annotation"
    ]
  },
  {
    "objectID": "develop/08a_FA_genomic_annotation.html#tools-for-accessing-databases",
    "href": "develop/08a_FA_genomic_annotation.html#tools-for-accessing-databases",
    "title": "Gene annotation",
    "section": "Tools for accessing databases",
    "text": "Tools for accessing databases\nWithin R, there are many popular packages used for gene/transcript-level annotation. These packages provide tools that take the list of genes you provide and retrieve information for each gene using one or more of the databases listed above.\n\nAnnotation tools: for accessing/querying annotations from a specific databases\n\n\n\n\nTool\nDescription\nPros\nCons\n\n\n\n\norg.Xx.eg.db\nQuery gene feature information for the organism of interest\ngene ID conversion, biotype and coordinate information\nonly latest genome build available\n\n\nEnsDb.Xx.vxx\nTranscript and gene-level information directly fetched from Ensembl API (similar to TxDb, but with filtering ability and versioned by Ensembl release)\neasy functions to extract features, direct filtering\nNot the most up-to-date annotations, more difficult to use than some packages\n\n\nTxDb.Xx.UCSC.hgxx.knownGene\nUCSC database for transcript and gene-level information or can create own TxDb from an SQLite database file using the GenomicFeatures package\nfeature information, easy functions to extract features\nonly available current and recent genome builds - can create your own, less up-to-date with annotations than Ensembl\n\n\nannotables\nGene-level feature information immediately available for the human and model organisms\nsuper quick and easy gene ID conversion, biotype and coordinate information\nstatic resource, not updated regularly\n\n\nbiomaRt\nAn R package version of the Ensembl BioMart online tool\nall Ensembl database information available, all organisms on Ensembl, wealth of information\nnan\n\n\n\n\n\n\nInterface tools\nOther packages are design for accessing/querying annotations from multiple different annotation sources\n\nAnnotationDbi: queries the OrgDb, TxDb, Go.db, EnsDb, and BioMart annotations.\n\nAnnotationHub: queries large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc.\n\n\n\n\n\n\n\nTip\n\n\n\nThese are both packages that can be used to create the tx2gene files that salmon gave us in case you did not have them.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Gene annotation"
    ]
  },
  {
    "objectID": "develop/08a_FA_genomic_annotation.html#annotationdbi",
    "href": "develop/08a_FA_genomic_annotation.html#annotationdbi",
    "title": "Gene annotation",
    "section": "AnnotationDbi",
    "text": "AnnotationDbi\nAnnotationDbi is an R package that provides an interface for connecting and querying various annotation databases using SQLite data storage. The AnnotationDbi packages can query the OrgDb, TxDb, EnsDb, Go.db, and BioMart annotations. There is helpful documentation available to reference when extracting data from any of these databases.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Gene annotation"
    ]
  },
  {
    "objectID": "develop/08a_FA_genomic_annotation.html#annotationhub",
    "href": "develop/08a_FA_genomic_annotation.html#annotationhub",
    "title": "Gene annotation",
    "section": "AnnotationHub",
    "text": "AnnotationHub\nAnnotationHub is a wonderful resource for accessing genomic data or querying large collection of whole genome resources, including ENSEMBL, UCSC, ENCODE, Broad Institute, KEGG, NIH Pathway Interaction Database, etc. All of this information is stored and easily accessible by directly connecting to the database.\nTo get started with AnnotationHub, we first load the library and connect to the database:\n# Load libraries\nlibrary(AnnotationHub)\nlibrary(ensembldb)\n\n# Connect to AnnotationHub\nah &lt;- AnnotationHub()\n\n\n\n\n\n\nWarning\n\n\n\nThe script will ask you to create a cache directory, type yes!\n\n\n\n\n\n\n\n\nWhat is a cache?\n\n\n\nA cache is used in R to store data or a copy of the data so that future requests can be served faster without having to re-run a lengthy computation.\nThe AnnotationHub() command creates a client that manages a local cache of the database, helping with quick and reproducible access. When encountering question AnnotationHub does not exist, create directory?, you can anwser either yes (create a permanent location to store cache) or no (create a temporary location to store cache). hubCache(ah) gets the file system location of the local AnnotationHub cache. hubUrl(ah) gets the URL for the online hub.\n\n\nTo see the types of information stored inside our database, we can just type the name of the object:\n\n\n\n\n\n\nNote\n\n\n\nResults here will differ from yours\n\n\n# Explore the AnnotationHub object\nah\nUsing the output, you can get an idea of the information that you can query within the AnnotationHub object. (Note that the output below will be different than yours!)\nAnnotationHub with 47240 records\n# snapshotDate(): 2019-10-29 \n# $dataprovider: BroadInstitute, Ensembl, UCSC, ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/, H...\n# $species: Homo sapiens, Mus musculus, Drosophila melanogaster, Bos taurus, Pan troglod...\n# $rdataclass: GRanges, BigWigFile, TwoBitFile, Rle, OrgDb, EnsDb, ChainFile, TxDb, Inpa...\n# additional mcols(): taxonomyid, genome, description, coordinate_1_based,\n#   maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl,\n#   sourcetype \n# retrieve records with, e.g., 'object[[\"AH5012\"]]' \n\n                title                                                                   \n    AH5012  | Chromosome Band                                                         \n    AH5013  | STS Markers                                                             \n    AH5014  | FISH Clones                                                             \n    AH5015  | Recomb Rate                                                             \n    AH5016  | ENCODE Pilot                                                            \n    ...       ...                                                                     \n    AH78364 | Xiphophorus_maculatus.X_maculatus-5.0-male.ncrna.2bit                   \n    AH78365 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.cdna.all.2bit       \n    AH78366 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_rm.toplevel.2bit\n    AH78367 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.dna_sm.toplevel.2bit\n    AH78368 | Zonotrichia_albicollis.Zonotrichia_albicollis-1.0.1.ncrna.2bit    \nNotice the note on retrieving records with object[[\"AH5012\"]] - this will be how we can extract a single record from the AnnotationHub object.\nIf you would like to see more information about any of the classes of data you can extract that information as well. For example, if you wanted to determine all species information available, you could explore that within the AnnotationHub object:\n# Explore all species information available\nunique(ah$species) %&gt;% head()\nIn addition to species information, there is also additional information about the type of Data Objects and the Data Providers:\n# Explore the types of Data Objects available\nunique(ah$rdataclass) %&gt;% head()\n\n# Explore the Data Providers\nunique(ah$dataprovider) %&gt;% head()\nNow that we know the types of information available from AnnotationHub we can query it for the information we want using the query() function. Let‚Äôs say we would like to return the Ensembl EnsDb information for Human. To return the records available, we need to use the terms as they are output from the ah object to extract the desired data.\n# Query AnnotationHub\nhuman_ens &lt;- query(ah, c(\"Homo sapiens\", \"EnsDb\"))\nThe query retrieves all hits for the EnsDb objects, and you will see that they are listed by the release number. The most current release for GRCh38 is Ensembl98 and AnnotationHub offers that as an option to use. However, if you look at options for older releases, for Homo sapiens it only go back as far as Ensembl 87. This is fine if you are using GRCh38, however if you were using an older genome build like hg19/GRCh37, you would need to load the EnsDb package if available for that release or you might need to build your own with ensembldb.\n\n\n\n\n\n\nNote\n\n\n\nResults here will differ from yours\n\n\nhuman_ens\nAnnotationHub with 13 records\n# snapshotDate(): 2019-10-29 \n# $dataprovider: Ensembl\n# $species: Homo sapiens\n# $rdataclass: EnsDb\n# additional mcols(): taxonomyid, genome, description, coordinate_1_based,\n#   maintainer, rdatadateadded, preparerclass, tags, rdatapath, sourceurl,\n#   sourcetype \n# retrieve records with, e.g., 'object[[\"AH53211\"]]' \n\n                title                            \n    AH53211 | Ensembl 87 EnsDb for Homo Sapiens\n    AH53715 | Ensembl 88 EnsDb for Homo Sapiens\n    AH56681 | Ensembl 89 EnsDb for Homo Sapiens\n    AH57757 | Ensembl 90 EnsDb for Homo Sapiens\n    AH60773 | Ensembl 91 EnsDb for Homo Sapiens\n    ...       ...                              \n    AH67950 | Ensembl 95 EnsDb for Homo sapiens\n    AH69187 | Ensembl 96 EnsDb for Homo sapiens\n    AH73881 | Ensembl 97 EnsDb for Homo sapiens\n    AH73986 | Ensembl 79 EnsDb for Homo sapiens\n    AH75011 | Ensembl 98 EnsDb for Homo sapiens\nIn our case, we are looking for the latest Ensembl release so that the annotations are the most up-to-date. To extract this information from AnnotationHub, we can use the AnnotationHub ID to subset the object:\n# Extract annotations of interest\nhuman_ens &lt;- human_ens[[length(human_ens)]] # We extract latest\nNow we can use ensembldb functions to extract the information at the gene, transcript, or exon levels. We are interested in the gene-level annotations, so we can extract that information as follows:\n# Extract gene-level information\ngenes(human_ens, return.type = \"data.frame\") %&gt;% head()\nBut note that it is just as easy to get the transcript- or exon-level information:\n# Extract transcript-level information\ntranscripts(human_ens, return.type = \"data.frame\") %&gt;% head()\n\n# Extract exon-level information\nexons(human_ens, return.type = \"data.frame\") %&gt;% head()\nTo obtain an annotation data frame using AnnotationHub, we‚Äôll use the genes() function, but only keep selected columns and filter out rows to keep those corresponding to our gene identifiers in our results file:\n# Create a gene-level dataframe \nannotations_ahb &lt;- genes(human_ens, return.type = \"data.frame\")  %&gt;%\n  dplyr::select(gene_id, gene_name, entrezid, gene_biotype, description) %&gt;% \n  dplyr::filter(gene_id %in% res_tableCont_tb$gene)\nThis dataframe looks like it should be fine as it is, but we look a little closer we will notice that the column containing Entrez identifiers is a list, and in fact there are many Ensembl identifiers that map to more than one Entrez identifier!\n# Wait a second, we don't have one-to-one mappings!\nclass(annotations_ahb$entrezid)\nwhich(map(annotations_ahb$entrezid, length) &gt; 1)\nSo what do we do here? And why do we have this problem? An answer from the Ensembl Help Desk is that this occurs when we cannot choose a perfect match; ie when we have two good matches, but one does not appear to match with a better percentage than the other. In that case, we assign both matches. What we will do is choose to keep the first identifier for these multiple mapping cases.\nannotations_ahb$entrezid &lt;- map(annotations_ahb$entrezid,1) %&gt;%  unlist()\n\n\n\n\n\n\nTip\n\n\n\nNot all databases handle multiple mappings in the same way. For example, if we used the OrgDb instead of the EnsDb:\nhuman_orgdb &lt;- query(ah, c(\"Homo sapiens\", \"OrgDb\"))\nhuman_orgdb &lt;- human_ens[[length(human_ens)]]\nannotations_orgdb &lt;- select(human_orgdb, res_tableCont_tb$gene, c(\"SYMBOL\", \"GENENAME\", \"ENTREZID\"), \"ENSEMBL\")\nWe would find that multiple mapping entries would be automatically reduced to one-to-one. We would also find that more than half of the input genes do not return any annotations. This is because the OrgDb family of database are primarily based on mapping using Entrez Gene identifiers. Since our data is based on Ensembl mappings, using the OrgDb would result in a loss of information.\n\n\nLet‚Äôs take a look and see how many of our Ensembl identifiers have an associated gene symbol, and how many of them are unique:\nwhich(is.na(annotations_ahb$gene_name)) %&gt;% length()\n\nwhich(duplicated(annotations_ahb$gene_name)) %&gt;% length()\nLet‚Äôs identify the non-duplicated genes and only keep the ones that are not duplicated:\n# Determine the indices for the non-duplicated genes\nnon_duplicates_idx &lt;- which(duplicated(annotations_ahb$gene_name) == FALSE)\n\n# How many rows does annotations_ahb have?\nannotations_ahb %&gt;% nrow()\n\n# Return only the non-duplicated genes using indices\nannotations_ahb &lt;- annotations_ahb[non_duplicates_idx, ]\n\n# How many rows are we left with after removing?\nannotations_ahb %&gt;% nrow()\nFinally, it would be good to know what proportion of the Ensembl identifiers map to an Entrez identifier:\n# Determine how many of the Entrez column entries are NA\nwhich(is.na(annotations_ahb$entrezid)) %&gt;%  length()\nThat‚Äôs more than half of our genes! If we plan on using Entrez ID results for downstream analysis, we should definitely keep this in mind. If you look at some of the Ensembl IDs from our query that returned NA, these map to pseudogenes (i.e ENSG00000265439) or non-coding RNAs (i.e.¬†ENSG00000265425). The discrepancy (which we can expect to observe) between databases is due to the fact that each implements its own different computational approaches for generating the gene builds.\n\nUsing AnnotationHub to create our tx2gene file\nTo create our tx2gene file, we would need to use a combination of the methods above and merge two dataframes together. For example:\n## DO NOT RUN THIS CODE\n\n# Create a transcript dataframe\n txdb &lt;- transcripts(human_ens, return.type = \"data.frame\") %&gt;%\n   dplyr::select(tx_id, gene_id)\n txdb &lt;- txdb[grep(\"ENST\", txdb$tx_id),]\n \n # Create a gene-level dataframe\n genedb &lt;- genes(human_ens, return.type = \"data.frame\")  %&gt;%\n   dplyr::select(gene_id, gene_name)\n \n # Merge the two dataframes together\n annotations &lt;- inner_join(txdb, genedb)\nIn this lesson our focus has been using annotation packages to extract information mainly just for gene ID conversion for the different tools that we use downstream. Many of the annotation packages we have presented have much more information than what we need for functional analysis and we have only just scratched the surface here. It‚Äôs good to know the capabilities of the tools we use, so we encourage you to spend some time exploring these packages to become more familiar with them.",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Gene annotation"
    ]
  },
  {
    "objectID": "develop/08a_FA_genomic_annotation.html#annotables-package",
    "href": "develop/08a_FA_genomic_annotation.html#annotables-package",
    "title": "Gene annotation",
    "section": "Annotables package",
    "text": "Annotables package\nThe annotables package is a super easy annotation package to use. It is not updated frequently, so it‚Äôs not great for getting the most up-to-date information for the current builds and does not have information for other organisms than human and mouse, but is a quick way to get annotation information.\n# Install package\nBiocManager::install(\"annotables\")\n\n# Load library\nlibrary(annotables)\n\n# Access previous build of annotations\ngrch38\nWe can see that the grch38 object already contains all the information we want in a super easy way. Let‚Äôs annotate the results of our shrunken DEA for Control vs Vampirium:\n## Re-run this code if you are unsure that you have the right table\nres_tableCont &lt;- lfcShrink(dds, coef = \"condition_control_vs_vampirium\")\nres_tableCont_tb &lt;- res_tableCont %&gt;%\n    data.frame() %&gt;%\n    rownames_to_column(var=\"gene\") %&gt;% \n    as_tibble()\n## Return the IDs for the gene symbols in the DE results\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableCont))\n\n## Merge the IDs with the results \nres_ids &lt;- inner_join(res_tableCont_tb, ids, by=c(\"gene\"=\"ensgene\"))\n\nhead(res_ids)\nOur data is now ready to use for functional analysis! We have all the ids necessary to proceed.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\n\nCreate a new res_ids object using the annotables package with the human build grch37. NOTE call it res_ids_grch37!\nWhat are the differences between the res_id_ahbobject and the res_ids_grch37?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nids_grch37 &lt;- grch37 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableCont))\n\nres_ids_grch37 &lt;- inner_join(res_tableCont_tb, ids_grch38, by=c(\"gene\"=\"ensgene\"))\n      \nhead(res_ids_grch37)\nLet‚Äôs compare it to the res_ids_ahb object\nhead(res_ids_ahb)\nWe can see that res_id_ahb contains less columns, but this is because we selected fewer columns in our previous steps. What about the the sizes of these tables?\nnrow(res_ids_ahb)\nnrow(res_ids_grch37)\nnrow(res_tableCont_tb)\nWe see that there is a difference in the number of genes. So what is happening? The gene IDs that we have in our count data are from the genome version grch37, and we are trying to match it to annotations from the more updated version grch38. There will be genes that are missing just because of the version. Then we have also removed duplicated gene names in our annotation_ahb object, which may contain different gene IDs. So we may have deleted some gene IDs that are not matching anymore with our results table. In any case, we should always annotate our genes with the version of the reference genome we used for alignment and quantification!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\n\nAnnotate the results of your DEA for Garlicum vs Vampirium with grch38.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n## Return the IDs for the gene symbols in the DE results\nids &lt;- grch38 %&gt;% dplyr::filter(ensgene %in% rownames(res_tableGar))\n\n## Merge the IDs with the results \nres_ids_Gar &lt;- inner_join(res_tableGar_tb, ids, by=c(\"gene\"=\"ensgene\"))\n\nhead(res_ids_Gar)\n\n\n\n\n\n\n\n\n\n\n\nThis lesson was originally developed by members of the teaching team (Mary Piper) at the Harvard Chan Bioinformatics Core (HBC).",
    "crumbs": [
      "Data analyses",
      "Functional Analysis",
      "Gene annotation"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html",
    "href": "develop/07_extra_contrast_design.html",
    "title": "Contrast designs",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 40 minutes\nüí¨ Learning Objectives:\n\nDemonstrate the use of the design formula with simple and complex designs\nConstruct R code to execute the differential expression analysis workflow with DESeq2\nDesign formula and contrasts\nThe final step in the differential expression analysis workflow is fitting the raw counts to the NB model and performing the statistical test for differentially expressed genes. In this step we essentially want to determine whether the mean expression levels of different sample groups are significantly different.\nThe DESeq2 paper was published in 2014, but the package is continually updated and available for use in R through Bioconductor. It builds on good ideas for dispersion estimation and use of Generalized Linear Models from the DSS and edgeR methods.\nDifferential expression analysis with DESeq2 involves multiple steps as displayed in the flowchart below in blue. Briefly, DESeq2 will model the raw counts, using normalization factors (size factors) to account for differences in library depth. Then, it will estimate the gene-wise dispersions and shrink these estimates to generate more accurate estimates of dispersion to model the counts. Finally, DESeq2 will fit the negative binomial model and perform hypothesis testing using the Wald test or Likelihood Ratio Test.\nPrior to performing the differential expression analysis, it is a good idea to know what sources of variation are present in your data, either by exploration during the QC and/or prior knowledge. Once you know the major sources of variation, you can remove them prior to analysis or control for them in the statistical model by including them in your design formula.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html#design-formula",
    "href": "develop/07_extra_contrast_design.html#design-formula",
    "title": "Contrast designs",
    "section": "Design formula",
    "text": "Design formula\nA design formula tells the statistical software the known sources of variation to control for, as well as, the factor of interest to test for during differential expression testing. For example, if you know that sex is a significant source of variation in your data, then sex should be included in your model. The design formula should have all of the factors in your metadata that account for major sources of variation in your data. The last factor entered in the formula should be the condition of interest.\nFor example, suppose you have the following metadata:\n\nIf you want to examine the expression differences between condition, and you know that major sources of variation include bloodtype and patient, then your design formula would be:\ndesign = ~ bloodtype + patient + condition\nThe tilde (~) should always precede your factors and tells DESeq2 to model the counts using the following formula. Note the factors included in the design formula need to match the column names in the metadata.\nIn this tutorial we show a general and flexible way to define contrasts, and is often useful for more complex contrasts or when the design of the experiment is imbalanced (e.g.¬†different number of replicates in each group). Although we focus on DESeq2, the approach can also be used with the other popular package edgeR.\nEach section below covers a particular experimental design, from simpler to more complex ones. The first chunk of code in each section is to simulate data, which has no particular meaning and is only done in order to have a DESeqDataSet object with the right kind of variables for each example. In practice, users can ignore this step as they should have created a DESeqDataSet object from their own data following the instructions in the vignette.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html#one-factor-two-levels",
    "href": "develop/07_extra_contrast_design.html#one-factor-two-levels",
    "title": "Contrast designs",
    "section": "One factor, two levels",
    "text": "One factor, two levels\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 6, betaSD = 2)\ndds$condition &lt;- factor(rep(c(\"control\", \"treat\"), each = 3))\nFirst we can look at our sample information:\ncolData(dds)\n## DataFrame with 6 rows and 1 column\n##         condition\n##          &lt;factor&gt;\n## sample1   control\n## sample2   control\n## sample3   control\n## sample4   treat  \n## sample5   treat  \n## sample6   treat\nOur factor of interest is condition and so we define our design and run the DESeq model fitting routine:\ndesign(dds) &lt;- ~ 1 + condition # or just `~ condition`\ndds &lt;- DESeq(dds) # equivalent to edgeR::glmFit()\nThen check what coefficients DESeq estimated:\nresultsNames(dds)\n## [1] \"Intercept\"                  \"condition_treat_vs_control\"\nWe can see that we have a coefficient for our intercept and coefficient for the effect of treat (i.e.¬†differences between treat versus control).\nUsing the more standard syntax, we can obtain the results for the effect of treat as such:\nres1 &lt;- results(dds, contrast = list(\"condition_treat_vs_control\"))\nres1\n## log2 fold change (MLE): condition_treat_vs_control effect \n## Wald test p-value: condition_treat_vs_control effect \n## DataFrame with 1000 rows and 6 columns\n##           baseMean log2FoldChange     lfcSE         stat      pvalue\n##          &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;   &lt;numeric&gt;\n## gene1     40.90941    1.267525859  0.574144  2.207679752   0.0272666\n## gene2     12.21876   -0.269917301  1.111127 -0.242922069   0.8080658\n## gene3      1.91439   -3.538133611  2.564464 -1.379677442   0.1676860\n## gene4     10.24472    0.954811627  1.166408  0.818591708   0.4130194\n## gene5     13.16824    0.000656519  0.868780  0.000755679   0.9993971\n## ...            ...            ...       ...          ...         ...\n## gene996   40.43827     -1.0291276  0.554587    -1.855664 0.063501471\n## gene997   52.88360      0.0622133  0.561981     0.110704 0.911851377\n## gene998   73.06582      1.3271896  0.576695     2.301373 0.021370581\n## gene999    8.87701     -5.8385374  1.549471    -3.768084 0.000164506\n## gene1000  37.06533      1.2669314  0.602010     2.104501 0.035334764\n##                 padj\n##            &lt;numeric&gt;\n## gene1      0.0712378\n## gene2      0.8779871\n## gene3      0.2943125\n## gene4      0.5692485\n## gene5      0.9996728\n## ...              ...\n## gene996  0.138827354\n## gene997  0.948279388\n## gene998  0.059599481\n## gene999  0.000914882\n## gene1000 0.087737235\nThe above is a simple way to obtain the results of interest. But it is worth understanding how DESeq is getting to these results by looking at the model‚Äôs matrix. DESeq defines the model matrix using base R functionality:\nmodel.matrix(design(dds), colData(dds))\n##         (Intercept) conditiontreat\n## sample1           1              0\n## sample2           1              0\n## sample3           1              0\n## sample4           1              1\n## sample5           1              1\n## sample6           1              1\n## attr(,\"assign\")\n## [1] 0 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\nWe can see that R coded condition as a dummy variable, with an intercept (common to all samples) and a ‚Äúconditiontreat‚Äù variable, which adds the effect of treat to samples 4-6.\nWe can actually set our contrasts in DESeq2::results() using a numeric vector. The way it works is to define a vector of ‚Äúweights‚Äù for the coefficient(s) we want to test for. In this case, we have (Intercept) and conditiontreat as our coefficients (see model matrix above), and we want to test for the effect of treat, so our contrast vector would be c(0, 1). In other words, we don‚Äôt care about the value of (Intercept) (so it has a weight of 0), and we‚Äôre only interested in the effect of treat (so we give it a weight of 1).\nIn this case the design is very simple, so we could define our contrast vector ‚Äúmanually‚Äù. But for complex designs this can get more difficult to do, so it‚Äôs worth mentioning the general way in which we can define this. For any contrast of interest, we can follow three steps:\n\nGet the model matrix\nSubset the matrix for each group of interest and calculate its column means - this results in a vector of coefficients for each group\nSubtract the group vectors from each other according to the comparison we‚Äôre interested in\n\nLet‚Äôs see this example in action:\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n##         (Intercept) conditiontreat\n## sample1           1              0\n## sample2           1              0\n## sample3           1              0\n## sample4           1              1\n## sample5           1              1\n## sample6           1              1\n## attr(,\"assign\")\n## [1] 0 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n# calculate the vector of coefficient weights in the treat\ntreat &lt;- colMeans(mod_mat[dds$condition == \"treat\", ])\ntreat\n##    (Intercept) conditiontreat \n##              1              1\n# calculate the vector of coefficient weights in the control\ncontrol &lt;- colMeans(mod_mat[dds$condition == \"control\", ])\ncontrol\n##    (Intercept) conditiontreat \n##              1              0\n# The contrast we are interested in is the difference between treat and control\ntreat - control\n##    (Intercept) conditiontreat \n##              0              1\nThat last step is where we define our contrast vector, and we can give this directly to the results function:\n# get the results for this contrast\nres2 &lt;- results(dds, contrast = treat - control)\nThis gives us exactly the same results as before, which we can check for example by plotting the log-fold-changes between the first and second approach:\nplot(res1$log2FoldChange, res2$log2FoldChange)",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html#recoding-the-design",
    "href": "develop/07_extra_contrast_design.html#recoding-the-design",
    "title": "Contrast designs",
    "section": "Recoding the design",
    "text": "Recoding the design\nOften, we can use different model matrices that essentially correspond to the same design. For example, we could recode our design above by removing the intercept:\ndesign(dds) &lt;- ~ 0 + condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n## [1] \"conditioncontrol\" \"conditiontreat\"\nIn this case we get a coefficient corresponding to the average expression in control and the average expression in the treat (rather than the difference between treat and control).\nIf we use the same contrast trick as before (using the model matrix), we can see the result is the same:\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n##         conditioncontrol conditiontreat\n## sample1                1              0\n## sample2                1              0\n## sample3                1              0\n## sample4                0              1\n## sample5                0              1\n## sample6                0              1\n## attr(,\"assign\")\n## [1] 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\n# calculate weights for coefficients in each condition\ntreat &lt;- colMeans(mod_mat[which(dds$condition == \"treat\"), ])\ncontrol &lt;- colMeans(mod_mat[which(dds$condition == \"control\"), ])\n# get the results for our contrast\nres3 &lt;- results(dds, contrast = treat - control)\nAgain, the results are essentially the same:\nplot(res1$log2FoldChange, res3$log2FoldChange)\n\nIn theory there‚Äôs no difference between these two ways of defining our design. The design with an intercept is more common, but for the purposes of understanding what‚Äôs going on, it‚Äôs sometimes easier to look at models without intercept.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html#one-factor-three-levels",
    "href": "develop/07_extra_contrast_design.html#one-factor-three-levels",
    "title": "Contrast designs",
    "section": "One factor, three levels",
    "text": "One factor, three levels\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 9, betaSD = 2)\ndds$condition &lt;- NULL\ndds$bloodtype &lt;- factor(rep(c(\"bloodA\", \"bloodB\", \"bloodO\"), each = 3))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\nFirst we can look at our sample information:\ncolData(dds)\n## DataFrame with 9 rows and 1 column\n##         bloodtype\n##          &lt;factor&gt;\n## sample1    bloodA\n## sample2    bloodA\n## sample3    bloodA\n## sample4    bloodB\n## sample5    bloodB\n## sample6    bloodB\n## sample7    bloodO\n## sample8    bloodO\n## sample9    bloodO\nAs in the previous example, we only have one factor of interest, bloodtype, and so we define our design and run the DESeq as before:\ndesign(dds) &lt;- ~ 1 + bloodtype\ndds &lt;- DESeq(dds)\n# check the coefficients estimated by DEseq\nresultsNames(dds)\n## [1] \"Intercept\"                  \"bloodtype_bloodA_vs_bloodO\"\n## [3] \"bloodtype_bloodB_vs_bloodO\"\n\nWe see that now we have 3 coefficients:\n\n- \"Intercept\" corresponds to bloodO bloodtype (our reference level)\n- \"bloodtype_bloodA_vs_bloodO\" corresponds to the difference between the reference level and bloodA\n- \"bloodtype_bloodB_vs_bloodO\" corresponds to the difference between the reference level and bloodB\n\nWe could obtain the difference between bloodO and any of the two bloodtypes easily:\n\n```{.r .code-overflow-wrap}\nres1_bloodA_bloodO &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\"))\nres1_bloodB_bloodO &lt;- results(dds, contrast = list(\"bloodtype_bloodB_vs_bloodO\"))\nFor comparing bloodA vs bloodB, however, we need to compare two coefficients with each other to check whether they are themselves different (check the slide to see the illustration). This is how the standard DESeq syntax would be:\nres1_bloodA_bloodB &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\", \n    \"bloodtype_bloodB_vs_bloodO\"))\nHowever, following our three steps detailed in the first section, we can define our comparisons from the design matrix:\n# define the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n##         (Intercept) bloodtypebloodA bloodtypebloodB\n## sample1           1               1               0\n## sample2           1               1               0\n## sample3           1               1               0\n## sample4           1               0               1\n## sample5           1               0               1\n## sample6           1               0               1\n## sample7           1               0               0\n## sample8           1               0               0\n## sample9           1               0               0\n## attr(,\"assign\")\n## [1] 0 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$bloodtype\n## [1] \"contr.treatment\"\n# calculate coefficient vectors for each group\nbloodA &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\", ])\nbloodB &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodB\", ])\nbloodO &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\", ])\nAnd we can now define any contrasts we want:\n# obtain results for each pairwise contrast\nres2_bloodA_bloodO &lt;- results(dds, contrast = bloodA - bloodO)\nres2_bloodB_bloodO &lt;- results(dds, contrast = bloodB - bloodO)\nres2_bloodA_bloodB &lt;- results(dds, contrast = bloodA - bloodB)\n\n# plot the results from the two approaches to check that they are identical\nplot(res1_bloodA_bloodO$log2FoldChange, res2_bloodA_bloodO$log2FoldChange)\nplot(res1_bloodB_bloodO$log2FoldChange, res2_bloodB_bloodO$log2FoldChange)\nplot(res1_bloodA_bloodB$log2FoldChange, res2_bloodA_bloodB$log2FoldChange)\n\nA and B against O\nWith this approach, we could even define a more unusual contrast, for example to find genes that differ between A and B against and O samples:\n# define vector of coefficients for A_B samples\nA_B &lt;- colMeans(mod_mat[dds$bloodtype %in% c(\"bloodA\", \"bloodB\"),])\n# Our contrast of interest is\nA_B - bloodO\n##     (Intercept) bloodtypebloodA bloodtypebloodB \n##             0.0             0.5             0.5\nNotice the contrast vector in this case assigns a ‚Äúweight‚Äù of 0.5 to each of bloodtypebloodA and bloodtypebloodB. This is equivalent to saying that we want to consider the average of bloodA and bloodB expression. In fact, we could have also defined our contrast vector like this:\n# average of bloodA and bloodB minus bloodO\n(bloodA + bloodB)/2 - bloodO\n##     (Intercept) bloodtypebloodA bloodtypebloodB \n##             0.0             0.5             0.5\nTo obtain our results, we use the results() function as before:\n# get the results between A_B and bloodA\nres2_AB &lt;- results(dds, contrast = A_B - bloodO)\n\n\nExtra: why not define a new group in our design matrix?\nFor this last example (A_B vs bloodO), we may have considered creating a new variable in our column data:\ndds$A_B &lt;- factor(dds$bloodtype %in% c(\"bloodA\", \"bloodB\"))\ncolData(dds)\n## DataFrame with 9 rows and 3 columns\n##         bloodtype sizeFactor      A_B\n##          &lt;factor&gt;  &lt;numeric&gt; &lt;factor&gt;\n## sample1    bloodA   0.972928    TRUE \n## sample2    bloodA   0.985088    TRUE \n## sample3    bloodA   0.960749    TRUE \n## sample4    bloodB   0.916582    TRUE \n## sample5    bloodB   0.936918    TRUE \n## sample6    bloodB   1.137368    TRUE \n## sample7    bloodO   1.071972    FALSE\n## sample8    bloodO   1.141490    FALSE\n## sample9    bloodO   1.140135    FALSE\nand then re-run DESeq with a new design:\ndesign(dds) &lt;- ~ 1 + A_B\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n## [1] \"Intercept\"         \"A_B_TRUE_vs_FALSE\"\nres1_A_B &lt;- results(dds, contrast = list(\"A_B_TRUE_vs_FALSE\"))\nHowever, in this model the gene dispersion is estimated together for bloodA and bloodB samples as if they were replicates of each other, which may result in inflated/deflated estimates. Instead, our approach above estimates the error within each of those groups.\nTo check the difference one could compare the two approaches visually:\n# compare the log-fold-changes between the two approaches\nplot(res1_A_B$log2FoldChange, res2_AB$log2FoldChange)\nabline(0, 1, col = \"brown\", lwd = 2)\n\n# compare the errors between the two approaches\nplot(res1_A_B$lfcSE, res2_AB$lfcSE)\nabline(0, 1, col = \"brown\", lwd = 2)",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html#two-factors-with-interaction",
    "href": "develop/07_extra_contrast_design.html#two-factors-with-interaction",
    "title": "Contrast designs",
    "section": "Two factors with interaction",
    "text": "Two factors with interaction\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 12, betaSD = 2)\ndds$bloodtype &lt;- factor(rep(c(\"bloodO\", \"bloodA\"), each = 6))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\ndds$condition &lt;- factor(rep(c(\"treat\", \"control\"), 6))\ndds &lt;- dds[, order(dds$bloodtype, dds$condition)]\ncolnames(dds) &lt;- paste0(\"sample\", 1:ncol(dds))\nFirst let‚Äôs look at our sample information:\ncolData(dds)\n## DataFrame with 12 rows and 2 columns\n##          condition bloodtype\n##           &lt;factor&gt;  &lt;factor&gt;\n## sample1    control    bloodO\n## sample2    control    bloodO\n## sample3    control    bloodO\n## sample4    treat      bloodO\n## sample5    treat      bloodO\n## ...            ...       ...\n## sample8    control    bloodA\n## sample9    control    bloodA\n## sample10   treat      bloodA\n## sample11   treat      bloodA\n## sample12   treat      bloodA\nThis time we have two factors of interest, and we want to model both with an interaction (i.e.¬†we assume that bloodA and bloodO samples may respond differently to treat/control). We define our design accordingly and fit the model:\ndesign(dds) &lt;- ~ 1 + bloodtype + condition + bloodtype:condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n## [1] \"Intercept\"                      \"bloodtype_bloodA_vs_bloodO\"    \n## [3] \"condition_treat_vs_control\"     \"bloodtypebloodA.conditiontreat\"\nBecause we have two factors and an interaction, the number of comparisons we can do is larger. Using our three-step approach from the model matrix, we do things exactly as we‚Äôve been doing so far:\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\n# Define coefficient vectors for each condition\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\nWe are now ready to define any contrast of interest from these vectors (for completeness we show the equivalent syntax using the coefficient‚Äôs names from DESeq).\nbloodA vs bloodO (in the control):\nres1 &lt;- results(dds, contrast = bloodA_control - bloodO_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(\"bloodtype_bloodA_vs_bloodO\"))\nbloodA vs bloodO (in the treatment):\nres1 &lt;- results(dds, contrast = bloodO_treat - bloodA_treat)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"bloodtype_bloodA_vs_bloodO\",\n    \"bloodtypebloodA.conditiontreat\")))\ntreat vs control (for bloodtypes O):\nres1 &lt;- results(dds, contrast = bloodO_treat - bloodO_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"condition_treat_vs_control\")))\ntreat vs control (for bloodtypes A):\nres1 &lt;- results(dds, contrast = bloodA_treat - bloodA_control)\n# or equivalently\nres2 &lt;- results(dds, contrast = list(c(\"condition_treat_vs_control\", \n    \"bloodtypebloodA.conditiontreat\")))\nInteraction between bloodtype and condition\nI.e. do bloodAs and bloodOs respond differently to the treatment?\nres1 &lt;- results(dds, \n    contrast = (bloodA_treat - bloodA_control) - (bloodO_treat - bloodO_control))\n# or equivalently\nres2 &lt;- results(dds, contrast = list(\"bloodtypebloodA.conditiontreat\"))\nIn conclusion, although we can define these contrasts using DESeq coefficient names, it is somewhat more explicit (and perhaps intuitive?) what it is we‚Äôre comparing using matrix-based contrasts.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html#three-factors-with-nesting",
    "href": "develop/07_extra_contrast_design.html#three-factors-with-nesting",
    "title": "Contrast designs",
    "section": "Three factors, with nesting",
    "text": "Three factors, with nesting\n# simulate data\ndds &lt;- makeExampleDESeqDataSet(n = 1000, m = 24, betaSD = 2)\ndds$bloodtype &lt;- factor(rep(c(\"bloodA\", \"bloodO\"), each = 12))\ndds$bloodtype &lt;- relevel(dds$bloodtype, \"bloodO\")\ndds$patient &lt;- factor(rep(LETTERS[1:4], each = 6))\ndds$condition &lt;- factor(rep(c(\"treat\", \"control\"), 12))\ndds &lt;- dds[, order(dds$bloodtype, dds$patient, dds$condition)]\ncolnames(dds) &lt;- paste0(\"sample\", 1:ncol(dds))\nFirst let‚Äôs look at our sample information:\ncolData(dds)\n## DataFrame with 24 rows and 3 columns\n##          condition bloodtype  patient\n##           &lt;factor&gt;  &lt;factor&gt; &lt;factor&gt;\n## sample1    control    bloodO        C\n## sample2    control    bloodO        C\n## sample3    control    bloodO        C\n## sample4    treat      bloodO        C\n## sample5    treat      bloodO        C\n## ...            ...       ...      ...\n## sample20   control    bloodA        B\n## sample21   control    bloodA        B\n## sample22   treat      bloodA        B\n## sample23   treat      bloodA        B\n## sample24   treat      bloodA        B\nNow we have three factors, but patient is nested within bloodtype (i.e.¬†a patient is either bloodA or bloodO, it cannot be both). Therefore, bloodtype is a linear combination with patient (or, another way to think about it is that bloodtype is redundant with patient). Because of this, we will define our design without including ‚Äúbloodtype‚Äù, although later we can compare groups of patient of the same bloodtype with each other.\ndesign(dds) &lt;- ~ 1 + patient + condition + patient:condition\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n## [1] \"Intercept\"                  \"patient_B_vs_A\"            \n## [3] \"patient_C_vs_A\"             \"patient_D_vs_A\"            \n## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\"   \n## [7] \"patientC.conditiontreat\"    \"patientD.conditiontreat\"\nNow it‚Äôs harder to define contrasts between groups of patient of the same bloodtype using DESeq‚Äôs coefficient names (although still possible). But using the model matrix approach, we do it in exactly the same way we have done so far!\nAgain, let‚Äôs define our groups from the model matrix:\n# get the model matrix\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\n# define coefficient vectors for each group\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\nIt‚Äôs worth looking at some of these vectors, to see that they are composed of weighted coefficients from different patient. For example, for ‚ÄúbloodO‚Äù patient, we have equal contribution from ‚ÄúpatientC‚Äù and ‚ÄúpatientD‚Äù:\nbloodO_control\n##             (Intercept)                patientB                patientC \n##                     1.0                     0.0                     0.5 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.5                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\nAnd so, when we define our contrasts, each patient will be correctly weighted:\nbloodO_treat - bloodO_control\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.0                     0.0 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.0                     1.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.5                     0.5\nWe can set our contrasts in exactly the same way as we did in the previous section (for completeness, we also give the contrasts using DESeq‚Äôs named coefficients).\nbloodA vs bloodO (in the control):\nres1_bloodA_bloodO_control &lt;- results(dds, contrast = bloodA_control - bloodO_control)\n# or equivalently\nres2_bloodA_bloodO_control &lt;- results(dds, \n    contrast = list(c(\"patient_B_vs_A\"), # Blood type A\n    c(\"patient_C_vs_A\", # Blood type O\n    \"patient_D_vs_A\"))) # Blood type O\nbloodA vs bloodO (in the treat):\nres1_bloodO_bloodA_treat &lt;- results(dds, contrast = bloodO_treat - bloodA_treat)\n# or equivalently\nres2_bloodO_bloodA_treat &lt;- results(dds, \n    contrast = list(c(\"patient_B_vs_A\", # Blood type A\n        \"patientB.conditiontreat\"), # Interaction of patient B with treatment\n        c(\"patient_C_vs_A\", # Blood type O\n            \"patient_D_vs_A\", # Blood type O\n            \"patientC.conditiontreat\", # Interaction of patient C with treatment\n            \"patientD.conditiontreat\"))) # Interaction of patient B with treatment\nAnd so on, for other contrasts of interest‚Ä¶",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/07_extra_contrast_design.html#extra-imbalanced-design",
    "href": "develop/07_extra_contrast_design.html#extra-imbalanced-design",
    "title": "Contrast designs",
    "section": "Extra: imbalanced design",
    "text": "Extra: imbalanced design\nLet‚Äôs take our previous example, but drop one of the samples from the data, so that we only have 2 replicates for it.\ndds &lt;- dds[, -1] # drop one of the patient C samples\ndds &lt;- DESeq(dds)\nresultsNames(dds)\n## [1] \"Intercept\"                  \"patient_B_vs_A\"            \n## [3] \"patient_C_vs_A\"             \"patient_D_vs_A\"            \n## [5] \"condition_treat_vs_control\" \"patientB.conditiontreat\"   \n## [7] \"patientC.conditiontreat\"    \"patientD.conditiontreat\"\nDefine our model matrix and coefficient vectors:\nmod_mat &lt;- model.matrix(design(dds), colData(dds))\nmod_mat\n    ##          (Intercept) patientB patientC patientD conditiontreat\n    ## sample2            1        0        1        0              0\n    ## sample3            1        0        1        0              0\n    ## sample4            1        0        1        0              1\n    ## sample5            1        0        1        0              1\n    ## sample6            1        0        1        0              1\n    ## sample7            1        0        0        1              0\n    ## sample8            1        0        0        1              0\n    ## sample9            1        0        0        1              0\n    ## sample10           1        0        0        1              1\n    ## sample11           1        0        0        1              1\n    ## sample12           1        0        0        1              1\n    ## sample13           1        0        0        0              0\n    ## sample14           1        0        0        0              0\n    ## sample15           1        0        0        0              0\n    ## sample16           1        0        0        0              1\n    ## sample17           1        0        0        0              1\n    ## sample18           1        0        0        0              1\n    ## sample19           1        1        0        0              0\n    ## sample20           1        1        0        0              0\n    ## sample21           1        1        0        0              0\n    ## sample22           1        1        0        0              1\n    ## sample23           1        1        0        0              1\n    ## sample24           1        1        0        0              1\n    ##          patientB:conditiontreat patientC:conditiontreat\n    ## sample2                        0                       0\n    ## sample3                        0                       0\n    ## sample4                        0                       1\n    ## sample5                        0                       1\n    ## sample6                        0                       1\n    ## sample7                        0                       0\n    ## sample8                        0                       0\n    ## sample9                        0                       0\n    ## sample10                       0                       0\n    ## sample11                       0                       0\n    ## sample12                       0                       0\n    ## sample13                       0                       0\n    ## sample14                       0                       0\n    ## sample15                       0                       0\n    ## sample16                       0                       0\n    ## sample17                       0                       0\n    ## sample18                       0                       0\n    ## sample19                       0                       0\n    ## sample20                       0                       0\n    ## sample21                       0                       0\n    ## sample22                       1                       0\n    ## sample23                       1                       0\n    ## sample24                       1                       0\n    ##          patientD:conditiontreat\n    ## sample2                        0\n    ## sample3                        0\n    ## sample4                        0\n    ## sample5                        0\n    ## sample6                        0\n    ## sample7                        0\n    ## sample8                        0\n    ## sample9                        0\n    ## sample10                       1\n    ## sample11                       1\n    ## sample12                       1\n    ## sample13                       0\n    ## sample14                       0\n    ## sample15                       0\n    ## sample16                       0\n    ## sample17                       0\n    ## sample18                       0\n    ## sample19                       0\n    ## sample20                       0\n    ## sample21                       0\n    ## sample22                       0\n    ## sample23                       0\n    ## sample24                       0\n    ## attr(,\"assign\")\n    ## [1] 0 1 1 1 2 3 3 3\n    ## attr(,\"contrasts\")\n    ## attr(,\"contrasts\")$patient\n    ## [1] \"contr.treatment\"\n    ## \n    ## attr(,\"contrasts\")$condition\n    ## [1] \"contr.treatment\"\n# define coefficient vectors for each group\nbloodO_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"control\", ])\nbloodA_control &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"control\", ])\nbloodO_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodO\" & dds$condition == \"treat\", ])\nbloodA_treat &lt;- colMeans(mod_mat[dds$bloodtype == \"bloodA\" & dds$condition == \"treat\", ])\nNow let‚Äôs check what happens to the bloodO_control group:\nbloodO_control\n##             (Intercept)                patientB                patientC \n##                     1.0                     0.0                     0.4 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.6                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\nNotice that whereas before ‚ÄúpatientC‚Äù and ‚ÄúpatientD‚Äù had each a weight of 0.5, now they have different weights. That‚Äôs because for patientC there‚Äôs only 2 replicates. So, we have a total of 5 bloodtype O individuals in the control (2 from patient C and 3 from D). Therefore, when we calculate the average coefficients for bloodOs, we need to do it as 0.4 x patientC + 0.6 x patientD.\nThe nice thing about this approach is that we do not need to worry about any of this, the weights come from our colMeans() call automatically. And now, any contrasts that we make will take these weights into account:\n# bloodA vs bloodO (in the control)\nbloodA_control - bloodO_control\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.5                    -0.4 \n##                patientD          conditiontreat patientB:conditiontreat \n##                    -0.6                     0.0                     0.0 \n## patientC:conditiontreat patientD:conditiontreat \n##                     0.0                     0.0\n# interaction\n(bloodA_treat - bloodA_control) - (bloodO_treat - bloodO_control)\n##             (Intercept)                patientB                patientC \n##                     0.0                     0.0                    -0.1 \n##                patientD          conditiontreat patientB:conditiontreat \n##                     0.1                     0.0                     0.5 \n## patientC:conditiontreat patientD:conditiontreat \n##                    -0.5                    -0.5\n\nPart of this lesson was originally developed by members of the teaching team (Mary Piper, Meeta Mistry, Radhika Khetani) at the Harvard Chan Bioinformatics Core (HBC).\nIn addition, we would like to thank Hugo Tavares from the Bioinformatics Training Facility of the University of Cambridge.",
    "crumbs": [
      "Data analyses",
      "Differential Expression",
      "Contrast designs"
    ]
  },
  {
    "objectID": "develop/04c_preprocessing_setup.html",
    "href": "develop/04c_preprocessing_setup.html",
    "title": "Ucloud setup",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 40 minutes\nüí¨ Learning Objectives: 1. Learn about the UCloud computing system. 2. Learn how to submit a job and explore your results folders. 3. Submit a nf-core RNAseq run on our data\nRunning the nf-core RNAseq pipeline in UCloud",
    "crumbs": [
      "Data processing",
      "Ucloud setup"
    ]
  },
  {
    "objectID": "develop/04c_preprocessing_setup.html#submit-a-job",
    "href": "develop/04c_preprocessing_setup.html#submit-a-job",
    "title": "Ucloud setup",
    "section": "Submit a job",
    "text": "Submit a job\nAccess Ucloud with your account and choose the project Sandbox RNASeq Workshop where you have been invited. Or ask to be invited to jose.romero@sund.ku.dk.\n\nClick on Apps on the left-side menu, and search for the application nf-core rnaseq and click on it.\n\nYou will be met with a series of possible parameters to choose. However, we have prepared the parameters already for you! Just click on Import parameters:\n\nThen, Import file from UCloud:\n\nAnd select the jobParameters_preprocessing.json file located in the sequencing_data drive:\nsandbox_bulkRNASeq -&gt; sequencing_data -&gt; Scripts -&gt; ucloud_preprocessing_setup -&gt; jobParameters_preprocessing.json\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that the hard-drive icon is labeled ‚Äúsequencing_data‚Äù and you are in the ‚Äúsandbox_bulkRNAseq‚Äù workspace.\nYou can select the drive by clicking on the down arrow (‚à®) icon to open a dropdown menu.\n\n\n\nYou are almost ready to run the app! But first, make sure your screen looks like the one shown here. Review the arguments used to run the job: we have specified a Job name, Hours, Machine type, and an additional parameter, Interactive mode. Interactive mode allows us to monitor the progress of the pipeline. Additionally, we have selected the sequencing_data folder to access our sequencing reads while the job is running.\n\nNow, click the button on the top-right of the screen (submit) to start the job.\nNext, wait until the screen looks like the figure below. This process usually takes a few minutes. You can always come back to this screen via the Runs button in the left menu on UCloud. From there, you can add extra time or stop the app if you no longer need it.\n\nNow, click on Open terminal on the top right-hand side of the screen. You will start terminal session through your browser! Once inside the terminal, you will need to do one last thing before starting the pipeline:\ntmux \n\nThe tmux command will start a virtual command line session that is recoverable. This is very important because once we start the pipeline, we will lose the ability to track the progress if your computer loses connection or is in sleeping mode. You will know you are inside the tmux virtual session by looking at the bottom of the screen:\n\n\n\n\n\n\n\nReconnect to tmux session\n\n\n\nIf you want to leave the tmux session, you can do so by pressing simultaneously Ctrl and B keys, and then press D. Then you can reconnect to the session using the command:\ntmux attach -t 0\n\n\nWe can finally start the run! Type in the command:\nbash ./sequencing_data/Scripts/ucloud_preprocessing_setup/preprocessing_salmon.sh \nThis will run a small bash script that will start the nf-core pipeline. It is usually a good idea to save the command that was used to run the pipeline! You should see now a prompt like this, which means that the pipeline started successfully!\n\nInside the preprocessing_salmon.sh script you will find:\n#!/bin/bash\n\ncp /work/sequencing_data/raw_reads/samplesheet.csv /work/samplesheet.csv\ncp /work/sequencing_data/Scripts/ucloud_preprocessing_setup/nf-params_salmon.json /work/nf-params_salmon.json\ncd /work\n\nnextflow run nf-core/rnaseq -r 3.11.2 -params-file /work/nf-params_salmon.json -profile conda --max_cpus 8 --max_memory 40GB\n\n# Search for the last file created by the pipeline, the multiqc_report, recursively\nfile_path=$(find /work/preprocessing_results_salmon -name \"multiqc_report.html\" 2&gt;/dev/null)\n\n# Check if the file exists\nif [[ -n \"$file_path\" ]]; then\n    # Clean run if the pipeline is completed\n    rm -r /work/work\n    mv /work/nf-params_salmon.json /work/preprocessing_results_salmon/nf-params_salmon.json\n\nfi\nYou see that we have copied the samplesheet.csv to the working directory /work. This is because the paths inside the samplesheet.csv for the fastq files of our samples are relative to the /work folder! It is very important that the fastq paths inside this file matches properly to the paths inside your job!\nWe have also copied the nf-params.json file with all the options used in the pipeline, so that you can find and replicate easily this run in the future. Finally, we remove the nextflow work directory if the pipeline completes successfully. this will save quite a bit if storage in the future, since the nextflow work directory will accumulate over runs.\nThen we are making sure that we are inside the correct folder before starting the job using cd /work. We will see in the section below the arguments we used to run the pipeline:",
    "crumbs": [
      "Data processing",
      "Ucloud setup"
    ]
  },
  {
    "objectID": "develop/04c_preprocessing_setup.html#understanding-the-pipeline-options",
    "href": "develop/04c_preprocessing_setup.html#understanding-the-pipeline-options",
    "title": "Ucloud setup",
    "section": "Understanding the pipeline options",
    "text": "Understanding the pipeline options\nLet‚Äôs divide the command into different sections. First we have:\nnextflow run nf-core/rnaseq -r 3.11.2\nWhile usually one would run an nf-core pipeline using nextflow run nf-core/rnaseq and fetch the pipeline remotely, UCloud has installed the pipelines locally. Specifically, we are using the version 3.11.2 by using the argument -r 3.11.2.\n\nSecond, we have:\n-params-file /work/sequencing_data/nf-params.json\nThe -params-file argument is another nextflow core argument that allows us to give the nf-core rnaseq pipeline arguments in a json file, instead of creating an excessively long command. Writing the parameters this way allows for better reproducibility, since you can reuse the file in the future. Inside this file, we find the following arguments:\n{\n    \"input\": \"/work/samplesheet.csv\",\n    \"outdir\": \"/work/preprocessing_results_salmon\",\n    \"fasta\": \"/work/sequencing_data/genomic_resources/homo_sapiens/GRCh38/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\",\n    \"gtf\": \"/work/sequencing_data/genomic_resources/homo_sapiens/GRCh38/Homo_sapiens.GRCh38.109.MODIFIED.gtf.gz\",\n    \"pseudo_aligner\": \"salmon\",\n    \"skip_stringtie\": true,\n    \"skip_rseqc\": true,\n    \"skip_preseq\": true,\n    \"skip_qualimap\": true,\n    \"skip_biotype_qc\": true,\n    \"skip_bigwig\": true,\n    \"skip_deseq2_qc\": true,\n    \"skip_bbsplit\": true,\n    \"skip_alignment\": true,\n    \"extra_salmon_quant_args\": \"--gcBias\"\n}\n--input parameter\nThe --input parameter points to the samplesheet.csv file that contains all the info regarding our samples. The file looks like this:\n\n\n\n\n\n\n\n\n\nsample\nfastq_1\nfastq_2\nstrandedness\ncondition\n\n\n\n\ncontrol_3\n778339/raw_reads/Control_3.fastq.gz\nNA\nunstranded\ncontrol\n\n\ncontrol_2\n778339/raw_reads/Control_2.fastq.gz\nNA\nunstranded\ncontrol\n\n\ncontrol_1\n778339/raw_reads/Control_1.fastq.gz\nNA\nunstranded\ncontrol\n\n\nvampirium_3\n778339/raw_reads/Vampirium_3.fastq.gz\nNA\nunstranded\nvampirum\n\n\nvampirium_2\n778339/raw_reads/Vampirium_2.fastq.gz\nNA\nunstranded\nvampirium\n\n\nvampirium_1\n778339/raw_reads/Vampirium_1.fastq.gz\nNA\nunstranded\nvampirium\n\n\ngarlicum_3\n778339/raw_reads/Garlicum_3.fastq.gz\nNA\nunstranded\ngarlicum\n\n\ngarlicum_2\n778339/raw_reads/Garlicum_2.fastq.gz\nNA\nunstranded\ngarlicum\n\n\n\n\n\n\n\n\nAs you can see, we have also provided an extra column called condition specifying the sample type. This will be very useful for our Differential Expression Analysis. In addition, you can also notice that we have a single-end RNAseq experiment in our hands. Finally, take a note at the fastq paths that we have provided! They all point to where the files are located inside our job!\n--outdir parameter\nThe --outdir parameter indicates where the results of the pipeline will be saved.\n--fasta parameter\nPath to FASTA reference genome file.\n--gtf parameter\nPath to GTF annotation file that contains genomic region information.\n--pseudo_aligner argument\nThe --pseudo_aligner argument indicates that we want to use salmon to quantify transcription levels.\nFinally, we are skipping several QC and extra steps that we did not explain in the previous lesson. Do not worry if you cannot manage to run the pipeline or you do not have the time, we have prepared a backup folder that contains the results from a traditional alignment + pseudoquantification for you to freely explore! (More about that below).\n\nWe can continue with the next argument:\n-profile conda\nUnfortunately, the UCloud implementation of the nf-core pipelines do not currently allow the use of docker or singularity, which are the recommended profile options. However, UCloud has made sure that there is a working conda environment ready to use!\nThen the last couple of arguments:\n--max_cpus 8 --max_memory 40GB\nThese are nf-core specific arguments that indicates nextflow to only use as maximum the number of CPUs and RAM we have requested when we submitted the job. We are using 8 cores since it is what we requested in the submission page (e.g.¬†if you submitted a job with 4 CPUs, this will be equal to 4). We are using slightly less RAM than we requested (48Gb) just in case there is a problem with memory overflow.",
    "crumbs": [
      "Data processing",
      "Ucloud setup"
    ]
  },
  {
    "objectID": "develop/04c_preprocessing_setup.html#restarting-a-failed-run",
    "href": "develop/04c_preprocessing_setup.html#restarting-a-failed-run",
    "title": "Ucloud setup",
    "section": "Restarting a failed run",
    "text": "Restarting a failed run\nWhen running a nf-core pipelines for the first time, you might encounter some errors, for example, one of your files has an incorrect path, or a program failed to do its job.\n\n\n\n\n\n\nFailure\n\n\n\n# Error in terminal\nError executing process &gt;\nCaused by:\n    Failed to create create conda environment\n\n\n\nOnce you fix the error, it is possible to resume a pipeline instead of restarting the whole workflow. You can do this by adding the -resume argument to the nextflow command:\nnextflow run nf-core/rnaseq -r 3.11.2 -params-file /work/sequencing_data/Scripts/nf-params.json -profile conda --max_cpus 8 --max_memory 40GB‚Äã -resume",
    "crumbs": [
      "Data processing",
      "Ucloud setup"
    ]
  },
  {
    "objectID": "develop/04c_preprocessing_setup.html#stopping-the-app",
    "href": "develop/04c_preprocessing_setup.html#stopping-the-app",
    "title": "Ucloud setup",
    "section": "Stopping the app",
    "text": "Stopping the app\nOnce the pipeline is done, go on Runs in uCloud and stop it from using more resources than necessary! This will help to keep the courses running for other people.",
    "crumbs": [
      "Data processing",
      "Ucloud setup"
    ]
  },
  {
    "objectID": "develop/04c_preprocessing_setup.html#saved-results",
    "href": "develop/04c_preprocessing_setup.html#saved-results",
    "title": "Ucloud setup",
    "section": "Saved results",
    "text": "Saved results\nAfter finishing the job, everything that the pipeline has created will be saved in your own personal ‚ÄúJobs‚Äù folder. Inside this folder there will be a subfolder called nf-core: rnaseq, which will contain all the jobs you have run with the nf-core app. Inside this folder, you will find the results folder named after the job name you gave when you submitted the job.\n\nYour material will be saved in a volume with your username, that you should be able to see under the menu Files &gt; Drives &gt; Member Files:YourName#numbers\n\n\n\nGo to Jobs -&gt; nf-core: rnaseq -&gt; job_name (RNAseq preprocessing ...) -&gt; preprocessing_results_salmon\n\n\nNow you have access to the full results of your pipeline! As explained in the previous lesson, the nf-core rnaseq workflow will create a MultiQC report summarizing several steps into a single html file that is interactive and explorable. In addition, there will be a folder with the results of the individual QC steps as well as the alignment and quantification results. Take your time and check it all out!",
    "crumbs": [
      "Data processing",
      "Ucloud setup"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html",
    "href": "develop/04a_preprocessing.html",
    "title": "Preprocessing steps",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: 40 minutes\nüí¨ Learning Objectives:\n\nUnderstand the different steps of the RNA-seq workflow, from RNA extraction to assessing the expression levels of genes.\nFrom raw sequence reads to count matrix: the RNA-seq workflow\nTo perform differential gene expression analysis (DEA), we need to start with a matrix of counts representing the levels of gene expression. It is important to understand how the count matrix is generated, before diving into the statistical analysis.\nIn this lesson we will briefly discuss the RNA-processing pipeline for bulk RNA-seq, and the different steps we take to go from raw sequencing reads to a gene expression count matrix.",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#rna-extraction-and-library-preparation",
    "href": "develop/04a_preprocessing.html#rna-extraction-and-library-preparation",
    "title": "Preprocessing steps",
    "section": "1. RNA Extraction and library preparation",
    "text": "1. RNA Extraction and library preparation\nBefore RNA can be sequenced, it must first be extracted and separated from its cellular environment and prepared into a cDNA library. There are a number of steps involved which are outlined in the figure below, and in parallel there are various quality checks implemented to make sure we have good quality RNA to move forward with. We briefly describe some of these steps below.\na. Enriching for RNA. Once the sample has been treated with DNAse to remove any contaminating DNA sequence, the sample undergoes either selection of the mRNA (polyA selection) or depletion of the ribosomal RNA (rRNA).\nGenerally, rRNA represents the majority of the RNA present in a cell, while messenger RNAs represent a small percentage of total RNA, ~2% in humans. Therefore, if we want to study the protein-coding genes, we need to enrich mRNA or deplete the rRNA. For differential gene expression analysis, it is best to enrich for Poly(A)+, unless you are aiming to obtain information about long non-coding RNAs, in which case rRNA depletion is recommended.\n\nRNA Quality check: It is essential to check the integrity of the extracted RNA prior to starting the cDNA library prepation. Traditionally, RNA integrity was assessed via gel electrophoresis by visual inspection of the ribosomal RNA bands; but that method is time consuming and imprecise. The Bioanalyzer system from Agilent will rapidly assess RNA integrity and calculate an RNA Integrity Number (RIN), which facilitates the interpretation and reproducibility of RNA quality. RIN, essentially, provides a means by which RNA quality from different samples can be compared to each other in a standardized manner.\n\nb. Fragmentation and size selection. The remaining RNA molecules are then fragmented. This is done either via chemical, enzymatic (e.g., RNAses) or physical processes (e.g., chemical/mechanical shearing). These fragments then undergo size selection to retain only those fragments within a size range that Illumina sequencing machines can handle best, i.e., between 150 to 300 bp.\n\nFragment size quality check: After size selection/exclusion the fragment size distribution should be assessed to ensure that it is unimodal and well-defined.\n\nc.¬†Reverse transcribe RNA into double-stranded cDNA. Information about which strand a fragment originated from can be preserved by creating stranded libraries. The most commonly used method incorporates deoxy-UTP during the synthesis of the second cDNA strand (for details see Levin et al.¬†(2010)). Once double-stranded cDNA fragments are generated, sequence adapters are ligated to the ends. (Size selection can be performed here instead of at the RNA-level.)\nd.¬†PCR amplification. If the amount of starting material is low and/or to increase the number of cDNA molecules to an amount sufficient for sequencing, libraries are usually PCR amplified. Run as few amplification cycles as possible to avoid PCR artifacts.\n\n\n\nImage source: Zeng and Mortavi, 2012",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#sequencing-illumina",
    "href": "develop/04a_preprocessing.html#sequencing-illumina",
    "title": "Preprocessing steps",
    "section": "2. Sequencing (Illumina)",
    "text": "2. Sequencing (Illumina)\nSequencing of the cDNA libraries will generate reads. Reads correspond to the nucleotide sequences of the ends of each of the cDNA fragments in the library. You will have the choice of sequencing either a single end of the cDNA fragments (single-end reads) or both ends of the fragments (paired-end reads).\n\n\nSE - Single end dataset =&gt; Only Read1\nPE - Paired-end dataset =&gt; Read1 + Read2\n\nPE can be 2 separate FastQ files or just one with interleaved pairs\n\n\nGenerally, single-end sequencing is sufficient unless it is expected that the reads will match multiple locations on the genome (e.g.¬†organisms with many paralogous genes), assemblies are being performed, or for splice isoform differentiation. On the other hand, paired-end sequencing helps resolve structural genome rearrangements e.g.¬†insertions, deletions, or inversions. Furthermore, paired reads improve the alignment/assembly of reads from repetitive regions. The downside of this type of sequencing is that it may be twice as expensive.\nThe scientific community is moving towards paired-end sequencing in general. However, for many purposes, single-end reads are perfectly adequate.\n\nSequencing-by-synthesis\nIllumina sequencing technology uses a sequencing-by-synthesis approach. To explore sequencing by synthesis in more depth, please watch this linked video on Illumina‚Äôs YouTube channel.\nWe have provided a brief explanation of the steps below:\n\nCluster growth: The DNA fragments in the cDNA library are denatured and hybridized to the glass flowcell (adapter complementarity). Each fragment is then clonally amplified, forming a cluster of double-stranded DNA. This step is necessary to ensure that the sequencing signal will be strong enough to be detected/captured unambiguously for each base of each fragment. NOTE: Number of clusters ~= Number of reads\nSequencing: The sequencing of the fragment ends is based on fluorophore labelled dNTPs with reversible terminator elements. In each sequencing cycle, a base is incorporated into every cluster and excited by a laser.\nImage acquisition: Each dNTP has a distinct excitatory signal emission which is captured by cameras.\nBase calling: The Base calling program will then generate the sequence of bases, i.e.¬†reads, for each fragment/cluster by assessing the images captured during the many sequencing cycles. In addition to calling the base in every position, the base caller will also report the certainty with which it was able to make the call (quality information). NOTE: Number of sequencing cycles = Length of reads",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#quality-control-of-raw-sequencing-data",
    "href": "develop/04a_preprocessing.html#quality-control-of-raw-sequencing-data",
    "title": "Preprocessing steps",
    "section": "3. Quality control of raw sequencing data",
    "text": "3. Quality control of raw sequencing data\nThe raw reads obtained from the sequencer are stored as FASTQ files. The FASTQ file format is the de facto file format for sequence reads generated from next-generation sequencing technologies.\nEach FASTQ file is a text file which represents sequence readouts for a sample. Each read is represented by 4 lines as shown below:\n@HWI-ST330:304:H045HADXX:1:1101:1111:61397\nCACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n+\n@?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI##################################################################################################################\n\n\n\n\n\n\n\nLine\nDescription\n\n\n\n\n1\nAlways begins with ‚Äò@‚Äô and then information about the read\n\n\n2\nThe actual DNA sequence, where N means that no base was called (poor quality)\n\n\n3\nAlways begins with a ‚Äò+‚Äô and sometimes the same info as in line 1\n\n\n4\nHas a string of characters which represent the quality scores; must have same number of characters as line 2\n\n\n\n¬†\nFastQC is a commonly used software that provides a simple way to do some quality control checks on raw sequence data.\nThe main functions include:\n\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your data\nExport of results to an HTML based permanent report\n\n\nQuality metrics\nHere you will find a list of metrics that FASTQC will calculate on your reads:\n\nPhred Quality Scores: Preprocessed reads are evaluated based on Phred quality scores. These scores represent the estimated probability of a base call being incorrect. Higher Phred scores indicate higher base-call accuracy.\nSequence Length Distribution: QC tools assess the length distribution of the preprocessed reads. This helps in identifying any biases introduced during preprocessing, such as excessive shortening of reads.\nAdapter Contamination: Even after preprocessing, it‚Äôs crucial to confirm that all adapter sequences have been successfully removed. Any remaining adapter contamination can adversely affect downstream analyses.\nGC Content: Evaluating the GC content of preprocessed reads helps in detecting biases that might have been introduced during library preparation or sequencing.\nDuplicate Reads: Preprocessed reads should be checked for duplicates. Duplicate reads can arise due to PCR amplification biases during library preparation.\nK-mer Content: QC tools can analyze the frequency distribution of k-mers (short sequences of length k). Deviations from the expected k-mer distribution may indicate biases or contamination.\nOverall Sequence Quality: A summary of the overall quality metrics, including mean Phred scores, per-base sequence quality, and sequence duplication levels, provides a comprehensive assessment of data quality.",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#read-filtering-and-trimming",
    "href": "develop/04a_preprocessing.html#read-filtering-and-trimming",
    "title": "Preprocessing steps",
    "section": "4. Read filtering and trimming",
    "text": "4. Read filtering and trimming\nThe reads in a FASTQ file may contain errors, low-quality bases and adapter sequences. To extract reliable biological information, it‚Äôs crucial to preprocess or ‚Äúclean‚Äù this data through trimming and filtering.\nTrimming involves the removal of low-quality bases from the ends of reads. Low-quality bases can arise due to various factors, such as limitations in the sequencing technology or degradation during sample preparation. Trimming helps to improve the overall quality of the data, which is essential for downstream analysis.\nAdditionally, adapter sequences, which are short DNA sequences used in library preparation, can be mistakenly sequenced along with the target DNA. Trimming these adapters is necessary to ensure accurate alignment and subsequent analysis.\nFiltering is a broader process that involves the removal of reads that do not meet specific quality criteria. For example, reads with an excessive number of low-quality bases or those that are too short may be discarded. This step helps to retain high-confidence data for downstream analysis.\nTrimming and filtering are crucial steps in NGS data processing because they improve the accuracy and reliability of the data. Without these steps, subsequent analyses like genome assembly, variant calling, and transcript quantification can be severely affected. By reducing noise and removing artifacts, researchers can obtain a clearer and more accurate picture of the biological information encoded in the sequencing data.\nTo trim and filter reads, we can use bioinformatics tools such as like Cutadapt and Trim Galore. They offer powerful and versatile functionalities for trimming and filtering raw sequencing reads, ensuring that only high-quality data is used for subsequent analyses.\n\nCutadapt\nCutadapt is a widely-used and highly flexible tool designed specifically for removing adapter sequences from NGS reads. Adapter sequences can be introduced during library preparation and may subsequently be sequenced along with the target DNA or RNA. Cutadapt employs a sophisticated algorithm to accurately and efficiently identify and trim these adapters.\nKey features of Cutadapt include:\n\nAdapter Detection and Removal: Cutadapt can detect and remove adapters with high precision, even in cases where the adapter sequence is only partially known.\nError-Tolerant Matching: It can perform error-tolerant matching, allowing it to handle cases where adapter sequences might have minor variations or mutations.\nQuality Trimming: Cutadapt can also perform quality trimming, which involves removing low-quality bases from the ends of reads. This feature helps in improving data quality.\nBatch Processing: It can process multiple files in a single run, making it efficient for handling large-scale datasets.\nFormat Compatibility: Cutadapt supports various file formats commonly used in NGS, such as FASTQ and SAM.\n\n\n\nTrim Galore\nTrim Galore is a user-friendly wrapper script that combines the functionalities of Cutadapt with FastQC, to provide a streamlined solution for trimming and quality control of NGS data. It simplifies the preprocessing workflow by automating the process of running Cutadapt and generating quality reports through FastQC.",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#quality-control-of-clean-sequencing-data",
    "href": "develop/04a_preprocessing.html#quality-control-of-clean-sequencing-data",
    "title": "Preprocessing steps",
    "section": "5. Quality control of clean sequencing data",
    "text": "5. Quality control of clean sequencing data\nAfter preprocessing the reads, we assess again the quality and reliability of the sequencing reads. This QC step is essential to ensure that only high-quality data is used for downstream analyses. We will check that the quality metrics calculated on the raw reads have improved:\n\nImproved Quality: Preprocessed reads typically exhibit higher quality scores and improved base call accuracy compared to raw reads. This is because preprocessing steps like adapter trimming and quality filtering remove low-quality bases and artifacts.\nReduced Noise and Artifacts: Preprocessing removes noise, such as adapter sequences, low-quality bases, and sequencing errors. This leads to cleaner data, enhancing the accuracy of downstream analyses.",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#alignment",
    "href": "develop/04a_preprocessing.html#alignment",
    "title": "Preprocessing steps",
    "section": "6. Alignment",
    "text": "6. Alignment\nAfter checking that the quality of our reads is adequate, we can proceed to aligning our sequencing reads to a reference genome. By doing this, we can identify variations, quantify gene expression levels and study other genomic features. For the purposes of this workshop, we are mostly interested in the ability to quantify gene expression levels for our differential expression analysis.\nAlignment is achieved by finding the best matching position in the reference genome for each read. This is a computationally intensive task due to the vast amount of data generated by NGS experiments. Alignment tools utilize various algorithms and techniques to efficiently perform this task, which can be mostly divided in two categories: traditional aligment and pseudoaligment.\n\nTraditional alignment tools\nTraditional alignment consists in the process described above, matching your preprocessed reads to a reference genome. This process involves determining the genomic location from which each read originated. The result of the alignment will be a SAM/BAM file, which will contain information regarding the quality and the genomic position of the aligned read.\nBelow we will highlight some of the most common alignment algorithms:\n\nBowtie2\nBowtie2 is a widely-used, ultra-fast alignment tool designed for aligning short reads (typically from Illumina platforms) to a reference genome. It employs a Burrows-Wheeler transform-based algorithm, which allows it to quickly and accurately align millions of reads. Bowtie is highly efficient, making it a popular choice for large-scale NGS projects.\n\n\nSTAR\nSTAR (Spliced Transcripts Alignment to a Reference) is a specialized alignment tool tailored for RNA-Seq data. It is designed to align reads to a reference genome, taking into account the splicing events that occur in eukaryotic genomes. STAR can align both short and long RNA-Seq reads, making it a versatile tool for gene expression analysis and transcriptome mapping.\n\n\nHISAT2\nHISAT2 (Hierarchical Indexing for Spliced Alignment of Transcripts) is another prominent alignment tool widely used for aligning RNA-Seq reads to a reference genome. It employs a hierarchical indexing approach that enables efficient and accurate alignment, particularly in the presence of spliced alignments. HISAT2 is known for its speed and sensitivity, making it a popular choice for transcriptome analysis. It also offers the advantage of reduced memory usage compared to some other alignment tools, making it suitable for a wide range of computational environments. HISAT2‚Äôs ability to accurately handle splice junctions makes it a valuable tool for studying alternative splicing events and other complex features of transcriptomes.\n\n\n\nPseudoalignment\nPseudoalignment is a concept in computational biology and genomics that offers an alternative approach to traditional read alignment. Unlike traditional alignment, which involves finding the exact position of a read within a reference genome, pseudoalignment estimates the likelihood that a read originates from a specific transcript or set of transcripts without explicitly mapping it to the reference genome.\nPseudoalignment tools, like Salmon and Kallisto, achieve this by building an index of transcript sequences rather than the entire genome. They use efficient algorithms to quickly determine which transcripts are likely to be the source of a given read. This approach significantly reduces the computational resources required for quantifying gene expression, as it circumvents the need to align every read to the entire genome. By focusing on transcripts, pseudoalignment provides a faster and more memory-efficient solution, making it especially advantageous for large-scale RNA-Seq studies and in situations where rapid quantification of gene expression levels is critical, such as in time-sensitive experiments or in scenarios with limited computational resources.\nPseudoalignment is well-suited for studying gene expression in well-annotated genomes, where the transcriptome is relatively well-characterized, although not so great with other under-studied organisms. We will highlight a couple of these algorithms below.\n\nSalmon\nSalmon uses a lightweight and rapid algorithm based on the concept of selective alignment. It directly quantifies transcript abundance without explicitly aligning reads to the reference genome. This makes Salmon especially efficient for large-scale RNA-Seq studies, where speed and accuracy are crucial.\n\n\nKallisto\nSimilar to Salmon, Kallisto employs a pseudoalignment strategy. It quantifies transcript abundance by estimating the compatibility of reads with known transcripts, bypassing the need for full alignment to the genome. This approach makes Kallisto extremely fast, making it an attractive choice for rapid and accurate gene expression quantification.",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#quality-control-of-aligned-reads",
    "href": "develop/04a_preprocessing.html#quality-control-of-aligned-reads",
    "title": "Preprocessing steps",
    "section": "7. Quality control of aligned reads",
    "text": "7. Quality control of aligned reads\nAfter aligning our reads, it is essential to perform some basic quality checks on the sequencing data. However, this step is only possible if you align your reads using a traditional algorithm, since pseudoaligment tools will not create a BAM file that can be checked for quality control.\n\nQualimap\nA tool called Qualimap explores the features of aligned reads in the context of the genomic region they map to, hence providing an overall view of the data quality (as an HTML file). Various quality metrics assessed by Qualimap include:\n\nDNA or rRNA contamination\n5‚Äô-3‚Äô biases\nCoverage biases\n\n\n\ndupRadar\nThe [dupRadar[(https://bioconductor.org/packages/release/bioc/vignettes/dupRadar/inst/doc/dupRadar.html) package provides an assessment of the level of duplication of your reads, allowing you to distinguish PCR amplification artifacts from true biological signals.\nThe number of reads per base assigned to a gene in an ideal RNA-Seq data set is expected to be proportional to the abundance of its transcripts in the sample. For lowly expressed genes we expect read duplication to happen rarely by chance, while for highly expressed genes - depending on the total sequencing depth - we expect read duplication to happen often.\nA good way to learn if a dataset is following this trend is by relating the normalized number of counts per gene (RPK, as a quantification of the gene expression) and the fraction represented by duplicated reads. For example, the plots below show very well how duplicates should look in a good experiment (left) compared to another experiment with duplication issues(right)\n\n\n\ndupRadar example",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#quantify-expression",
    "href": "develop/04a_preprocessing.html#quantify-expression",
    "title": "Preprocessing steps",
    "section": "8. Quantify expression",
    "text": "8. Quantify expression\nOnce we have explored the quality of our raw reads, we can move on to quantifying expression at the transcript level. The goal of this step is to identify from which transcript each of the reads originated from and the total number of reads associated with each transcript.\nQuantification from BAM files is the traditional method of estimating gene expression levels. It involves aligning reads to a reference genome using tools like Bowtie, STAR, or HISAT2, and then counting the number of reads that map to each gene or transcript. This process relies on the generation of a BAM (Binary Alignment/Map) file, which records the alignment information for each read. Quantification tools like featureCounts or HTSeq then process the BAM file to count reads that align to each annotated gene.\nPseudoaligment tools such as Kallisto and Salmon they perform pseudoalignment and quantification in the same step by quickly mapping reads to a set of reference transcripts. This is done using an indexing strategy that efficiently assigns reads to potential transcript sources. Pseudoquantification is particularly fast and memory-efficient, making it ideal for large-scale transcriptome studies. It provides accurate estimates of transcript abundance, even in the presence of complex transcript structures.\nIn this course, we will use the expression estimates, often referred to as ‚Äòpseudocounts‚Äô, obtained from Salmon as the starting point for the differential gene expression analysis.",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  },
  {
    "objectID": "develop/04a_preprocessing.html#aggregation-of-quality-control-checks",
    "href": "develop/04a_preprocessing.html#aggregation-of-quality-control-checks",
    "title": "Preprocessing steps",
    "section": "9. Aggregation of quality control checks",
    "text": "9. Aggregation of quality control checks\nThroughout the workflow we have performed various steps of quality checks on our data. You will need to do this for every sample in your dataset, making sure these metrics are consistent across the samples for a given experiment. Outlier samples should be flagged for further investigation and potential removal.\nManually tracking these metrics and browsing through multiple HTML reports (FastQC, Qualimap) and log files (Salmon, STAR) for each sample is tedious and prone to errors. MultiQC is a tool which aggregates results from several tools and generates a single HTML report with plots to visualize and compare various QC metrics between the samples. Assessment of the QC metrics may result in the removal of samples before proceeding to the next step, if necessary.\nOnce the QC has been performed on all the samples, we are ready to get started with Differential Gene Expression analysis with DESeq2!\n\n\nThis lesson was originally developed by members of the teaching team at the Harvard Chan Bioinformatics Core (Meeta Mistry, Radhika Khetani and Mary Piper) (HBC).",
    "crumbs": [
      "Data processing",
      "Preprocessing steps"
    ]
  }
]